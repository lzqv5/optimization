{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from time import time\n",
    "from scipy.interpolate import interp1d\n",
    "from libsvm.svmutil import svm_read_problem # https://blog.csdn.net/u013630349/article/details/47323883\n",
    "\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single(seq, xlabel='Iteration', ylabel='Gradient Norm', title=''):\n",
    "    plt.figure()\n",
    "    iterations = np.arange(len(seq))+1\n",
    "    plt.semilogy(iterations,seq)\n",
    "    plt.xticks(iterations)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "def plot_multi_seqs(seqs, xlabel='Iteration', ylabel='Gradient Norm', title='', xtick_step = 50):\n",
    "    plt.figure(figsize=(16,8), dpi=150)\n",
    "    maxLen = 0\n",
    "    for seq in seqs:\n",
    "        iterations = seq.index\n",
    "        if iterations.size > maxLen:\n",
    "            maxLen = iterations.size\n",
    "        plt.semilogy(iterations, seq, label=seq.name)\n",
    "    plt.xticks(np.arange(stop=maxLen,step=xtick_step)+1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45546, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(path):\n",
    "    b, A = svm_read_problem(path)\n",
    "    rows = len(b)   # 矩阵行数, i.e. sample 数\n",
    "    cols = max([max(row.keys()) if len(row)>0 else 0 for row in A])  # 矩阵列数, i.e. feature 数\n",
    "    b = np.array(b)\n",
    "    A_np = np.zeros((rows,cols))\n",
    "    for r in range(rows):\n",
    "        for c in A[r].keys():\n",
    "            # MatLab 是 1-index, python 则是 0-index\n",
    "            A_np[r,c-1] = A[r][c]\n",
    "    # 清楚全 0 features\n",
    "    effective_row_ids = []\n",
    "    for idx, row in enumerate(A_np):\n",
    "        if np.sum(row) > 1e-3:\n",
    "            effective_row_ids.append(idx)\n",
    "    return b[effective_row_ids], A_np[effective_row_ids]\n",
    "\n",
    "b, A = read_data('w8a')\n",
    "m,n = A.shape\n",
    "m,n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令 $g_i(x)=\\exp(b_i x^\\top a_i), i=1,2,...,m$\n",
    "\n",
    "所以可以得到目标函数为:\n",
    "$$\n",
    "f(x)=\\frac{1}{m}\\sum_{i=1}^{m}\\log\\left( 1+ \\frac{1}{g_i(x)}\\right) + \\frac{1}{100m}\\left\\|x\\right\\|^2\n",
    "$$\n",
    "进而可以得到梯度 $\\nabla f(x)$ 和 Hessian矩阵 $\\nabla^2 f(x)$ 的形式:\n",
    "$$\n",
    "\\nabla f(x) = \\frac{1}{m}\\sum_{i=1}^{m}\\left[ -b_i a_i (1+g_i(x))^{-1} \\right] + \\frac{1}{50m}x\n",
    "$$\n",
    "$$\n",
    "\\nabla^2 f(x) =  \\frac{1}{m}\\sum_{i=1}^{m}\\left( b_i^2 \\frac{g_i(x)}{(1+g_i(x))^2}a_i a_i^\\top \\right) + \\frac{1}{50m}I\n",
    "$$\n",
    "\n",
    "此处函数内的 $a_i,b_i$ 和数据本身相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    bAx = b*(A@x)\n",
    "    exp_mbAx = np.exp(-bAx)\n",
    "    log1p_exp = np.log(1+exp_mbAx)\n",
    "    overflow_idxs = np.where(exp_mbAx==float('inf'))\n",
    "    log1p_exp[overflow_idxs] = -bAx[overflow_idxs]\n",
    "    # return np.log(1+np.exp(-bAx)).mean() + 1/(100*m)* x.T@x\n",
    "    return log1p_exp.mean() + 1/(100*m)* x.T@x\n",
    "\n",
    "def f_grad(x):\n",
    "    # return np.ones(m)@(np.expand_dims((-b)/(1+np.exp(b*(A@x))), axis=1)*A)/m\n",
    "    return np.ones(m)@(np.expand_dims((-b)/(1+np.exp(b*(A@x))), axis=1)*A)/m + 1/(50*m)*x\n",
    "\n",
    "def f_hessian(x):\n",
    "    Ax = A@x\n",
    "    exp_bAx = np.exp(b*Ax)\n",
    "    # return (A.T @ (np.expand_dims(b*b*exp_bAx/(1+exp_bAx)**2, axis=1)*A) )/m\n",
    "    return (A.T @ (np.expand_dims(b*b*exp_bAx/(1+exp_bAx)**2, axis=1)*A) )/m + 1/(50*m)*np.diag([1.0]*x.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Armijo rule \n",
    "def armijo_search(f, f_grad, xk, t_hat, alpha, beta, D, isNewton=False, dk=None):\n",
    "    if isNewton:\n",
    "        assert dk is not None\n",
    "    tk = t_hat*1\n",
    "    grad = f_grad(xk)\n",
    "    while True:\n",
    "        if isNewton:\n",
    "            if np.linalg.norm(xk+tk*dk,ord=2)<=D/2 and f(xk+tk*dk) <= f(xk) + alpha*tk*grad.T@dk:\n",
    "                break\n",
    "        else:\n",
    "            if np.linalg.norm(xk-tk*grad,ord=2)<=D/2 and f(xk-tk*grad) <= f(xk)-alpha*tk*grad.T@grad:\n",
    "                break\n",
    "        tk *= beta\n",
    "    return tk\n",
    "\n",
    "#* 梯度下降法\n",
    "def gradient_descent(f, f_grad, x0, D, t_hat=1, epsilon=1e-6, max_iters=10000):\n",
    "    # func_val_record = []\n",
    "    func_val_record = [f(x0)]\n",
    "    grad_norm_record = []\n",
    "    xk = x0\n",
    "    t_s = time()\n",
    "    # for idx in trange(max_iters):\n",
    "    for idx in range(max_iters):\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=t_hat, alpha=0.1, beta=0.5, D=D)\n",
    "        xk_next = xk-tk*f_grad(xk)\n",
    "        fval_xk_next = f(xk_next)\n",
    "        grad_xk_next = f_grad(xk_next)\n",
    "        func_val_record.append(fval_xk_next)\n",
    "        grad_norm_next = np.linalg.norm(grad_xk_next,ord=2)\n",
    "        grad_norm_record.append(grad_norm_next)\n",
    "        # termination criteria\n",
    "        # if grad_norm_next<=epsilon:\n",
    "        #     break\n",
    "        if grad_norm_next<=epsilon or np.linalg.norm(xk_next)>=D/2-1e-3:\n",
    "            break\n",
    "        # if (func_val_record[-1]-func_val_record[-2])/func_val_record[-2]<=epsilon:\n",
    "        #     break\n",
    "        else:\n",
    "            print(f'Iteration {idx} - Grad. Norm.:',grad_norm_next, 'tk:',tk, 'x_norm:',np.linalg.norm(xk_next))\n",
    "        xk = xk_next\n",
    "    t_e = time()\n",
    "    return xk_next, np.asarray(func_val_record), np.asarray(grad_norm_record), t_e-t_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Grad. Norm.: 0.052109680543728935 tk: 5 x_norm: 3.1948158750857765\n",
      "Iteration 1 - Grad. Norm.: 0.03874538795695223 tk: 5 x_norm: 3.3007664609434073\n",
      "Iteration 2 - Grad. Norm.: 0.03247236111795165 tk: 5 x_norm: 3.39832364755234\n",
      "Iteration 3 - Grad. Norm.: 0.028707614758793267 tk: 5 x_norm: 3.488927444461727\n",
      "Iteration 4 - Grad. Norm.: 0.026137955548665787 tk: 5 x_norm: 3.573880284211031\n",
      "Iteration 5 - Grad. Norm.: 0.024238255924168794 tk: 5 x_norm: 3.654164258131663\n",
      "Iteration 6 - Grad. Norm.: 0.02275515238979999 tk: 5 x_norm: 3.730525846831219\n",
      "Iteration 7 - Grad. Norm.: 0.021550498078459098 tk: 5 x_norm: 3.803544343824972\n",
      "Iteration 8 - Grad. Norm.: 0.02054217078675201 tk: 5 x_norm: 3.873677891009221\n",
      "Iteration 9 - Grad. Norm.: 0.01967811317099874 tk: 5 x_norm: 3.9412945794501484\n",
      "Iteration 10 - Grad. Norm.: 0.018923649545407387 tk: 5 x_norm: 4.006694049320368\n",
      "Iteration 11 - Grad. Norm.: 0.018254755248512777 tk: 5 x_norm: 4.070122926376995\n",
      "Iteration 12 - Grad. Norm.: 0.017654243223426912 tk: 5 x_norm: 4.131786136275055\n",
      "Iteration 13 - Grad. Norm.: 0.017109485628512194 tk: 5 x_norm: 4.1918553780763546\n",
      "Iteration 14 - Grad. Norm.: 0.01661099162910515 tk: 5 x_norm: 4.250475586797769\n",
      "Iteration 15 - Grad. Norm.: 0.016151486504397905 tk: 5 x_norm: 4.307769938849529\n",
      "Iteration 16 - Grad. Norm.: 0.015725296588775545 tk: 5 x_norm: 4.3638437802112815\n",
      "Iteration 17 - Grad. Norm.: 0.015327927455063974 tk: 5 x_norm: 4.41878774419224\n",
      "Iteration 18 - Grad. Norm.: 0.01495576795345957 tk: 5 x_norm: 4.472680250221823\n",
      "Iteration 19 - Grad. Norm.: 0.014605878410723272 tk: 5 x_norm: 4.525589523576644\n",
      "Iteration 20 - Grad. Norm.: 0.014275836426987468 tk: 5 x_norm: 4.577575239960449\n",
      "Iteration 21 - Grad. Norm.: 0.013963622905490478 tk: 5 x_norm: 4.628689873249136\n",
      "Iteration 22 - Grad. Norm.: 0.013667536698368668 tk: 5 x_norm: 4.6789798061940004\n",
      "Iteration 23 - Grad. Norm.: 0.013386129933440693 tk: 5 x_norm: 4.728486250282938\n",
      "Iteration 24 - Grad. Norm.: 0.013118158498739279 tk: 5 x_norm: 4.777246010846559\n",
      "Iteration 25 - Grad. Norm.: 0.012862543774248811 tk: 5 x_norm: 4.825292125878381\n",
      "Iteration 26 - Grad. Norm.: 0.012618342799414947 tk: 5 x_norm: 4.872654401231959\n",
      "Iteration 27 - Grad. Norm.: 0.012384724827479992 tk: 5 x_norm: 4.919359860382872\n",
      "Iteration 28 - Grad. Norm.: 0.01216095275544152 tk: 5 x_norm: 4.9654331234576885\n",
      "Iteration 29 - Grad. Norm.: 0.01194636830344556 tk: 5 x_norm: 5.010896727490012\n",
      "Iteration 30 - Grad. Norm.: 0.011740380096867469 tk: 5 x_norm: 5.055771397686673\n",
      "Iteration 31 - Grad. Norm.: 0.011542454009604618 tk: 5 x_norm: 5.100076277744397\n",
      "Iteration 32 - Grad. Norm.: 0.011352105279463776 tk: 5 x_norm: 5.1438291258517665\n",
      "Iteration 33 - Grad. Norm.: 0.011168892020605283 tk: 5 x_norm: 5.187046481870443\n",
      "Iteration 34 - Grad. Norm.: 0.010992409844044392 tk: 5 x_norm: 5.229743810258475\n",
      "Iteration 35 - Grad. Norm.: 0.01082228736248468 tk: 5 x_norm: 5.271935622535017\n",
      "Iteration 36 - Grad. Norm.: 0.010658182405517774 tk: 5 x_norm: 5.313635582457262\n",
      "Iteration 37 - Grad. Norm.: 0.010499778809307676 tk: 5 x_norm: 5.3548565965613575\n",
      "Iteration 38 - Grad. Norm.: 0.010346783674123578 tk: 5 x_norm: 5.395610892289318\n",
      "Iteration 39 - Grad. Norm.: 0.010198925005613414 tk: 5 x_norm: 5.435910085567235\n",
      "Iteration 40 - Grad. Norm.: 0.010055949673115894 tk: 5 x_norm: 5.475765239403506\n",
      "Iteration 41 - Grad. Norm.: 0.00991762163179917 tk: 5 x_norm: 5.515186914828691\n",
      "Iteration 42 - Grad. Norm.: 0.009783720365906778 tk: 5 x_norm: 5.554185215292414\n",
      "Iteration 43 - Grad. Norm.: 0.009654039518584677 tk: 5 x_norm: 5.592769825460382\n",
      "Iteration 44 - Grad. Norm.: 0.009528385680190871 tk: 5 x_norm: 5.630950045210301\n",
      "Iteration 45 - Grad. Norm.: 0.00940657731205772 tk: 5 x_norm: 5.668734819504399\n",
      "Iteration 46 - Grad. Norm.: 0.009288443786697239 tk: 5 x_norm: 5.706132764714663\n",
      "Iteration 47 - Grad. Norm.: 0.009173824528649512 tk: 5 x_norm: 5.743152191891231\n",
      "Iteration 48 - Grad. Norm.: 0.009062568242754069 tk: 5 x_norm: 5.779801127392229\n",
      "Iteration 49 - Grad. Norm.: 0.008954532218712436 tk: 5 x_norm: 5.816087331232378\n",
      "Iteration 50 - Grad. Norm.: 0.008849581702513225 tk: 5 x_norm: 5.852018313456026\n",
      "Iteration 51 - Grad. Norm.: 0.008747589326689507 tk: 5 x_norm: 5.887601348796511\n",
      "Iteration 52 - Grad. Norm.: 0.008648434592534953 tk: 5 x_norm: 5.922843489846482\n",
      "Iteration 53 - Grad. Norm.: 0.008552003398367881 tk: 5 x_norm: 5.9577515789322195\n",
      "Iteration 54 - Grad. Norm.: 0.00845818760873844 tk: 5 x_norm: 5.992332258857884\n",
      "Iteration 55 - Grad. Norm.: 0.008366884660153454 tk: 5 x_norm: 6.026591982662588\n",
      "Iteration 56 - Grad. Norm.: 0.00827799719946767 tk: 5 x_norm: 6.060537022513511\n",
      "Iteration 57 - Grad. Norm.: 0.00819143275157915 tk: 5 x_norm: 6.094173477841201\n",
      "Iteration 58 - Grad. Norm.: 0.008107103413483375 tk: 5 x_norm: 6.127507282808913\n",
      "Iteration 59 - Grad. Norm.: 0.008024925572098265 tk: 5 x_norm: 6.160544213195214\n",
      "Iteration 60 - Grad. Norm.: 0.007944819643579409 tk: 5 x_norm: 6.193289892758523\n",
      "Iteration 61 - Grad. Norm.: 0.007866709832110139 tk: 5 x_norm: 6.2257497991429025\n",
      "Iteration 62 - Grad. Norm.: 0.007790523906379961 tk: 5 x_norm: 6.257929269376674\n",
      "Iteration 63 - Grad. Norm.: 0.007716192992164045 tk: 5 x_norm: 6.289833505008441\n",
      "Iteration 64 - Grad. Norm.: 0.007643651379589124 tk: 5 x_norm: 6.32146757691937\n",
      "Iteration 65 - Grad. Norm.: 0.007572836343822244 tk: 5 x_norm: 6.352836429845363\n",
      "Iteration 66 - Grad. Norm.: 0.007503687978050367 tk: 5 x_norm: 6.383944886638537\n",
      "Iteration 67 - Grad. Norm.: 0.007436149037734645 tk: 5 x_norm: 6.414797652293544\n",
      "Iteration 68 - Grad. Norm.: 0.007370164795224553 tk: 5 x_norm: 6.445399317761025\n",
      "Iteration 69 - Grad. Norm.: 0.007305682903906841 tk: 5 x_norm: 6.4757543635677415\n",
      "Iteration 70 - Grad. Norm.: 0.007242653271142864 tk: 5 x_norm: 6.505867163260412\n",
      "Iteration 71 - Grad. Norm.: 0.007181027939318518 tk: 5 x_norm: 6.535741986688262\n",
      "Iteration 72 - Grad. Norm.: 0.007120760974392532 tk: 5 x_norm: 6.565383003137404\n",
      "Iteration 73 - Grad. Norm.: 0.00706180836138476 tk: 5 x_norm: 6.594794284328683\n",
      "Iteration 74 - Grad. Norm.: 0.0070041279062951845 tk: 5 x_norm: 6.623979807289179\n",
      "Iteration 75 - Grad. Norm.: 0.006947679143988687 tk: 5 x_norm: 6.652943457106436\n",
      "Iteration 76 - Grad. Norm.: 0.006892423251620309 tk: 5 x_norm: 6.681689029573478\n",
      "Iteration 77 - Grad. Norm.: 0.006838322967211188 tk: 5 x_norm: 6.710220233731739\n",
      "Iteration 78 - Grad. Norm.: 0.006785342513017727 tk: 5 x_norm: 6.738540694318314\n",
      "Iteration 79 - Grad. Norm.: 0.006733447523365182 tk: 5 x_norm: 6.766653954123283\n",
      "Iteration 80 - Grad. Norm.: 0.006682604976643397 tk: 5 x_norm: 6.79456347626223\n",
      "Iteration 81 - Grad. Norm.: 0.006632783131185928 tk: 5 x_norm: 6.8222726463686225\n",
      "Iteration 82 - Grad. Norm.: 0.006583951464775524 tk: 5 x_norm: 6.849784774710303\n",
      "Iteration 83 - Grad. Norm.: 0.006536080617538423 tk: 5 x_norm: 6.877103098233852\n",
      "Iteration 84 - Grad. Norm.: 0.006489142338007995 tk: 5 x_norm: 6.904230782540435\n",
      "Iteration 85 - Grad. Norm.: 0.006443109432154281 tk: 5 x_norm: 6.931170923796241\n",
      "Iteration 86 - Grad. Norm.: 0.006397955715191257 tk: 5 x_norm: 6.957926550580559\n",
      "Iteration 87 - Grad. Norm.: 0.006353655965987035 tk: 5 x_norm: 6.984500625674142\n",
      "Iteration 88 - Grad. Norm.: 0.006310185883914801 tk: 5 x_norm: 7.010896047790501\n",
      "Iteration 89 - Grad. Norm.: 0.006267522047993853 tk: 5 x_norm: 7.037115653252374\n",
      "Iteration 90 - Grad. Norm.: 0.0062256418781806155 tk: 5 x_norm: 7.063162217615676\n",
      "Iteration 91 - Grad. Norm.: 0.006184523598679103 tk: 5 x_norm: 7.0890384572429275\n",
      "Iteration 92 - Grad. Norm.: 0.006144146203149559 tk: 5 x_norm: 7.114747030828146\n",
      "Iteration 93 - Grad. Norm.: 0.006104489421701856 tk: 5 x_norm: 7.140290540875032\n",
      "Iteration 94 - Grad. Norm.: 0.006065533689568208 tk: 5 x_norm: 7.1656715351301425\n",
      "Iteration 95 - Grad. Norm.: 0.0060272601173566075 tk: 5 x_norm: 7.190892507972765\n",
      "Iteration 96 - Grad. Norm.: 0.0059896504627929064 tk: 5 x_norm: 7.215955901762982\n",
      "Iteration 97 - Grad. Norm.: 0.005952687103865626 tk: 5 x_norm: 7.2408641081494585\n",
      "Iteration 98 - Grad. Norm.: 0.005916353013292988 tk: 5 x_norm: 7.2656194693383656\n",
      "Iteration 99 - Grad. Norm.: 0.005880631734237009 tk: 5 x_norm: 7.290224279324753\n",
      "Iteration 100 - Grad. Norm.: 0.005845507357194208 tk: 5 x_norm: 7.3146807850877344\n",
      "Iteration 101 - Grad. Norm.: 0.005810964497996975 tk: 5 x_norm: 7.338991187750657\n",
      "Iteration 102 - Grad. Norm.: 0.005776988276863809 tk: 5 x_norm: 7.363157643707488\n",
      "Iteration 103 - Grad. Norm.: 0.005743564298440498 tk: 5 x_norm: 7.38718226571656\n",
      "Iteration 104 - Grad. Norm.: 0.005710678632777918 tk: 5 x_norm: 7.4110671239627255\n",
      "Iteration 105 - Grad. Norm.: 0.005678317797195463 tk: 5 x_norm: 7.434814247089042\n",
      "Iteration 106 - Grad. Norm.: 0.00564646873898221 tk: 5 x_norm: 7.458425623198962\n",
      "Iteration 107 - Grad. Norm.: 0.005615118818890957 tk: 5 x_norm: 7.481903200829988\n",
      "Iteration 108 - Grad. Norm.: 0.005584255795382843 tk: 5 x_norm: 7.505248889899782\n",
      "Iteration 109 - Grad. Norm.: 0.005553867809582866 tk: 5 x_norm: 7.528464562625599\n",
      "Iteration 110 - Grad. Norm.: 0.005523943370908989 tk: 5 x_norm: 7.551552054417913\n",
      "Iteration 111 - Grad. Norm.: 0.0054944713433397645 tk: 5 x_norm: 7.574513164749113\n",
      "Iteration 112 - Grad. Norm.: 0.005465440932287348 tk: 5 x_norm: 7.597349657998026\n",
      "Iteration 113 - Grad. Norm.: 0.005436841672044922 tk: 5 x_norm: 7.620063264271106\n",
      "Iteration 114 - Grad. Norm.: 0.0054086634137791205 tk: 5 x_norm: 7.64265568020099\n",
      "Iteration 115 - Grad. Norm.: 0.005380896314039965 tk: 5 x_norm: 7.6651285697231835\n",
      "Iteration 116 - Grad. Norm.: 0.005353530823762216 tk: 5 x_norm: 7.687483564831559\n",
      "Iteration 117 - Grad. Norm.: 0.005326557677733729 tk: 5 x_norm: 7.709722266313349\n",
      "Iteration 118 - Grad. Norm.: 0.00529996788450755 tk: 5 x_norm: 7.7318462444642835\n",
      "Iteration 119 - Grad. Norm.: 0.005273752716736079 tk: 5 x_norm: 7.753857039784492\n",
      "Iteration 120 - Grad. Norm.: 0.005247903701906571 tk: 5 x_norm: 7.775756163655808\n",
      "Iteration 121 - Grad. Norm.: 0.005222412613458654 tk: 5 x_norm: 7.797545099001002\n",
      "Iteration 122 - Grad. Norm.: 0.0051972714622654 tk: 5 x_norm: 7.81922530092558\n",
      "Iteration 123 - Grad. Norm.: 0.005172472488460624 tk: 5 x_norm: 7.840798197342625\n",
      "Iteration 124 - Grad. Norm.: 0.005148008153596033 tk: 5 x_norm: 7.8622651895812625\n",
      "Iteration 125 - Grad. Norm.: 0.005123871133112667 tk: 5 x_norm: 7.883627652979195\n",
      "Iteration 126 - Grad. Norm.: 0.005100054309111971 tk: 5 x_norm: 7.904886937459868\n",
      "Iteration 127 - Grad. Norm.: 0.005076550763412619 tk: 5 x_norm: 7.926044368094676\n",
      "Iteration 128 - Grad. Norm.: 0.005053353770879928 tk: 5 x_norm: 7.947101245650691\n",
      "Iteration 129 - Grad. Norm.: 0.005030456793015412 tk: 5 x_norm: 7.968058847124352\n",
      "Iteration 130 - Grad. Norm.: 0.005007853471794668 tk: 5 x_norm: 7.988918426261543\n",
      "Iteration 131 - Grad. Norm.: 0.004985537623742439 tk: 5 x_norm: 8.009681214064443\n",
      "Iteration 132 - Grad. Norm.: 0.00496350323423421 tk: 5 x_norm: 8.030348419285577\n",
      "Iteration 133 - Grad. Norm.: 0.00494174445201434 tk: 5 x_norm: 8.050921228909427\n",
      "Iteration 134 - Grad. Norm.: 0.004920255583921091 tk: 5 x_norm: 8.071400808621982\n",
      "Iteration 135 - Grad. Norm.: 0.004899031089809628 tk: 5 x_norm: 8.091788303268565\n",
      "Iteration 136 - Grad. Norm.: 0.004878065577664277 tk: 5 x_norm: 8.112084837300293\n",
      "Iteration 137 - Grad. Norm.: 0.0048573537988919 tk: 5 x_norm: 8.132291515209515\n",
      "Iteration 138 - Grad. Norm.: 0.0048368906437886855 tk: 5 x_norm: 8.152409421954514\n",
      "Iteration 139 - Grad. Norm.: 0.004816671137172886 tk: 5 x_norm: 8.1724396233738\n",
      "Iteration 140 - Grad. Norm.: 0.004796690434176531 tk: 5 x_norm: 8.192383166590304\n",
      "Iteration 141 - Grad. Norm.: 0.004776943816189436 tk: 5 x_norm: 8.212241080405732\n",
      "Iteration 142 - Grad. Norm.: 0.004757426686949174 tk: 5 x_norm: 8.232014375685388\n",
      "Iteration 143 - Grad. Norm.: 0.004738134568770871 tk: 5 x_norm: 8.251704045733728\n",
      "Iteration 144 - Grad. Norm.: 0.004719063098911212 tk: 5 x_norm: 8.271311066660848\n",
      "Iteration 145 - Grad. Norm.: 0.004700208026061033 tk: 5 x_norm: 8.290836397740293\n",
      "Iteration 146 - Grad. Norm.: 0.004681565206961339 tk: 5 x_norm: 8.310280981758263\n",
      "Iteration 147 - Grad. Norm.: 0.0046631306031377695 tk: 5 x_norm: 8.329645745354577\n",
      "Iteration 148 - Grad. Norm.: 0.0046449002777486995 tk: 5 x_norm: 8.348931599355577\n",
      "Iteration 149 - Grad. Norm.: 0.004626870392542518 tk: 5 x_norm: 8.368139439099176\n",
      "Iteration 150 - Grad. Norm.: 0.004609037204919693 tk: 5 x_norm: 8.387270144752287\n",
      "Iteration 151 - Grad. Norm.: 0.004591397065095509 tk: 5 x_norm: 8.406324581620847\n",
      "Iteration 152 - Grad. Norm.: 0.004573946413359559 tk: 5 x_norm: 8.425303600452606\n",
      "Iteration 153 - Grad. Norm.: 0.004556681777428187 tk: 5 x_norm: 8.444208037732885\n",
      "Iteration 154 - Grad. Norm.: 0.004539599769886302 tk: 5 x_norm: 8.463038715973518\n",
      "Iteration 155 - Grad. Norm.: 0.00452269708571512 tk: 5 x_norm: 8.481796443995103\n",
      "Iteration 156 - Grad. Norm.: 0.004505970499902511 tk: 5 x_norm: 8.500482017202804\n",
      "Iteration 157 - Grad. Norm.: 0.004489416865132891 tk: 5 x_norm: 8.519096217855822\n",
      "Iteration 158 - Grad. Norm.: 0.004473033109553527 tk: 5 x_norm: 8.537639815330706\n",
      "Iteration 159 - Grad. Norm.: 0.0044568162346144986 tk: 5 x_norm: 8.556113566378716\n",
      "Iteration 160 - Grad. Norm.: 0.004440763312979452 tk: 5 x_norm: 8.574518215377303\n",
      "Iteration 161 - Grad. Norm.: 0.004424871486504592 tk: 5 x_norm: 8.592854494575944\n",
      "Iteration 162 - Grad. Norm.: 0.0044091379642833385 tk: 5 x_norm: 8.611123124336434\n",
      "Iteration 163 - Grad. Norm.: 0.004393560020754221 tk: 5 x_norm: 8.62932481336775\n",
      "Iteration 164 - Grad. Norm.: 0.004378134993869741 tk: 5 x_norm: 8.647460258955713\n",
      "Iteration 165 - Grad. Norm.: 0.0043628602833239135 tk: 5 x_norm: 8.665530147187466\n",
      "Iteration 166 - Grad. Norm.: 0.004347733348836403 tk: 5 x_norm: 8.683535153170999\n",
      "Iteration 167 - Grad. Norm.: 0.00433275170849119 tk: 5 x_norm: 8.70147594124977\n",
      "Iteration 168 - Grad. Norm.: 0.004317912937127823 tk: 5 x_norm: 8.719353165212606\n",
      "Iteration 169 - Grad. Norm.: 0.004303214664783347 tk: 5 x_norm: 8.737167468498956\n",
      "Iteration 170 - Grad. Norm.: 0.004288654575183136 tk: 5 x_norm: 8.754919484399625\n",
      "Iteration 171 - Grad. Norm.: 0.004274230404278877 tk: 5 x_norm: 8.772609836253098\n",
      "Iteration 172 - Grad. Norm.: 0.004259939938832053 tk: 5 x_norm: 8.79023913763761\n",
      "Iteration 173 - Grad. Norm.: 0.004245781015041309 tk: 5 x_norm: 8.807807992558946\n",
      "Iteration 174 - Grad. Norm.: 0.004231751517212209 tk: 5 x_norm: 8.82531699563423\n",
      "Iteration 175 - Grad. Norm.: 0.004217849376467856 tk: 5 x_norm: 8.842766732271683\n",
      "Iteration 176 - Grad. Norm.: 0.004204072569498997 tk: 5 x_norm: 8.860157778846485\n",
      "Iteration 177 - Grad. Norm.: 0.004190419117352251 tk: 5 x_norm: 8.877490702872857\n",
      "Iteration 178 - Grad. Norm.: 0.0041768870842551304 tk: 5 x_norm: 8.894766063172419\n",
      "Iteration 179 - Grad. Norm.: 0.004163474576476612 tk: 5 x_norm: 8.911984410038947\n",
      "Iteration 180 - Grad. Norm.: 0.00415017974122207 tk: 5 x_norm: 8.929146285399577\n",
      "Iteration 181 - Grad. Norm.: 0.004137000765561341 tk: 5 x_norm: 8.946252222972568\n",
      "Iteration 182 - Grad. Norm.: 0.0041239358753888955 tk: 5 x_norm: 8.963302748421711\n",
      "Iteration 183 - Grad. Norm.: 0.004110983334414956 tk: 5 x_norm: 8.98029837950742\n",
      "Iteration 184 - Grad. Norm.: 0.0040981414431865764 tk: 5 x_norm: 8.997239626234652\n",
      "Iteration 185 - Grad. Norm.: 0.004085408538137675 tk: 5 x_norm: 9.014126990997669\n",
      "Iteration 186 - Grad. Norm.: 0.004072782990667028 tk: 5 x_norm: 9.030960968721734\n",
      "Iteration 187 - Grad. Norm.: 0.004060263206243351 tk: 5 x_norm: 9.047742047001842\n",
      "Iteration 188 - Grad. Norm.: 0.004047847623536561 tk: 5 x_norm: 9.064470706238508\n",
      "Iteration 189 - Grad. Norm.: 0.004035534713574314 tk: 5 x_norm: 9.081147419770717\n",
      "Iteration 190 - Grad. Norm.: 0.0040233229789230765 tk: 5 x_norm: 9.097772654006082\n",
      "Iteration 191 - Grad. Norm.: 0.0040112109528928725 tk: 5 x_norm: 9.114346868548282\n",
      "Iteration 192 - Grad. Norm.: 0.0039991971987649345 tk: 5 x_norm: 9.130870516321844\n",
      "Iteration 193 - Grad. Norm.: 0.003987280309041588 tk: 5 x_norm: 9.147344043694327\n",
      "Iteration 194 - Grad. Norm.: 0.003975458904717568 tk: 5 x_norm: 9.163767890595956\n",
      "Iteration 195 - Grad. Norm.: 0.003963731634572135 tk: 5 x_norm: 9.180142490636797\n",
      "Iteration 196 - Grad. Norm.: 0.003952097174481336 tk: 5 x_norm: 9.196468271221502\n",
      "Iteration 197 - Grad. Norm.: 0.003940554226749706 tk: 5 x_norm: 9.212745653661653\n",
      "Iteration 198 - Grad. Norm.: 0.003929101519460888 tk: 5 x_norm: 9.22897505328586\n",
      "Iteration 199 - Grad. Norm.: 0.003917737805846478 tk: 5 x_norm: 9.245156879547515\n",
      "Iteration 200 - Grad. Norm.: 0.003906461863672607 tk: 5 x_norm: 9.261291536130411\n",
      "Iteration 201 - Grad. Norm.: 0.003895272494643651 tk: 5 x_norm: 9.277379421052132\n",
      "Iteration 202 - Grad. Norm.: 0.003884168523822536 tk: 5 x_norm: 9.293420926765386\n",
      "Iteration 203 - Grad. Norm.: 0.003873148799067174 tk: 5 x_norm: 9.309416440257232\n",
      "Iteration 204 - Grad. Norm.: 0.003862212190482467 tk: 5 x_norm: 9.325366343146303\n",
      "Iteration 205 - Grad. Norm.: 0.003851357589887427 tk: 5 x_norm: 9.341271011778074\n",
      "Iteration 206 - Grad. Norm.: 0.003840583910296955 tk: 5 x_norm: 9.357130817318152\n",
      "Iteration 207 - Grad. Norm.: 0.0038298900854178133 tk: 5 x_norm: 9.372946125843725\n",
      "Iteration 208 - Grad. Norm.: 0.0038192750691583787 tk: 5 x_norm: 9.388717298433141\n",
      "Iteration 209 - Grad. Norm.: 0.003808737835151722 tk: 5 x_norm: 9.404444691253678\n",
      "Iteration 210 - Grad. Norm.: 0.003798277376291648 tk: 5 x_norm: 9.420128655647579\n",
      "Iteration 211 - Grad. Norm.: 0.0037878927042812854 tk: 5 x_norm: 9.43576953821631\n",
      "Iteration 212 - Grad. Norm.: 0.003777582849193835 tk: 5 x_norm: 9.451367680903184\n",
      "Iteration 213 - Grad. Norm.: 0.003767346859045158 tk: 5 x_norm: 9.466923421074299\n",
      "Iteration 214 - Grad. Norm.: 0.003757183799377783 tk: 5 x_norm: 9.482437091597857\n",
      "Iteration 215 - Grad. Norm.: 0.0037470927528560558 tk: 5 x_norm: 9.497909020921941\n",
      "Iteration 216 - Grad. Norm.: 0.0037370728188720584 tk: 5 x_norm: 9.513339533150699\n",
      "Iteration 217 - Grad. Norm.: 0.0037271231131619877 tk: 5 x_norm: 9.528728948119044\n",
      "Iteration 218 - Grad. Norm.: 0.0037172427674327076 tk: 5 x_norm: 9.544077581465874\n",
      "Iteration 219 - Grad. Norm.: 0.003707430928998128 tk: 5 x_norm: 9.559385744705834\n",
      "Iteration 220 - Grad. Norm.: 0.00369768676042518 tk: 5 x_norm: 9.574653745299665\n",
      "Iteration 221 - Grad. Norm.: 0.003688009439189058 tk: 5 x_norm: 9.589881886723186\n",
      "Iteration 222 - Grad. Norm.: 0.0036783981573374775 tk: 5 x_norm: 9.605070468534876\n",
      "Iteration 223 - Grad. Norm.: 0.00366885212116371 tk: 5 x_norm: 9.620219786442187\n",
      "Iteration 224 - Grad. Norm.: 0.003659370550888077 tk: 5 x_norm: 9.635330132366512\n",
      "Iteration 225 - Grad. Norm.: 0.0036499526803477284 tk: 5 x_norm: 9.650401794506912\n",
      "Iteration 226 - Grad. Norm.: 0.0036405977566944386 tk: 5 x_norm: 9.66543505740259\n",
      "Iteration 227 - Grad. Norm.: 0.0036313050401001637 tk: 5 x_norm: 9.680430201994136\n",
      "Iteration 228 - Grad. Norm.: 0.0036220738034701853 tk: 5 x_norm: 9.695387505683604\n",
      "Iteration 229 - Grad. Norm.: 0.00361290333216359 tk: 5 x_norm: 9.71030724239342\n",
      "Iteration 230 - Grad. Norm.: 0.003603792923720884 tk: 5 x_norm: 9.725189682624134\n",
      "Iteration 231 - Grad. Norm.: 0.003594741887598559 tk: 5 x_norm: 9.740035093511064\n",
      "Iteration 232 - Grad. Norm.: 0.003585749544910379 tk: 5 x_norm: 9.75484373887986\n",
      "Iteration 233 - Grad. Norm.: 0.003576815228175211 tk: 5 x_norm: 9.769615879300982\n",
      "Iteration 234 - Grad. Norm.: 0.0035679382810712527 tk: 5 x_norm: 9.78435177214314\n",
      "Iteration 235 - Grad. Norm.: 0.0035591180581964043 tk: 5 x_norm: 9.7990516716257\n",
      "Iteration 236 - Grad. Norm.: 0.0035503539248346832 tk: 5 x_norm: 9.8137158288701\n",
      "Iteration 237 - Grad. Norm.: 0.0035416452567284683 tk: 5 x_norm: 9.828344491950283\n",
      "Iteration 238 - Grad. Norm.: 0.003532991439856431 tk: 5 x_norm: 9.842937905942149\n",
      "Iteration 239 - Grad. Norm.: 0.0035243918702169935 tk: 5 x_norm: 9.857496312972092\n",
      "Iteration 240 - Grad. Norm.: 0.0035158459536171406 tk: 5 x_norm: 9.8720199522646\n",
      "Iteration 241 - Grad. Norm.: 0.0035073531054664897 tk: 5 x_norm: 9.886509060188947\n",
      "Iteration 242 - Grad. Norm.: 0.0034989127505763986 tk: 5 x_norm: 9.900963870305036\n",
      "Iteration 243 - Grad. Norm.: 0.0034905243229640453 tk: 5 x_norm: 9.915384613408317\n",
      "Iteration 244 - Grad. Norm.: 0.003482187265661298 tk: 5 x_norm: 9.929771517573922\n",
      "Iteration 245 - Grad. Norm.: 0.003473901030528257 tk: 5 x_norm: 9.944124808199922\n",
      "Iteration 246 - Grad. Norm.: 0.0034656650780713503 tk: 5 x_norm: 9.958444708049793\n",
      "Iteration 247 - Grad. Norm.: 0.003457478877265842 tk: 5 x_norm: 9.972731437294097\n",
      "Iteration 248 - Grad. Norm.: 0.003449341905382658 tk: 5 x_norm: 9.986985213551353\n",
      "Iteration 249 - Grad. Norm.: 0.003445294909853667 tk: 2.5 x_norm: 9.994094541898372\n",
      "Iteration 250 - Grad. Norm.: 0.0034432767417426333 tk: 1.25 x_norm: 9.997644843128413\n",
      "最小值: 0.095011\t耗时: 26.128632s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "init_x = np.zeros(n)+0.005\n",
    "x_opt_gd, _, _, t_gd = gradient_descent(f=f, f_grad=f_grad, x0=init_x, D=20, t_hat=5, epsilon=1e-6, max_iters=300)\n",
    "optim_val = f(x_opt_gd)\n",
    "print(f'最小值: {optim_val:>2f}\\t耗时: {t_gd:>2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09501128827728388"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_opt_gd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton (Interior Point Methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logarithmic barrier:\n",
    "$$\n",
    "\\phi(x)=-\\log(-g(x)),\\quad g(x)=\\left\\|x\\right\\|_2 - D/2\n",
    "$$\n",
    "$$\n",
    "\\nabla g(x) = x/\\left\\|x\\right\\|_2, \\quad \\nabla^2 g(x)= \\left\\|x\\right\\|_2^{-1}I-\\left\\|x\\right\\|_2^{-3}xx^\\top\n",
    "$$\n",
    "gradient and hessian:\n",
    "$$\n",
    "\\nabla \\phi(x) = \\frac{1}{-g(x)}\\nabla g(x)=\\frac{x/\\left\\|x\\right\\|_2}{D/2-\\left\\|x\\right\\|_2}=\\frac{x}{\\left\\|x\\right\\|_2(D/2-\\left\\|x\\right\\|_2)}\n",
    "$$\n",
    "$$\n",
    "\\nabla^2\\phi(x)=\\frac{1}{g(x)^2}\\nabla g(x)\\nabla g(x)^\\top + \\frac{1}{-g(x)}\\nabla^2 g(x) = \\frac{1}{\\left\\|x\\right\\|_2(D/2-\\left\\|x\\right\\|_2)}I+\\frac{2\\left\\|x\\right\\|_2-D/2}{\\left\\|x\\right\\|_2^3(D/2-\\left\\|x\\right\\|_2)^2}xx^\\top\n",
    "$$\n",
    "central path - for $t>0$:\n",
    "$$\n",
    "\\min_x tf(x)+\\phi(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x,D):\n",
    "    return -np.log(D/2-np.linalg.norm(x,ord=2))\n",
    "\n",
    "def phi_grad(x,D):\n",
    "    x_norm = np.linalg.norm(x,ord=2)\n",
    "    return x/(x_norm*(D/2-x_norm))\n",
    "\n",
    "def phi_hessian(x,D):\n",
    "    x_norm = np.linalg.norm(x,ord=2)\n",
    "    xxT = np.matmul(x[:,None],x[None,:])    # x * xT\n",
    "    return np.eye(x.size)/(x_norm*(D/2-x_norm)) + (2*x_norm-D/2)/(x_norm**3 * (D/2-x_norm)**2)*xxT\n",
    "\n",
    "\n",
    "#* 外部迭代\n",
    "def barrier_method(t_init, f, f_grad, f_hessian, phi, phi_grad, phi_hessian, A, b, x0, D, num_constraints, mu,\n",
    "                        method='newton', epsilon=1e-6, maxIter=20):\n",
    "    xt = x0\n",
    "    t = t_init\n",
    "    duality_gaps = []\n",
    "    t_s = time()\n",
    "    for i in range(maxIter):\n",
    "        xt,num_newton_step = solve_central(f=lambda x:t*f(x)+phi(x,D), \n",
    "                                f_grad=lambda x:t*f_grad(x)+phi_grad(x,D), \n",
    "                                f_hessian=lambda x:t*f_hessian(x)+phi_hessian(x,D),\n",
    "                                x0=xt, D=D, method=method, epsilon=epsilon*1e3)\n",
    "        duality_gaps.extend([num_constraints/t]*num_newton_step)\n",
    "        if num_constraints/t < epsilon:\n",
    "            break\n",
    "        t *= mu\n",
    "    t_e = time()\n",
    "    return xt, t_e-t_s, duality_gaps\n",
    "\n",
    "def solve_central(f, f_grad, f_hessian, x0, D, method='newton', epsilon=1e-6, max_iter=50):\n",
    "    if method == 'newton':\n",
    "        return damped_newton(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=x0, D=D, epsilon=epsilon, max_iter=max_iter)\n",
    "    if method == 'bfgs':\n",
    "        return bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=x0, D=D, epsilon=epsilon, max_iter=max_iter)\n",
    "\n",
    "\n",
    "#* 阻尼牛顿\n",
    "def damped_newton(f, f_grad, f_hessian, x0, D, epsilon=1e-6, max_iter=50):\n",
    "    xk = x0\n",
    "    iter_cnt = 0\n",
    "    for idx in range(max_iter):\n",
    "        iter_cnt += 1\n",
    "        grad = f_grad(xk)\n",
    "        hessian = f_hessian(xk)\n",
    "        dk = -np.linalg.inv(hessian)@grad\n",
    "        decrement = (-grad@dk)**0.5\n",
    "        if decrement**2/2 <= epsilon:\n",
    "            print('** End The Loop - Iter Cnt.:',iter_cnt, 'Decrement:',decrement, 'fval:',f(xk))\n",
    "            return xk, iter_cnt\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=1, alpha=0.1, beta=0.5, D=D, isNewton=True, dk=dk)\n",
    "        print('Iter Cnt.:',iter_cnt, 'Decrement:',decrement, 'fval:',f(xk), 'tk:',tk)\n",
    "        xk += tk*dk\n",
    "    return xk, iter_cnt\n",
    "\n",
    "#* 拟牛顿\n",
    "def bfgs(f, f_grad, f_hessian, x0, D, alpha=0.1, beta=0.5, epsilon=1e-6, max_iter=500):\n",
    "    xk = x0\n",
    "    hessian = f_hessian(x0)\n",
    "    mat_k = np.linalg.inv(hessian) \n",
    "    # mat_k = np.eye(n) \n",
    "    iter_cnt = 0\n",
    "    for idx in range(max_iter):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        dk = -mat_k@grad_k \n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, D, c1=1e-4, c2=0.9)\n",
    "        if tk<0:\n",
    "            return xk, iter_cnt-1\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        # if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon or np.linalg.norm(xk_next)>=D/2-1e-2:\n",
    "            return xk_next, iter_cnt\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        mat_k = update_approximation_bfgs(mat=mat_k, sk=sk, yk=grad_next-grad_k)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt\n",
    "        \n",
    "def update_approximation_bfgs(mat, sk, yk, mat_type='H'):\n",
    "    rhok = 1/(yk@sk)\n",
    "    if mat_type == 'H':\n",
    "        Hkyk = mat@yk\n",
    "        ykTHkyk = yk@Hkyk\n",
    "        HkykskT = Hkyk[:,None]@sk[None,:]\n",
    "        skskT = sk[:,None]@sk[None,:]\n",
    "        mat_new = mat + rhok*((rhok*ykTHkyk+1)*skskT - HkykskT - HkykskT.T)\n",
    "    else:\n",
    "        Bksk = mat@sk\n",
    "        skTBksk = sk@Bksk\n",
    "        mat_new = mat - Bksk[:,None]@Bksk[None,:]/skTBksk + yk[:,None]@yk[None,:]*rhok\n",
    "    return mat_new\n",
    "\n",
    "#* 拟牛顿方法 - 选择步长\n",
    "def wolfe_condition(f, f_grad, xk, pk, D, c1=1e-4, c2=0.9, multiplier=1.2, t0=0, tmax=2):\n",
    "    ### \n",
    "    while (np.linalg.norm(xk+tmax*pk)>=D/2):\n",
    "        tmax /= 2\n",
    "        # print('tmax:',tmax)\n",
    "        if tmax<1e-6:\n",
    "            # print('too small stepsize')\n",
    "            return -1\n",
    "    ###\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "def zoom(f, f_grad, xk, pk, fval, grad, c1, c2, t_lo, t_hi):\n",
    "    while True:\n",
    "        # print(f\"t_lo: {t_lo}\\tt_hi: {t_hi}\")\n",
    "        t = (t_lo+t_hi)/2\n",
    "        xk_next = xk + t*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if fval_next > fval + c1*t*grad@pk or fval_next >= f(xk+t_lo*pk):\n",
    "            t_hi = t\n",
    "        else:\n",
    "            grad_next = f_grad(xk_next)\n",
    "            grad_next_T_pk = grad_next@pk\n",
    "            if np.abs(grad_next_T_pk) <= -c2*grad@pk:\n",
    "                return t\n",
    "            if grad_next_T_pk*(t_hi-t_lo)>=0:\n",
    "                t_hi = t_lo\n",
    "            t_lo = t\n",
    "        if t_lo == t_hi: # 死循环\n",
    "            return -1\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter Cnt.: 1 Decrement: 0.9525578038914555 fval: -1.569533749629282 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6084895374423559 fval: -1.7498984994316462 tk: 0.5\n",
      "Iter Cnt.: 3 Decrement: 0.12992597126245592 fval: -1.8633527068729119 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.02321306437254386 fval: -1.8726656070316618\n",
      "Iter Cnt.: 1 Decrement: 1.0792049141682987 fval: 0.647848502583277 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6100747846231196 fval: -0.08446265646887308 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.23545159529444123 fval: -0.3070194485297655 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.03707000873467996 fval: -0.33731360473380945\n",
      "Iter Cnt.: 1 Decrement: 2.185111949659234 fval: 12.3868314478077 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6861200416531448 fval: 9.862690130100074 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.1074930906532772 fval: 9.60733766618385 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.004044385787036891 fval: 9.601432568933406\n",
      "Iter Cnt.: 1 Decrement: 4.7492332061745826 fval: 101.25102919604771 tk: 0.25\n",
      "Iter Cnt.: 2 Decrement: 1.5063942741053518 fval: 96.55952964013056 tk: 0.5\n",
      "Iter Cnt.: 3 Decrement: 0.2516796064492284 fval: 95.84917578348347 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.01932715163629368 fval: 95.81665806667954\n",
      "Iter Cnt.: 1 Decrement: 7.758057290120473 fval: 945.2189941916731 tk: 0.125\n",
      "Iter Cnt.: 2 Decrement: 0.4660397496437592 fval: 938.9329615227451 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.14794578373910416 fval: 938.8530207878181 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.020506191622691856 fval: 938.8411657445458\n",
      "Iter Cnt.: 1 Decrement: 8.68203280676348 fval: 9355.058817088458 tk: 0.0625\n",
      "Iter Cnt.: 2 Decrement: 3.5343922841032853 fval: 9350.582936507835 tk: 0.25\n",
      "Iter Cnt.: 3 Decrement: 0.45948028913901945 fval: 9348.706638986409 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.21033478846173254 fval: 9348.576998294318 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.0441416320702611 fval: 9348.552201230903\n",
      "Iter Cnt.: 1 Decrement: 8.547191160550971 fval: 93431.25018213892 tk: 0.0625\n",
      "Iter Cnt.: 2 Decrement: 3.4575580534280257 fval: 93426.9136562485 tk: 0.25\n",
      "Iter Cnt.: 3 Decrement: 0.39430599106298314 fval: 93425.05666820492 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.15534873841802843 fval: 93424.96311223789 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.024121528570717442 fval: 93424.9499260611\n",
      "Iter Cnt.: 1 Decrement: 8.757583002532396 fval: 934174.6931805913 tk: 0.0625\n",
      "Iter Cnt.: 2 Decrement: 3.4179025244706693 fval: 934170.1449633552 tk: 0.25\n",
      "Iter Cnt.: 3 Decrement: 0.3569769006429667 fval: 934168.2972183257 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.12741973345093416 fval: 934168.2215044968 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.016234885295921552 fval: 934168.2127568781\n",
      "最小值: 0.093416\t耗时: 4.598532s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "# D=20\n",
    "t_init = 1\n",
    "x0 = np.zeros(n)+0.005\n",
    "x_opt_ipm_damped, t_ipm_damped, duality_gaps_damped = barrier_method(t_init=t_init, f=f, f_grad=f_grad, f_hessian=f_hessian, phi=phi, phi_grad=phi_grad, phi_hessian=phi_hessian, \n",
    "                A=A, b=b, x0=x0, D=20, num_constraints=1, method='newton', mu=10, epsilon=1e-6, maxIter=20)\n",
    "print(f'最小值: {f(x_opt_ipm_damped):>2f}\\t耗时: {t_ipm_damped:>2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.16436546152681786 tk: 1.0 x_norm: 1.895870645724538\n",
      "Iteration 2 - grad_norm: 0.14891025638579403 tk: 1.2 x_norm: 1.6123917628543183\n",
      "Iteration 3 - grad_norm: 0.07502011871157341 tk: 0.5 x_norm: 0.9970644876961946\n",
      "Iteration 4 - grad_norm: 0.05610577996870103 tk: 1.0 x_norm: 1.0941712317097694\n",
      "Iteration 5 - grad_norm: 0.014832457759573174 tk: 1.0 x_norm: 1.3777964276903911\n",
      "Iteration 6 - grad_norm: 0.010120591705236754 tk: 1.0 x_norm: 1.4373307072874\n",
      "Iteration 7 - grad_norm: 0.008785266082478564 tk: 1.0 x_norm: 1.4558377622885526\n",
      "Iteration 8 - grad_norm: 0.006119698831746697 tk: 1.0 x_norm: 1.4713877021416755\n",
      "Iteration 9 - grad_norm: 0.0031432242900151616 tk: 1.0 x_norm: 1.4686542555331454\n",
      "Iteration 10 - grad_norm: 0.0015329498593511898 tk: 1.0 x_norm: 1.4568608633253517\n",
      "Iteration 11 - grad_norm: 0.0012077880680353415 tk: 1.0 x_norm: 1.4511717558898638\n",
      "Iteration 12 - grad_norm: 0.001027187177675304 tk: 1.0 x_norm: 1.4491015164638759\n",
      "Iteration 1 - grad_norm: 0.37897369409781073 tk: 1.0 x_norm: 2.6716247250573217\n",
      "Iteration 2 - grad_norm: 0.19103101073342035 tk: 1.0 x_norm: 3.3781214992859194\n",
      "Iteration 3 - grad_norm: 0.06582550066550386 tk: 1.0 x_norm: 4.022965750889254\n",
      "Iteration 4 - grad_norm: 0.02000640938903534 tk: 1.0 x_norm: 4.279907977435319\n",
      "Iteration 5 - grad_norm: 0.00721373677400255 tk: 1.0 x_norm: 4.340148399787634\n",
      "Iteration 6 - grad_norm: 0.004324818616191462 tk: 1.0 x_norm: 4.339109585529465\n",
      "Iteration 7 - grad_norm: 0.002451388127396861 tk: 1.0 x_norm: 4.324917840136136\n",
      "Iteration 8 - grad_norm: 0.0012405472026928452 tk: 1.0 x_norm: 4.317989629676852\n",
      "Iteration 1 - grad_norm: 0.9369479151549006 tk: 0.5 x_norm: 6.15267366704988\n",
      "Iteration 2 - grad_norm: 0.5634705426245702 tk: 0.5 x_norm: 7.329215158895444\n",
      "Iteration 3 - grad_norm: 0.10944500176601571 tk: 1.0 x_norm: 8.371728209845907\n",
      "Iteration 4 - grad_norm: 0.05280962811543018 tk: 1.0 x_norm: 8.11705849342592\n",
      "Iteration 5 - grad_norm: 0.030415371883788872 tk: 1.0 x_norm: 8.186857560658796\n",
      "Iteration 6 - grad_norm: 0.017508852202837284 tk: 1.0 x_norm: 8.219009126919097\n",
      "Iteration 7 - grad_norm: 0.0075133818810660825 tk: 1.0 x_norm: 8.216061865604626\n",
      "Iteration 8 - grad_norm: 0.001340380112068907 tk: 1.0 x_norm: 8.208845480131444\n",
      "Iteration 1 - grad_norm: 4.322107438530214 tk: 0.125 x_norm: 8.796834487492438\n",
      "Iteration 2 - grad_norm: 3.5118232407719683 tk: 0.125 x_norm: 9.25065685097002\n",
      "Iteration 3 - grad_norm: 2.7713303580466757 tk: 0.125 x_norm: 9.470911011967987\n",
      "Iteration 4 - grad_norm: 1.5470289871640288 tk: 0.25 x_norm: 9.654831243792751\n",
      "Iteration 5 - grad_norm: 0.16764841411077577 tk: 0.5 x_norm: 9.756881970626992\n",
      "Iteration 6 - grad_norm: 0.07257469678303852 tk: 1.0 x_norm: 9.754279962435104\n",
      "Iteration 7 - grad_norm: 0.036219703009681935 tk: 0.5 x_norm: 9.758335600568904\n",
      "Iteration 8 - grad_norm: 0.03340843693411504 tk: 0.5 x_norm: 9.760403428493149\n",
      "Iteration 9 - grad_norm: 0.008991311842119412 tk: 0.5 x_norm: 9.758971567713655\n",
      "Iteration 10 - grad_norm: 0.010167210266545287 tk: 0.5 x_norm: 9.758272034660449\n",
      "Iteration 11 - grad_norm: 0.004981563166981456 tk: 0.5 x_norm: 9.758448716929273\n",
      "Iteration 12 - grad_norm: 0.0013525759945646363 tk: 0.5 x_norm: 9.758683469552656\n",
      "Iteration 13 - grad_norm: 0.0013849749422579277 tk: 0.5 x_norm: 9.758723594700347\n",
      "Iteration 1 - grad_norm: 33.565102325860074 tk: 0.0625 x_norm: 9.863697887592807\n",
      "Iteration 2 - grad_norm: 27.673734162209993 tk: 0.0625 x_norm: 9.922530042982562\n",
      "Iteration 3 - grad_norm: 17.339054623722927 tk: 0.125 x_norm: 9.956491767035814\n",
      "Iteration 4 - grad_norm: 6.245752487869007 tk: 0.25 x_norm: 9.970477307596576\n",
      "Iteration 5 - grad_norm: 1.0520143018555663 tk: 0.5 x_norm: 9.974302196042903\n",
      "Iteration 6 - grad_norm: 0.1512116470555265 tk: 0.25 x_norm: 9.974880861373796\n",
      "Iteration 7 - grad_norm: 0.20836123720456382 tk: 1.0 x_norm: 9.97505837917602\n",
      "Iteration 8 - grad_norm: 0.19179521975429206 tk: 0.25 x_norm: 9.975046582116324\n",
      "Iteration 9 - grad_norm: 0.04708056757578243 tk: 0.5 x_norm: 9.974957892286785\n",
      "Iteration 10 - grad_norm: 0.08991912185314754 tk: 0.5 x_norm: 9.974885820392672\n",
      "Iteration 11 - grad_norm: 0.08057421155201498 tk: 0.25 x_norm: 9.974892136524838\n",
      "Iteration 12 - grad_norm: 0.01006769948033861 tk: 0.5 x_norm: 9.974945316102483\n",
      "Iteration 13 - grad_norm: 0.02662949329134319 tk: 0.5 x_norm: 9.974956612450665\n",
      "Iteration 14 - grad_norm: 0.011437620004993355 tk: 0.5 x_norm: 9.974945218269946\n",
      "Iteration 15 - grad_norm: 0.007220000357902804 tk: 0.5 x_norm: 9.974935427407608\n",
      "Iteration 16 - grad_norm: 0.006460160579116465 tk: 0.5 x_norm: 9.97493640098992\n",
      "Iteration 17 - grad_norm: 0.0027677591015975397 tk: 0.5 x_norm: 9.97494061107029\n",
      "Iteration 18 - grad_norm: 0.002198951839202836 tk: 0.5 x_norm: 9.97494135768509\n",
      "Iteration 19 - grad_norm: 0.0017196036482067336 tk: 0.5 x_norm: 9.974940538911618\n",
      "Iteration 20 - grad_norm: 0.0011119312641601025 tk: 0.5 x_norm: 9.974939804871173\n",
      "Iteration 21 - grad_norm: 0.001009941277284192 tk: 0.5 x_norm: 9.974939651169079\n",
      "Iteration 1 - grad_norm: 322.912326472404 tk: 0.054 x_norm: 9.986764512495123\n",
      "最小值: 0.093416\t耗时: 10.437220s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "# D=20\n",
    "t_init = 1\n",
    "x0 = np.zeros(n)+0.005\n",
    "x_opt_ipm_bfgs, t_ipm_bfgs, duality_gaps_bfgs = barrier_method(t_init=t_init, f=f, f_grad=f_grad, f_hessian=f_hessian, phi=phi, phi_grad=phi_grad, phi_hessian=phi_hessian, \n",
    "                A=A, b=b, x0=x0, D=20, num_constraints=1, method='bfgs', mu=10, epsilon=1e-6, maxIter=20)\n",
    "print(f'最小值: {f(x_opt_ipm_bfgs):>2f}\\t耗时: {t_ipm_bfgs:>2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.142308002552664e-06"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_opt_ipm_bfgs)-f(x_opt_ipm_damped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure damped newton w/o constraints (D=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_norm: 0.18017720987362976 tk: 1 x_norm: 11.165278301280532\n",
      "grad_norm: 0.07430235763542076 tk: 1 x_norm: 18.78206938371404\n",
      "grad_norm: 0.03121190141848213 tk: 1 x_norm: 27.44040477856484\n",
      "grad_norm: 0.012796497780903252 tk: 1 x_norm: 36.77727415441325\n",
      "grad_norm: 0.004894660242605477 tk: 1 x_norm: 45.9257331177887\n",
      "grad_norm: 0.0016353055256952348 tk: 1 x_norm: 54.13674948486485\n",
      "grad_norm: 0.0004390302509728913 tk: 1 x_norm: 60.10520617618396\n",
      "grad_norm: 9.125726807373675e-05 tk: 1 x_norm: 63.53445177996058\n",
      "grad_norm: 2.4768185298650292e-05 tk: 1 x_norm: 65.32760767837152\n",
      "grad_norm: 6.8735057881137935e-06 tk: 1 x_norm: 66.52694639503159\n",
      "grad_norm: 1.2104066887873718e-06 tk: 1 x_norm: 67.1480388626449\n",
      "grad_norm: 9.161945407374053e-08 tk: 1 x_norm: 67.27924417413054\n",
      "最小值: 0.058278\t耗时: 2.188134s\n"
     ]
    }
   ],
   "source": [
    "#* pure阻尼牛顿\n",
    "def pure_damped_newton(f, f_grad, f_hessian, x0, D, epsilon=1e-6, max_iters=100):\n",
    "    func_val_record = []\n",
    "    grad_norm_record = []\n",
    "    xk = x0\n",
    "    t_s = time()\n",
    "    # for idx in trange(max_iters):\n",
    "    for idx in range(max_iters):\n",
    "        grad = f_grad(xk)\n",
    "        # print(np.linalg.norm(grad))\n",
    "        hessian = f_hessian(xk)\n",
    "        # dk = -np.linalg.pinv(hessian)@grad\n",
    "        dk = -np.linalg.inv(hessian)@grad\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=1, alpha=0.1, beta=0.5, D=D, isNewton=True, dk=dk)\n",
    "        xk_next = xk + tk*dk\n",
    "        func_val_record.append(f(xk_next))\n",
    "        # grad_norm_record.append(np.linalg.norm(f_grad(xk_next),ord=2))\n",
    "        grad_next = np.linalg.norm(f_grad(xk_next),ord=2)\n",
    "        grad_norm_record.append(grad_next)\n",
    "        # termination criteria\n",
    "        if grad_next<=epsilon:\n",
    "            break\n",
    "        else:\n",
    "            print('grad_norm:',grad_next,\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        xk = xk_next\n",
    "    t_e = time()\n",
    "    return xk_next, np.asarray(func_val_record), np.asarray(grad_norm_record), t_e-t_s\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "# init_x = np.zeros(n)+0.5\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# D=500 半径足够大，本质上是无约束\n",
    "x_opt_damped, _, _, t_damped = pure_damped_newton(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, D=500, epsilon=1e-8, max_iters=50)\n",
    "print(f'最小值: {f(x_opt_damped):>2f}\\t耗时: {t_damped:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure quasi newton w/o constraints (D=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.18017720987362976 tk: 1.0 x_norm: 11.165278301280532\n",
      "Iteration 2 - grad_norm: 0.11332750219773532 tk: 1.0 x_norm: 15.135338733342227\n",
      "Iteration 3 - grad_norm: 0.0588887609960591 tk: 1.0 x_norm: 21.533528580786278\n",
      "Iteration 4 - grad_norm: 0.03370457453496356 tk: 1.0 x_norm: 28.007182606015498\n",
      "Iteration 5 - grad_norm: 0.018882899036194684 tk: 1.0 x_norm: 35.87806759864599\n",
      "Iteration 6 - grad_norm: 0.010866841039356227 tk: 1.0 x_norm: 44.44864398760293\n",
      "Iteration 7 - grad_norm: 0.006490045202947455 tk: 1.0 x_norm: 53.327850892309826\n",
      "Iteration 8 - grad_norm: 0.004300271670796263 tk: 1.0 x_norm: 61.34159830918189\n",
      "Iteration 9 - grad_norm: 0.0033286798522448226 tk: 1.0 x_norm: 67.46185331330766\n",
      "Iteration 10 - grad_norm: 0.0029130781154975767 tk: 1.0 x_norm: 71.52202526197196\n",
      "Iteration 11 - grad_norm: 0.0026709621915537753 tk: 1.0 x_norm: 74.55496886554727\n",
      "Iteration 12 - grad_norm: 0.0024711794717667194 tk: 1.0 x_norm: 77.01301640920776\n",
      "Iteration 13 - grad_norm: 0.0022580722379469745 tk: 1.0 x_norm: 77.61081896031442\n",
      "Iteration 14 - grad_norm: 0.0019611305006011176 tk: 1.0 x_norm: 74.87516342684319\n",
      "Iteration 15 - grad_norm: 0.001815015621707459 tk: 1.0 x_norm: 69.7557515224829\n",
      "Iteration 16 - grad_norm: 0.0019063176832953225 tk: 1.0 x_norm: 64.42711104360677\n",
      "Iteration 17 - grad_norm: 0.0018763345697549107 tk: 1.0 x_norm: 61.94319651661975\n",
      "Iteration 18 - grad_norm: 0.0017208657496739978 tk: 1.0 x_norm: 61.616147745504676\n",
      "Iteration 19 - grad_norm: 0.001386078159312382 tk: 1.0 x_norm: 62.89409076494536\n",
      "Iteration 20 - grad_norm: 0.0011635196375529271 tk: 1.0 x_norm: 65.10222137327875\n",
      "Iteration 21 - grad_norm: 0.0010389973610115406 tk: 1.0 x_norm: 67.10553071463985\n",
      "Iteration 22 - grad_norm: 0.0009588307486315539 tk: 1.0 x_norm: 68.51480003888817\n",
      "Iteration 23 - grad_norm: 0.0008895651592463788 tk: 1.0 x_norm: 69.19359689165321\n",
      "Iteration 24 - grad_norm: 0.0008186623859286282 tk: 1.0 x_norm: 68.98677847889323\n",
      "Iteration 25 - grad_norm: 0.0007557517882971561 tk: 1.0 x_norm: 67.95288736419309\n",
      "Iteration 26 - grad_norm: 0.0007121398766398684 tk: 1.0 x_norm: 66.85210538322347\n",
      "Iteration 27 - grad_norm: 0.0006733727089787008 tk: 1.0 x_norm: 66.27575818961851\n",
      "Iteration 28 - grad_norm: 0.0006374724502729076 tk: 1.0 x_norm: 66.21837447067597\n",
      "Iteration 29 - grad_norm: 0.0006106643068685932 tk: 1.0 x_norm: 66.6604945874876\n",
      "Iteration 30 - grad_norm: 0.0005820467844169081 tk: 1.0 x_norm: 67.3594410816704\n",
      "Iteration 31 - grad_norm: 0.0005408033541224585 tk: 1.0 x_norm: 67.9537369898761\n",
      "Iteration 32 - grad_norm: 0.000496140086031849 tk: 1.0 x_norm: 68.26786077567732\n",
      "Iteration 33 - grad_norm: 0.00047270438200389266 tk: 1.0 x_norm: 68.15910372645311\n",
      "Iteration 34 - grad_norm: 0.00048387612497401426 tk: 1.0 x_norm: 67.58414815459903\n",
      "Iteration 35 - grad_norm: 0.0004949857444074271 tk: 1.0 x_norm: 66.8415994894382\n",
      "Iteration 36 - grad_norm: 0.0004830587107062824 tk: 1.0 x_norm: 66.2013133539269\n",
      "Iteration 37 - grad_norm: 0.00045357425432472284 tk: 1.0 x_norm: 65.76826874617754\n",
      "Iteration 38 - grad_norm: 0.00042773931332716576 tk: 1.0 x_norm: 65.70252926200358\n",
      "Iteration 39 - grad_norm: 0.0004186094679425927 tk: 1.0 x_norm: 65.99003723825544\n",
      "Iteration 40 - grad_norm: 0.00041343858748386523 tk: 1.0 x_norm: 66.35426551052815\n",
      "Iteration 41 - grad_norm: 0.0004022697535017772 tk: 1.0 x_norm: 66.55788455813193\n",
      "Iteration 42 - grad_norm: 0.00038180143129293043 tk: 1.0 x_norm: 66.47462204646745\n",
      "Iteration 43 - grad_norm: 0.0003565784345057724 tk: 1.0 x_norm: 66.05187093719077\n",
      "Iteration 44 - grad_norm: 0.00034110264322729904 tk: 1.0 x_norm: 65.52533249507341\n",
      "Iteration 45 - grad_norm: 0.0003426694582493172 tk: 1.0 x_norm: 65.08158142810443\n",
      "Iteration 46 - grad_norm: 0.0003684732813880633 tk: 1.0 x_norm: 64.85402514554046\n",
      "Iteration 47 - grad_norm: 0.00040412997955255087 tk: 1.0 x_norm: 64.88151195654284\n",
      "Iteration 48 - grad_norm: 0.00042213333317814175 tk: 1.0 x_norm: 65.14413768046361\n",
      "Iteration 49 - grad_norm: 0.00040287318633914587 tk: 1.0 x_norm: 65.44287737547742\n",
      "Iteration 50 - grad_norm: 0.0003666344055566153 tk: 1.0 x_norm: 65.62255594844466\n",
      "Iteration 51 - grad_norm: 0.00033128011177081985 tk: 1.0 x_norm: 65.62718310996702\n",
      "Iteration 52 - grad_norm: 0.00030289212583518816 tk: 1.0 x_norm: 65.45906510939469\n",
      "Iteration 53 - grad_norm: 0.00028180582867004176 tk: 1.0 x_norm: 65.20941241610925\n",
      "Iteration 54 - grad_norm: 0.00026894200452419345 tk: 1.0 x_norm: 65.00170960963544\n",
      "Iteration 55 - grad_norm: 0.00027781629094515296 tk: 1.0 x_norm: 64.95305740063749\n",
      "Iteration 56 - grad_norm: 0.0002967967177199764 tk: 1.0 x_norm: 65.02718560645997\n",
      "Iteration 57 - grad_norm: 0.00031281611824603167 tk: 1.0 x_norm: 65.17260428963303\n",
      "Iteration 58 - grad_norm: 0.00030969535289721744 tk: 1.0 x_norm: 65.28106275216449\n",
      "Iteration 59 - grad_norm: 0.00029124217321438797 tk: 1.0 x_norm: 65.28609374185417\n",
      "Iteration 60 - grad_norm: 0.0002709063017345956 tk: 1.0 x_norm: 65.1845188321373\n",
      "Iteration 61 - grad_norm: 0.00025693696164495684 tk: 1.0 x_norm: 65.03668367200459\n",
      "Iteration 62 - grad_norm: 0.00025143613901653023 tk: 1.0 x_norm: 64.90198743368664\n",
      "Iteration 63 - grad_norm: 0.00025720154144584034 tk: 1.0 x_norm: 64.8564570444718\n",
      "Iteration 64 - grad_norm: 0.0002721671706573624 tk: 1.0 x_norm: 64.95854253365678\n",
      "Iteration 65 - grad_norm: 0.00028435146169293856 tk: 1.0 x_norm: 65.16643596355497\n",
      "Iteration 66 - grad_norm: 0.00028290856587634823 tk: 1.0 x_norm: 65.3946086088038\n",
      "Iteration 67 - grad_norm: 0.00026580143747703433 tk: 1.0 x_norm: 65.53589472783098\n",
      "Iteration 68 - grad_norm: 0.00024012384964942608 tk: 1.0 x_norm: 65.53277574317818\n",
      "Iteration 69 - grad_norm: 0.00021757052632545988 tk: 1.0 x_norm: 65.37316764075369\n",
      "Iteration 70 - grad_norm: 0.0002061136369235042 tk: 1.0 x_norm: 65.13992753734306\n",
      "Iteration 71 - grad_norm: 0.000208200724182249 tk: 1.0 x_norm: 64.92712022591911\n",
      "Iteration 72 - grad_norm: 0.00022739214701570309 tk: 1.0 x_norm: 64.82108164299814\n",
      "Iteration 73 - grad_norm: 0.00025293698658785074 tk: 1.0 x_norm: 64.84510423938877\n",
      "Iteration 74 - grad_norm: 0.00027095668768186104 tk: 1.0 x_norm: 64.9423115506049\n",
      "Iteration 75 - grad_norm: 0.000271633251287112 tk: 1.0 x_norm: 65.03832823818831\n",
      "Iteration 76 - grad_norm: 0.0002553217488570924 tk: 1.0 x_norm: 65.06149701335129\n",
      "Iteration 77 - grad_norm: 0.00023183958088104114 tk: 1.0 x_norm: 64.9922623512936\n",
      "Iteration 78 - grad_norm: 0.00021097538776471464 tk: 1.0 x_norm: 64.87230843398723\n",
      "Iteration 79 - grad_norm: 0.000197107215238403 tk: 1.0 x_norm: 64.77244395746897\n",
      "Iteration 80 - grad_norm: 0.00019210223526650816 tk: 1.0 x_norm: 64.76963373929584\n",
      "Iteration 81 - grad_norm: 0.00019972055112803814 tk: 1.0 x_norm: 64.92306844224667\n",
      "Iteration 82 - grad_norm: 0.0002142167737529503 tk: 1.0 x_norm: 65.175004040805\n",
      "Iteration 83 - grad_norm: 0.00022406160302169143 tk: 1.0 x_norm: 65.42888040984931\n",
      "Iteration 84 - grad_norm: 0.00022310761362967336 tk: 1.0 x_norm: 65.6168710303731\n",
      "Iteration 85 - grad_norm: 0.00021105414748008206 tk: 1.0 x_norm: 65.69602435511749\n",
      "Iteration 86 - grad_norm: 0.00019287647373586965 tk: 1.0 x_norm: 65.66360448702385\n",
      "Iteration 87 - grad_norm: 0.00017796377473094094 tk: 1.0 x_norm: 65.5666691885147\n",
      "Iteration 88 - grad_norm: 0.00017126011443343236 tk: 1.0 x_norm: 65.50175304465651\n",
      "Iteration 89 - grad_norm: 0.00017215574140614422 tk: 1.0 x_norm: 65.53715261627754\n",
      "Iteration 90 - grad_norm: 0.00018277664694540675 tk: 1.0 x_norm: 65.68685059428456\n",
      "Iteration 91 - grad_norm: 0.00019827392391654535 tk: 1.0 x_norm: 65.90339209759593\n",
      "Iteration 92 - grad_norm: 0.00020989597979334277 tk: 1.0 x_norm: 66.13717475352544\n",
      "Iteration 93 - grad_norm: 0.00020955073909552577 tk: 1.0 x_norm: 66.29896973244166\n",
      "Iteration 94 - grad_norm: 0.0001973481278317285 tk: 1.0 x_norm: 66.31094391087183\n",
      "Iteration 95 - grad_norm: 0.000181264649539806 tk: 1.0 x_norm: 66.21111011098965\n",
      "Iteration 96 - grad_norm: 0.00016752563817009302 tk: 1.0 x_norm: 66.08169960137703\n",
      "Iteration 97 - grad_norm: 0.0001603997096920889 tk: 1.0 x_norm: 66.01364465386418\n",
      "Iteration 98 - grad_norm: 0.00016003278184955204 tk: 1.0 x_norm: 66.09693483692287\n",
      "Iteration 99 - grad_norm: 0.00016381451401201923 tk: 1.0 x_norm: 66.36677253938824\n",
      "Iteration 100 - grad_norm: 0.0001687814390948431 tk: 1.0 x_norm: 66.72061283286416\n",
      "Iteration 101 - grad_norm: 0.000173365873496343 tk: 1.0 x_norm: 67.05128008227862\n",
      "Iteration 102 - grad_norm: 0.0001775727859368129 tk: 1.0 x_norm: 67.28823194891385\n",
      "Iteration 103 - grad_norm: 0.0001805264634599637 tk: 1.0 x_norm: 67.34314446442674\n",
      "Iteration 104 - grad_norm: 0.00017867831009058305 tk: 1.0 x_norm: 67.20722814670413\n",
      "Iteration 105 - grad_norm: 0.00017039854943053852 tk: 1.0 x_norm: 66.97191825608355\n",
      "Iteration 106 - grad_norm: 0.00015945260180363937 tk: 1.0 x_norm: 66.75968437120379\n",
      "Iteration 107 - grad_norm: 0.0001505236267637822 tk: 1.0 x_norm: 66.64894059909466\n",
      "Iteration 108 - grad_norm: 0.0001468766374922231 tk: 1.0 x_norm: 66.71410581943866\n",
      "Iteration 109 - grad_norm: 0.0001453885181544397 tk: 1.0 x_norm: 66.93248852727353\n",
      "Iteration 110 - grad_norm: 0.00014195981830332103 tk: 1.0 x_norm: 67.18138011875712\n",
      "Iteration 111 - grad_norm: 0.00013763134239294766 tk: 1.0 x_norm: 67.34483620621606\n",
      "Iteration 112 - grad_norm: 0.000135115769356685 tk: 1.0 x_norm: 67.34182921580653\n",
      "Iteration 113 - grad_norm: 0.00013375848075421317 tk: 1.0 x_norm: 67.1772212318783\n",
      "Iteration 114 - grad_norm: 0.0001306595143205679 tk: 1.0 x_norm: 66.94785099106718\n",
      "Iteration 115 - grad_norm: 0.00012595780911159017 tk: 1.0 x_norm: 66.78731438671275\n",
      "Iteration 116 - grad_norm: 0.00012293008870478453 tk: 1.0 x_norm: 66.78700894681047\n",
      "Iteration 117 - grad_norm: 0.00012274225107835086 tk: 1.0 x_norm: 66.9732308878052\n",
      "Iteration 118 - grad_norm: 0.00012170517688985373 tk: 1.0 x_norm: 67.25023958962603\n",
      "Iteration 119 - grad_norm: 0.0001176972107543347 tk: 1.0 x_norm: 67.51083874275601\n",
      "Iteration 120 - grad_norm: 0.00011173988097871811 tk: 1.0 x_norm: 67.67862433600132\n",
      "Iteration 121 - grad_norm: 0.00010610875292069336 tk: 1.0 x_norm: 67.69274707104832\n",
      "Iteration 122 - grad_norm: 0.00010196793766571017 tk: 1.0 x_norm: 67.55101650273728\n",
      "BFGS - 迭代次数: 123\t最小值: 0.058483\t耗时: 16.134935s\n"
     ]
    }
   ],
   "source": [
    "#* 拟牛顿\n",
    "def quasi_newton_bfgs(f, f_grad, f_hessian, x0, mat_type='H', alpha=0.1, beta=0.5, epsilon=1e-6, max_iters=500):\n",
    "    assert mat_type in ['H','B']\n",
    "    xk = x0\n",
    "    hessian = f_hessian(x0)\n",
    "    mat_k = np.linalg.inv(hessian) if mat_type=='H' else hessian\n",
    "    iter_cnt = 0\n",
    "    t_s = time()\n",
    "    for idx in range(max_iters):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        dk = -mat_k@grad_k if mat_type=='H' else -np.linalg.inv(mat_k)@grad_k\n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, c1=1e-4, c2=0.9)\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "            return xk_next, iter_cnt, time()-t_s\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        mat_k = update_approximation_bfgs(mat=mat_k, sk=sk, yk=grad_next-grad_k, mat_type=mat_type)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt, time()-t_s\n",
    "\n",
    "def wolfe_condition(f, f_grad, xk, pk, c1=1e-4, c2=0.9, multiplier=1.25, t0=0, tmax=2):\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "np.random.seed(1000)\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# 使用 quasi-newton 求解无约束问题\n",
    "# quasi_newton_bfgs(f, f_grad, f_hessian, x0, mat_type='H', alpha=0.1, beta=0.5, epsilon=1e-6, maxIter=100):\n",
    "x_opt_bfgs, iter_cnt_bfgs, t_bfgs = quasi_newton_bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, mat_type='H', epsilon=1e-4, max_iters=200)\n",
    "# x_opt_bfgs, iter_cnt_bfgs, t_bfgs = quasi_newton_bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, mat_type='B', epsilon=1e-8, max_iters=200)\n",
    "print(f'BFGS - 迭代次数: {iter_cnt_bfgs}\\t最小值: {f(x_opt_bfgs):>2f}\\t耗时: {t_bfgs:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure l-bfgs w/o constraints (D=500) ***aborted***\n",
    "\n",
    "Not very appropriate for IPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.2874976403305301 tk: 1.0 x_norm: 0.6034130174696074\n",
      "Iteration 2 - grad_norm: 0.2838872583169178 tk: 1.25 x_norm: 0.6295060924456327\n",
      "Iteration 3 - grad_norm: 0.28242345968900984 tk: 3.0517578125 x_norm: 0.6542786523350346\n",
      "Iteration 4 - grad_norm: 0.28217543253346206 tk: 3.814697265625 x_norm: 0.6749511140438361\n",
      "Iteration 5 - grad_norm: 0.2821953930122363 tk: 5.9604644775390625 x_norm: 0.6953605998289375\n",
      "Iteration 6 - grad_norm: 0.28217480273736845 tk: 5.9604644775390625 x_norm: 0.7144840093383312\n",
      "Iteration 7 - grad_norm: 0.2821900982989895 tk: 7.450580596923828 x_norm: 0.7425670327511641\n",
      "Iteration 8 - grad_norm: 0.2820953597561407 tk: 11.641532182693481 x_norm: 0.782945591576831\n",
      "Iteration 9 - grad_norm: 0.28219147481740264 tk: 7.450580596923828 x_norm: 0.8100803607423993\n",
      "Iteration 10 - grad_norm: 0.282316987039029 tk: 1.5625 x_norm: 0.8507287723275746\n",
      "Iteration 11 - grad_norm: 0.2825591070321152 tk: 1.0 x_norm: 0.9281565145082193\n",
      "Iteration 12 - grad_norm: 0.28284588710723013 tk: 1.0 x_norm: 1.016427673000379\n",
      "Iteration 13 - grad_norm: 0.28318353701809906 tk: 1.0 x_norm: 1.1146069916278616\n",
      "Iteration 14 - grad_norm: 0.2835713586151958 tk: 1.0 x_norm: 1.2202781596444714\n",
      "Iteration 15 - grad_norm: 0.28400950195151586 tk: 1.0 x_norm: 1.3318416238473683\n",
      "L-BFGS - 迭代次数: 15\t最小值: 0.417851\t耗时: 11.990113s\n"
     ]
    }
   ],
   "source": [
    "class Container():\n",
    "    def __init__(self,numStore,dim) -> None:\n",
    "        self.ss = [np.zeros(dim) for i in range(numStore)]\n",
    "        self.ys = [np.zeros(dim) for i in range(numStore)]\n",
    "        self.cur_idx = 0\n",
    "        self.numStore = numStore\n",
    "        self.dim = dim\n",
    "    def update_container(self,sk,yk):\n",
    "        self.ss[self.cur_idx] = sk\n",
    "        self.ys[self.cur_idx] = yk\n",
    "        self.cur_idx = (self.cur_idx+1)%self.numStore\n",
    "    def cal_descent_direction(self,grad):\n",
    "        # self.cur_idx # last index\n",
    "        start_idx = self.cur_idx-1 # first index\n",
    "        for i in range(start_idx,start_idx-self.numStore,-1):\n",
    "            ysTss = self.ss[i]@self.ys[i]\n",
    "            alpha_i = (self.ss[i]@grad)/ysTss if ysTss != 0 else 0\n",
    "            grad -= alpha_i*self.ys[i]\n",
    "        r = np.eye(self.dim)@grad\n",
    "        for i in range(self.cur_idx, self.cur_idx+self.numStore):\n",
    "            trueId = i if i<self.numStore else i-self.numStore\n",
    "            ysTss = self.ss[trueId]@self.ys[trueId]\n",
    "            alpha_i = 0\n",
    "            beta = 0\n",
    "            if ysTss!=0:\n",
    "                alpha_i = (self.ss[trueId]@grad)/ysTss\n",
    "                beta = (self.ys[trueId]@r)/ysTss\n",
    "            r = r + (alpha_i-beta)*self.ss[trueId]\n",
    "        return r\n",
    "        \n",
    "\n",
    "#* 拟牛顿\n",
    "def quasi_newton_lbfgs(f, f_grad, f_hessian, x0, m=15, epsilon=1e-6, max_iters=500):\n",
    "    xk = x0\n",
    "    # hessian = f_hessian(x0)\n",
    "    # mat_k = np.linalg.inv(hessian) \n",
    "    # mat_k = np.eye(n)\n",
    "    dk = -f_grad(xk)\n",
    "    iter_cnt = 0\n",
    "    container = Container(numStore=m, dim=xk.size)\n",
    "    t_s = time()\n",
    "    for idx in range(max_iters):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        # dk = -mat_k@grad_k \n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, c1=1e-4, c2=0.9)\n",
    "        if tk<0:\n",
    "            return xk, iter_cnt-1, time()-t_s\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "            return xk_next, iter_cnt, time()-t_s\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        # mat_k = update_approximation_lbfgs(mat=mat_k, sk=sk, yk=grad_next-grad_k)\n",
    "        container.update_container(sk=sk, yk=grad_next-grad_k)\n",
    "        dk = -container.cal_descent_direction(grad_next)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt, time()-t_s\n",
    "\n",
    "def wolfe_condition(f, f_grad, xk, pk, c1=1e-4, c2=0.9, multiplier=1.25, t0=0, tmax=2):\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "\n",
    "np.random.seed(1000)\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# 使用 quasi-newton 求解无约束问题\n",
    "x_opt_lbfgs, iter_cnt_lbfgs, t_lbfgs = quasi_newton_lbfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, m=15, epsilon=1e-3, max_iters=200)\n",
    "print(f'L-BFGS - 迭代次数: {iter_cnt_lbfgs}\\t最小值: {f(x_opt_lbfgs):>2f}\\t耗时: {t_lbfgs:>2f}s')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVXPY-计算 optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优值: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzq22/anaconda3/envs/optim/lib/python3.8/site-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 8 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# D=500\n",
    "# x = cp.Variable(n)\n",
    "# # objective = cp.Minimize(sum(cp.log1p(cp.exp(-b*(A@x))))/m + 1/(100*m)* x.T@x)\n",
    "# # objective = cp.Minimize(sum(cp.log1p(cp.exp(-b*(A@x))))/m)\n",
    "# objective = cp.Minimize(cp.norm(x)**2 + sum(cp.log1p(cp.exp(-b*(A@x))))/m)\n",
    "# constraints = [cp.norm(x)<=D/2]\n",
    "# problem = cp.Problem(objective,constraints)\n",
    "# result = problem.solve()\n",
    "# print(f'最优值: {f(x.value)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyhton 3 [optim]",
   "language": "python",
   "name": "optim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
