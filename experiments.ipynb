{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from time import time\n",
    "from scipy.interpolate import interp1d\n",
    "from libsvm.svmutil import svm_read_problem # https://blog.csdn.net/u013630349/article/details/47323883\n",
    "\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single(seq, xlabel='Iteration', ylabel='Gradient Norm', title=''):\n",
    "    plt.figure()\n",
    "    iterations = np.arange(len(seq))+1\n",
    "    plt.semilogy(iterations,seq)\n",
    "    plt.xticks(iterations)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "def plot_multi_seqs(seqs, xlabel='Iteration', ylabel='Gradient Norm', title='', xtick_step = 50):\n",
    "    plt.figure(figsize=(16,8), dpi=150)\n",
    "    maxLen = 0\n",
    "    for seq in seqs:\n",
    "        iterations = seq.index\n",
    "        if iterations.size > maxLen:\n",
    "            maxLen = iterations.size\n",
    "        plt.semilogy(iterations, seq, label=seq.name)\n",
    "    plt.xticks(np.arange(stop=maxLen,step=xtick_step)+1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45546, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(path):\n",
    "    b, A = svm_read_problem(path)\n",
    "    rows = len(b)   # 矩阵行数, i.e. sample 数\n",
    "    cols = max([max(row.keys()) if len(row)>0 else 0 for row in A])  # 矩阵列数, i.e. feature 数\n",
    "    b = np.array(b)\n",
    "    A_np = np.zeros((rows,cols))\n",
    "    for r in range(rows):\n",
    "        for c in A[r].keys():\n",
    "            # MatLab 是 1-index, python 则是 0-index\n",
    "            A_np[r,c-1] = A[r][c]\n",
    "    # 清楚全 0 features\n",
    "    effective_row_ids = []\n",
    "    for idx, row in enumerate(A_np):\n",
    "        if np.sum(row) > 1e-3:\n",
    "            effective_row_ids.append(idx)\n",
    "    return b[effective_row_ids], A_np[effective_row_ids]\n",
    "\n",
    "b, A = read_data('w8a')\n",
    "m,n = A.shape\n",
    "m,n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令 $g_i(x)=\\exp(b_i x^\\top a_i), i=1,2,...,m$\n",
    "\n",
    "所以可以得到目标函数为:\n",
    "$$\n",
    "f(x)=\\frac{1}{m}\\sum_{i=1}^{m}\\log\\left( 1+ \\frac{1}{g_i(x)}\\right) + \\frac{1}{100m}\\left\\|x\\right\\|^2\n",
    "$$\n",
    "进而可以得到梯度 $\\nabla f(x)$ 和 Hessian矩阵 $\\nabla^2 f(x)$ 的形式:\n",
    "$$\n",
    "\\nabla f(x) = \\frac{1}{m}\\sum_{i=1}^{m}\\left[ -b_i a_i (1+g_i(x))^{-1} \\right] + \\frac{1}{50m}x\n",
    "$$\n",
    "$$\n",
    "\\nabla^2 f(x) =  \\frac{1}{m}\\sum_{i=1}^{m}\\left( b_i^2 \\frac{g_i(x)}{(1+g_i(x))^2}a_i a_i^\\top \\right) + \\frac{1}{50m}I\n",
    "$$\n",
    "\n",
    "此处函数内的 $a_i,b_i$ 和数据本身相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 100\n",
    "\n",
    "def f(x):\n",
    "    bAx = b*(A@x)\n",
    "    exp_mbAx = np.exp(-bAx)\n",
    "    log1p_exp = np.log(1+exp_mbAx)\n",
    "    overflow_idxs = np.where(exp_mbAx==float('inf'))\n",
    "    log1p_exp[overflow_idxs] = -bAx[overflow_idxs]\n",
    "    # return np.log(1+np.exp(-bAx)).mean() + 1/(100*m)* x.T@x\n",
    "    return log1p_exp.mean() + 1/(lamda*m)* x.T@x\n",
    "\n",
    "def f_grad(x):\n",
    "    # return np.ones(m)@(np.expand_dims((-b)/(1+np.exp(b*(A@x))), axis=1)*A)/m\n",
    "    return np.ones(m)@(np.expand_dims((-b)/(1+np.exp(b*(A@x))), axis=1)*A)/m + 2/(lamda*m)*x\n",
    "\n",
    "def f_hessian(x):\n",
    "    Ax = A@x\n",
    "    exp_bAx = np.exp(b*Ax)\n",
    "    # return (A.T @ (np.expand_dims(b*b*exp_bAx/(1+exp_bAx)**2, axis=1)*A) )/m\n",
    "    # return (A.T @ (np.expand_dims(b*b*exp_bAx/(1+exp_bAx)**2, axis=1)*A) )/m + 2/(lamda*m)*np.diag([1.0]*x.size)\n",
    "    return (A.T @ (np.expand_dims(b*b*exp_bAx/(1+exp_bAx)**2, axis=1)*A) )/m + 2/(lamda*m)*np.eye(x.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Armijo rule \n",
    "def armijo_search(f, f_grad, xk, t_hat, alpha, beta, D, isNewton=False, dk=None):\n",
    "    if isNewton:\n",
    "        assert dk is not None\n",
    "    tk = t_hat*1\n",
    "    grad = f_grad(xk)\n",
    "    while True:\n",
    "        if isNewton:\n",
    "            if np.linalg.norm(xk+tk*dk,ord=2)<=D/2 and f(xk+tk*dk) <= f(xk) + alpha*tk*grad.T@dk:\n",
    "                break\n",
    "        else:\n",
    "            if np.linalg.norm(xk-tk*grad,ord=2)<=D/2 and f(xk-tk*grad) <= f(xk)-alpha*tk*grad.T@grad:\n",
    "                break\n",
    "        tk *= beta\n",
    "    return tk\n",
    "\n",
    "#* 梯度下降法\n",
    "def gradient_descent(f, f_grad, x0, D, t_hat=1, epsilon=1e-6, max_iters=10000):\n",
    "    # func_val_record = []\n",
    "    func_val_record = [f(x0)]\n",
    "    grad_norm_record = []\n",
    "    xk = x0\n",
    "    t_s = time()\n",
    "    # for idx in trange(max_iters):\n",
    "    for idx in range(max_iters):\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=t_hat, alpha=0.1, beta=0.5, D=D)\n",
    "        xk_next = xk-tk*f_grad(xk)\n",
    "        fval_xk_next = f(xk_next)\n",
    "        grad_xk_next = f_grad(xk_next)\n",
    "        func_val_record.append(fval_xk_next)\n",
    "        grad_norm_next = np.linalg.norm(grad_xk_next,ord=2)\n",
    "        grad_norm_record.append(grad_norm_next)\n",
    "        # termination criteria\n",
    "        # if grad_norm_next<=epsilon:\n",
    "        #     break\n",
    "        if grad_norm_next<=epsilon or np.linalg.norm(xk_next)>=D/2-1e-3:\n",
    "            break\n",
    "        # if (func_val_record[-1]-func_val_record[-2])/func_val_record[-2]<=epsilon:\n",
    "        #     break\n",
    "        else:\n",
    "            print(f'Iteration {idx} - Grad. Norm.:',grad_norm_next, 'tk:',tk, 'x_norm:',np.linalg.norm(xk_next))\n",
    "        xk = xk_next\n",
    "    t_e = time()\n",
    "    return xk_next, np.asarray(func_val_record), np.asarray(grad_norm_record), t_e-t_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Grad. Norm.: 0.052109680543728935 tk: 5 x_norm: 3.1948158750857765\n",
      "Iteration 1 - Grad. Norm.: 0.03874538795695223 tk: 5 x_norm: 3.3007664609434073\n",
      "Iteration 2 - Grad. Norm.: 0.03247236111795165 tk: 5 x_norm: 3.39832364755234\n",
      "Iteration 3 - Grad. Norm.: 0.028707614758793267 tk: 5 x_norm: 3.488927444461727\n",
      "Iteration 4 - Grad. Norm.: 0.026137955548665787 tk: 5 x_norm: 3.573880284211031\n",
      "Iteration 5 - Grad. Norm.: 0.024238255924168794 tk: 5 x_norm: 3.654164258131663\n",
      "Iteration 6 - Grad. Norm.: 0.02275515238979999 tk: 5 x_norm: 3.730525846831219\n",
      "Iteration 7 - Grad. Norm.: 0.021550498078459098 tk: 5 x_norm: 3.803544343824972\n",
      "Iteration 8 - Grad. Norm.: 0.02054217078675201 tk: 5 x_norm: 3.873677891009221\n",
      "Iteration 9 - Grad. Norm.: 0.01967811317099874 tk: 5 x_norm: 3.9412945794501484\n",
      "Iteration 10 - Grad. Norm.: 0.018923649545407387 tk: 5 x_norm: 4.006694049320368\n",
      "Iteration 11 - Grad. Norm.: 0.018254755248512777 tk: 5 x_norm: 4.070122926376995\n",
      "Iteration 12 - Grad. Norm.: 0.017654243223426912 tk: 5 x_norm: 4.131786136275055\n",
      "Iteration 13 - Grad. Norm.: 0.017109485628512194 tk: 5 x_norm: 4.1918553780763546\n",
      "Iteration 14 - Grad. Norm.: 0.01661099162910515 tk: 5 x_norm: 4.250475586797769\n",
      "Iteration 15 - Grad. Norm.: 0.016151486504397905 tk: 5 x_norm: 4.307769938849529\n",
      "Iteration 16 - Grad. Norm.: 0.015725296588775545 tk: 5 x_norm: 4.3638437802112815\n",
      "Iteration 17 - Grad. Norm.: 0.015327927455063974 tk: 5 x_norm: 4.41878774419224\n",
      "Iteration 18 - Grad. Norm.: 0.01495576795345957 tk: 5 x_norm: 4.472680250221823\n",
      "Iteration 19 - Grad. Norm.: 0.014605878410723272 tk: 5 x_norm: 4.525589523576644\n",
      "Iteration 20 - Grad. Norm.: 0.014275836426987468 tk: 5 x_norm: 4.577575239960449\n",
      "Iteration 21 - Grad. Norm.: 0.013963622905490478 tk: 5 x_norm: 4.628689873249136\n",
      "Iteration 22 - Grad. Norm.: 0.013667536698368668 tk: 5 x_norm: 4.6789798061940004\n",
      "Iteration 23 - Grad. Norm.: 0.013386129933440693 tk: 5 x_norm: 4.728486250282938\n",
      "Iteration 24 - Grad. Norm.: 0.013118158498739279 tk: 5 x_norm: 4.777246010846559\n",
      "Iteration 25 - Grad. Norm.: 0.012862543774248811 tk: 5 x_norm: 4.825292125878381\n",
      "Iteration 26 - Grad. Norm.: 0.012618342799414947 tk: 5 x_norm: 4.872654401231959\n",
      "Iteration 27 - Grad. Norm.: 0.012384724827479992 tk: 5 x_norm: 4.919359860382872\n",
      "Iteration 28 - Grad. Norm.: 0.01216095275544152 tk: 5 x_norm: 4.9654331234576885\n",
      "Iteration 29 - Grad. Norm.: 0.01194636830344556 tk: 5 x_norm: 5.010896727490012\n",
      "Iteration 30 - Grad. Norm.: 0.011740380096867469 tk: 5 x_norm: 5.055771397686673\n",
      "Iteration 31 - Grad. Norm.: 0.011542454009604618 tk: 5 x_norm: 5.100076277744397\n",
      "Iteration 32 - Grad. Norm.: 0.011352105279463776 tk: 5 x_norm: 5.1438291258517665\n",
      "Iteration 33 - Grad. Norm.: 0.011168892020605283 tk: 5 x_norm: 5.187046481870443\n",
      "Iteration 34 - Grad. Norm.: 0.010992409844044392 tk: 5 x_norm: 5.229743810258475\n",
      "Iteration 35 - Grad. Norm.: 0.01082228736248468 tk: 5 x_norm: 5.271935622535017\n",
      "Iteration 36 - Grad. Norm.: 0.010658182405517774 tk: 5 x_norm: 5.313635582457262\n",
      "Iteration 37 - Grad. Norm.: 0.010499778809307676 tk: 5 x_norm: 5.3548565965613575\n",
      "Iteration 38 - Grad. Norm.: 0.010346783674123578 tk: 5 x_norm: 5.395610892289318\n",
      "Iteration 39 - Grad. Norm.: 0.010198925005613414 tk: 5 x_norm: 5.435910085567235\n",
      "Iteration 40 - Grad. Norm.: 0.010055949673115894 tk: 5 x_norm: 5.475765239403506\n",
      "Iteration 41 - Grad. Norm.: 0.00991762163179917 tk: 5 x_norm: 5.515186914828691\n",
      "Iteration 42 - Grad. Norm.: 0.009783720365906778 tk: 5 x_norm: 5.554185215292414\n",
      "Iteration 43 - Grad. Norm.: 0.009654039518584677 tk: 5 x_norm: 5.592769825460382\n",
      "Iteration 44 - Grad. Norm.: 0.009528385680190871 tk: 5 x_norm: 5.630950045210301\n",
      "Iteration 45 - Grad. Norm.: 0.00940657731205772 tk: 5 x_norm: 5.668734819504399\n",
      "Iteration 46 - Grad. Norm.: 0.009288443786697239 tk: 5 x_norm: 5.706132764714663\n",
      "Iteration 47 - Grad. Norm.: 0.009173824528649512 tk: 5 x_norm: 5.743152191891231\n",
      "Iteration 48 - Grad. Norm.: 0.009062568242754069 tk: 5 x_norm: 5.779801127392229\n",
      "Iteration 49 - Grad. Norm.: 0.008954532218712436 tk: 5 x_norm: 5.816087331232378\n",
      "Iteration 50 - Grad. Norm.: 0.008849581702513225 tk: 5 x_norm: 5.852018313456026\n",
      "Iteration 51 - Grad. Norm.: 0.008747589326689507 tk: 5 x_norm: 5.887601348796511\n",
      "Iteration 52 - Grad. Norm.: 0.008648434592534953 tk: 5 x_norm: 5.922843489846482\n",
      "Iteration 53 - Grad. Norm.: 0.008552003398367881 tk: 5 x_norm: 5.9577515789322195\n",
      "Iteration 54 - Grad. Norm.: 0.00845818760873844 tk: 5 x_norm: 5.992332258857884\n",
      "Iteration 55 - Grad. Norm.: 0.008366884660153454 tk: 5 x_norm: 6.026591982662588\n",
      "Iteration 56 - Grad. Norm.: 0.00827799719946767 tk: 5 x_norm: 6.060537022513511\n",
      "Iteration 57 - Grad. Norm.: 0.00819143275157915 tk: 5 x_norm: 6.094173477841201\n",
      "Iteration 58 - Grad. Norm.: 0.008107103413483375 tk: 5 x_norm: 6.127507282808913\n",
      "Iteration 59 - Grad. Norm.: 0.008024925572098265 tk: 5 x_norm: 6.160544213195214\n",
      "Iteration 60 - Grad. Norm.: 0.007944819643579409 tk: 5 x_norm: 6.193289892758523\n",
      "Iteration 61 - Grad. Norm.: 0.007866709832110139 tk: 5 x_norm: 6.2257497991429025\n",
      "Iteration 62 - Grad. Norm.: 0.007790523906379961 tk: 5 x_norm: 6.257929269376674\n",
      "Iteration 63 - Grad. Norm.: 0.007716192992164045 tk: 5 x_norm: 6.289833505008441\n",
      "Iteration 64 - Grad. Norm.: 0.007643651379589124 tk: 5 x_norm: 6.32146757691937\n",
      "Iteration 65 - Grad. Norm.: 0.007572836343822244 tk: 5 x_norm: 6.352836429845363\n",
      "Iteration 66 - Grad. Norm.: 0.007503687978050367 tk: 5 x_norm: 6.383944886638537\n",
      "Iteration 67 - Grad. Norm.: 0.007436149037734645 tk: 5 x_norm: 6.414797652293544\n",
      "Iteration 68 - Grad. Norm.: 0.007370164795224553 tk: 5 x_norm: 6.445399317761025\n",
      "Iteration 69 - Grad. Norm.: 0.007305682903906841 tk: 5 x_norm: 6.4757543635677415\n",
      "Iteration 70 - Grad. Norm.: 0.007242653271142864 tk: 5 x_norm: 6.505867163260412\n",
      "Iteration 71 - Grad. Norm.: 0.007181027939318518 tk: 5 x_norm: 6.535741986688262\n",
      "Iteration 72 - Grad. Norm.: 0.007120760974392532 tk: 5 x_norm: 6.565383003137404\n",
      "Iteration 73 - Grad. Norm.: 0.00706180836138476 tk: 5 x_norm: 6.594794284328683\n",
      "Iteration 74 - Grad. Norm.: 0.0070041279062951845 tk: 5 x_norm: 6.623979807289179\n",
      "Iteration 75 - Grad. Norm.: 0.006947679143988687 tk: 5 x_norm: 6.652943457106436\n",
      "Iteration 76 - Grad. Norm.: 0.006892423251620309 tk: 5 x_norm: 6.681689029573478\n",
      "Iteration 77 - Grad. Norm.: 0.006838322967211188 tk: 5 x_norm: 6.710220233731739\n",
      "Iteration 78 - Grad. Norm.: 0.006785342513017727 tk: 5 x_norm: 6.738540694318314\n",
      "Iteration 79 - Grad. Norm.: 0.006733447523365182 tk: 5 x_norm: 6.766653954123283\n",
      "Iteration 80 - Grad. Norm.: 0.006682604976643397 tk: 5 x_norm: 6.79456347626223\n",
      "Iteration 81 - Grad. Norm.: 0.006632783131185928 tk: 5 x_norm: 6.8222726463686225\n",
      "Iteration 82 - Grad. Norm.: 0.006583951464775524 tk: 5 x_norm: 6.849784774710303\n",
      "Iteration 83 - Grad. Norm.: 0.006536080617538423 tk: 5 x_norm: 6.877103098233852\n",
      "Iteration 84 - Grad. Norm.: 0.006489142338007995 tk: 5 x_norm: 6.904230782540435\n",
      "Iteration 85 - Grad. Norm.: 0.006443109432154281 tk: 5 x_norm: 6.931170923796241\n",
      "Iteration 86 - Grad. Norm.: 0.006397955715191257 tk: 5 x_norm: 6.957926550580559\n",
      "Iteration 87 - Grad. Norm.: 0.006353655965987035 tk: 5 x_norm: 6.984500625674142\n",
      "Iteration 88 - Grad. Norm.: 0.006310185883914801 tk: 5 x_norm: 7.010896047790501\n",
      "Iteration 89 - Grad. Norm.: 0.006267522047993853 tk: 5 x_norm: 7.037115653252374\n",
      "Iteration 90 - Grad. Norm.: 0.0062256418781806155 tk: 5 x_norm: 7.063162217615676\n",
      "Iteration 91 - Grad. Norm.: 0.006184523598679103 tk: 5 x_norm: 7.0890384572429275\n",
      "Iteration 92 - Grad. Norm.: 0.006144146203149559 tk: 5 x_norm: 7.114747030828146\n",
      "Iteration 93 - Grad. Norm.: 0.006104489421701856 tk: 5 x_norm: 7.140290540875032\n",
      "Iteration 94 - Grad. Norm.: 0.006065533689568208 tk: 5 x_norm: 7.1656715351301425\n",
      "Iteration 95 - Grad. Norm.: 0.0060272601173566075 tk: 5 x_norm: 7.190892507972765\n",
      "Iteration 96 - Grad. Norm.: 0.0059896504627929064 tk: 5 x_norm: 7.215955901762982\n",
      "Iteration 97 - Grad. Norm.: 0.005952687103865626 tk: 5 x_norm: 7.2408641081494585\n",
      "Iteration 98 - Grad. Norm.: 0.005916353013292988 tk: 5 x_norm: 7.2656194693383656\n",
      "Iteration 99 - Grad. Norm.: 0.005880631734237009 tk: 5 x_norm: 7.290224279324753\n",
      "Iteration 100 - Grad. Norm.: 0.005845507357194208 tk: 5 x_norm: 7.3146807850877344\n",
      "Iteration 101 - Grad. Norm.: 0.005810964497996975 tk: 5 x_norm: 7.338991187750657\n",
      "Iteration 102 - Grad. Norm.: 0.005776988276863809 tk: 5 x_norm: 7.363157643707488\n",
      "Iteration 103 - Grad. Norm.: 0.005743564298440498 tk: 5 x_norm: 7.38718226571656\n",
      "Iteration 104 - Grad. Norm.: 0.005710678632777918 tk: 5 x_norm: 7.4110671239627255\n",
      "Iteration 105 - Grad. Norm.: 0.005678317797195463 tk: 5 x_norm: 7.434814247089042\n",
      "Iteration 106 - Grad. Norm.: 0.00564646873898221 tk: 5 x_norm: 7.458425623198962\n",
      "Iteration 107 - Grad. Norm.: 0.005615118818890957 tk: 5 x_norm: 7.481903200829988\n",
      "Iteration 108 - Grad. Norm.: 0.005584255795382843 tk: 5 x_norm: 7.505248889899782\n",
      "Iteration 109 - Grad. Norm.: 0.005553867809582866 tk: 5 x_norm: 7.528464562625599\n",
      "Iteration 110 - Grad. Norm.: 0.005523943370908989 tk: 5 x_norm: 7.551552054417913\n",
      "Iteration 111 - Grad. Norm.: 0.0054944713433397645 tk: 5 x_norm: 7.574513164749113\n",
      "Iteration 112 - Grad. Norm.: 0.005465440932287348 tk: 5 x_norm: 7.597349657998026\n",
      "Iteration 113 - Grad. Norm.: 0.005436841672044922 tk: 5 x_norm: 7.620063264271106\n",
      "Iteration 114 - Grad. Norm.: 0.0054086634137791205 tk: 5 x_norm: 7.64265568020099\n",
      "Iteration 115 - Grad. Norm.: 0.005380896314039965 tk: 5 x_norm: 7.6651285697231835\n",
      "Iteration 116 - Grad. Norm.: 0.005353530823762216 tk: 5 x_norm: 7.687483564831559\n",
      "Iteration 117 - Grad. Norm.: 0.005326557677733729 tk: 5 x_norm: 7.709722266313349\n",
      "Iteration 118 - Grad. Norm.: 0.00529996788450755 tk: 5 x_norm: 7.7318462444642835\n",
      "Iteration 119 - Grad. Norm.: 0.005273752716736079 tk: 5 x_norm: 7.753857039784492\n",
      "Iteration 120 - Grad. Norm.: 0.005247903701906571 tk: 5 x_norm: 7.775756163655808\n",
      "Iteration 121 - Grad. Norm.: 0.005222412613458654 tk: 5 x_norm: 7.797545099001002\n",
      "Iteration 122 - Grad. Norm.: 0.0051972714622654 tk: 5 x_norm: 7.81922530092558\n",
      "Iteration 123 - Grad. Norm.: 0.005172472488460624 tk: 5 x_norm: 7.840798197342625\n",
      "Iteration 124 - Grad. Norm.: 0.005148008153596033 tk: 5 x_norm: 7.8622651895812625\n",
      "Iteration 125 - Grad. Norm.: 0.005123871133112667 tk: 5 x_norm: 7.883627652979195\n",
      "Iteration 126 - Grad. Norm.: 0.005100054309111971 tk: 5 x_norm: 7.904886937459868\n",
      "Iteration 127 - Grad. Norm.: 0.005076550763412619 tk: 5 x_norm: 7.926044368094676\n",
      "Iteration 128 - Grad. Norm.: 0.005053353770879928 tk: 5 x_norm: 7.947101245650691\n",
      "Iteration 129 - Grad. Norm.: 0.005030456793015412 tk: 5 x_norm: 7.968058847124352\n",
      "Iteration 130 - Grad. Norm.: 0.005007853471794668 tk: 5 x_norm: 7.988918426261543\n",
      "Iteration 131 - Grad. Norm.: 0.004985537623742439 tk: 5 x_norm: 8.009681214064443\n",
      "Iteration 132 - Grad. Norm.: 0.00496350323423421 tk: 5 x_norm: 8.030348419285577\n",
      "Iteration 133 - Grad. Norm.: 0.00494174445201434 tk: 5 x_norm: 8.050921228909427\n",
      "Iteration 134 - Grad. Norm.: 0.004920255583921091 tk: 5 x_norm: 8.071400808621982\n",
      "Iteration 135 - Grad. Norm.: 0.004899031089809628 tk: 5 x_norm: 8.091788303268565\n",
      "Iteration 136 - Grad. Norm.: 0.004878065577664277 tk: 5 x_norm: 8.112084837300293\n",
      "Iteration 137 - Grad. Norm.: 0.0048573537988919 tk: 5 x_norm: 8.132291515209515\n",
      "Iteration 138 - Grad. Norm.: 0.0048368906437886855 tk: 5 x_norm: 8.152409421954514\n",
      "Iteration 139 - Grad. Norm.: 0.004816671137172886 tk: 5 x_norm: 8.1724396233738\n",
      "Iteration 140 - Grad. Norm.: 0.004796690434176531 tk: 5 x_norm: 8.192383166590304\n",
      "Iteration 141 - Grad. Norm.: 0.004776943816189436 tk: 5 x_norm: 8.212241080405732\n",
      "Iteration 142 - Grad. Norm.: 0.004757426686949174 tk: 5 x_norm: 8.232014375685388\n",
      "Iteration 143 - Grad. Norm.: 0.004738134568770871 tk: 5 x_norm: 8.251704045733728\n",
      "Iteration 144 - Grad. Norm.: 0.004719063098911212 tk: 5 x_norm: 8.271311066660848\n",
      "Iteration 145 - Grad. Norm.: 0.004700208026061033 tk: 5 x_norm: 8.290836397740293\n",
      "Iteration 146 - Grad. Norm.: 0.004681565206961339 tk: 5 x_norm: 8.310280981758263\n",
      "Iteration 147 - Grad. Norm.: 0.0046631306031377695 tk: 5 x_norm: 8.329645745354577\n",
      "Iteration 148 - Grad. Norm.: 0.0046449002777486995 tk: 5 x_norm: 8.348931599355577\n",
      "Iteration 149 - Grad. Norm.: 0.004626870392542518 tk: 5 x_norm: 8.368139439099176\n",
      "Iteration 150 - Grad. Norm.: 0.004609037204919693 tk: 5 x_norm: 8.387270144752287\n",
      "Iteration 151 - Grad. Norm.: 0.004591397065095509 tk: 5 x_norm: 8.406324581620847\n",
      "Iteration 152 - Grad. Norm.: 0.004573946413359559 tk: 5 x_norm: 8.425303600452606\n",
      "Iteration 153 - Grad. Norm.: 0.004556681777428187 tk: 5 x_norm: 8.444208037732885\n",
      "Iteration 154 - Grad. Norm.: 0.004539599769886302 tk: 5 x_norm: 8.463038715973518\n",
      "Iteration 155 - Grad. Norm.: 0.00452269708571512 tk: 5 x_norm: 8.481796443995103\n",
      "Iteration 156 - Grad. Norm.: 0.004505970499902511 tk: 5 x_norm: 8.500482017202804\n",
      "Iteration 157 - Grad. Norm.: 0.004489416865132891 tk: 5 x_norm: 8.519096217855822\n",
      "Iteration 158 - Grad. Norm.: 0.004473033109553527 tk: 5 x_norm: 8.537639815330706\n",
      "Iteration 159 - Grad. Norm.: 0.0044568162346144986 tk: 5 x_norm: 8.556113566378716\n",
      "Iteration 160 - Grad. Norm.: 0.004440763312979452 tk: 5 x_norm: 8.574518215377303\n",
      "Iteration 161 - Grad. Norm.: 0.004424871486504592 tk: 5 x_norm: 8.592854494575944\n",
      "Iteration 162 - Grad. Norm.: 0.0044091379642833385 tk: 5 x_norm: 8.611123124336434\n",
      "Iteration 163 - Grad. Norm.: 0.004393560020754221 tk: 5 x_norm: 8.62932481336775\n",
      "Iteration 164 - Grad. Norm.: 0.004378134993869741 tk: 5 x_norm: 8.647460258955713\n",
      "Iteration 165 - Grad. Norm.: 0.0043628602833239135 tk: 5 x_norm: 8.665530147187466\n",
      "Iteration 166 - Grad. Norm.: 0.004347733348836403 tk: 5 x_norm: 8.683535153170999\n",
      "Iteration 167 - Grad. Norm.: 0.00433275170849119 tk: 5 x_norm: 8.70147594124977\n",
      "Iteration 168 - Grad. Norm.: 0.004317912937127823 tk: 5 x_norm: 8.719353165212606\n",
      "Iteration 169 - Grad. Norm.: 0.004303214664783347 tk: 5 x_norm: 8.737167468498956\n",
      "Iteration 170 - Grad. Norm.: 0.004288654575183136 tk: 5 x_norm: 8.754919484399625\n",
      "Iteration 171 - Grad. Norm.: 0.004274230404278877 tk: 5 x_norm: 8.772609836253098\n",
      "Iteration 172 - Grad. Norm.: 0.004259939938832053 tk: 5 x_norm: 8.79023913763761\n",
      "Iteration 173 - Grad. Norm.: 0.004245781015041309 tk: 5 x_norm: 8.807807992558946\n",
      "Iteration 174 - Grad. Norm.: 0.004231751517212209 tk: 5 x_norm: 8.82531699563423\n",
      "Iteration 175 - Grad. Norm.: 0.004217849376467856 tk: 5 x_norm: 8.842766732271683\n",
      "Iteration 176 - Grad. Norm.: 0.004204072569498997 tk: 5 x_norm: 8.860157778846485\n",
      "Iteration 177 - Grad. Norm.: 0.004190419117352251 tk: 5 x_norm: 8.877490702872857\n",
      "Iteration 178 - Grad. Norm.: 0.0041768870842551304 tk: 5 x_norm: 8.894766063172419\n",
      "Iteration 179 - Grad. Norm.: 0.004163474576476612 tk: 5 x_norm: 8.911984410038947\n",
      "Iteration 180 - Grad. Norm.: 0.00415017974122207 tk: 5 x_norm: 8.929146285399577\n",
      "Iteration 181 - Grad. Norm.: 0.004137000765561341 tk: 5 x_norm: 8.946252222972568\n",
      "Iteration 182 - Grad. Norm.: 0.0041239358753888955 tk: 5 x_norm: 8.963302748421711\n",
      "Iteration 183 - Grad. Norm.: 0.004110983334414956 tk: 5 x_norm: 8.98029837950742\n",
      "Iteration 184 - Grad. Norm.: 0.0040981414431865764 tk: 5 x_norm: 8.997239626234652\n",
      "Iteration 185 - Grad. Norm.: 0.004085408538137675 tk: 5 x_norm: 9.014126990997669\n",
      "Iteration 186 - Grad. Norm.: 0.004072782990667028 tk: 5 x_norm: 9.030960968721734\n",
      "Iteration 187 - Grad. Norm.: 0.004060263206243351 tk: 5 x_norm: 9.047742047001842\n",
      "Iteration 188 - Grad. Norm.: 0.004047847623536561 tk: 5 x_norm: 9.064470706238508\n",
      "Iteration 189 - Grad. Norm.: 0.004035534713574314 tk: 5 x_norm: 9.081147419770717\n",
      "Iteration 190 - Grad. Norm.: 0.0040233229789230765 tk: 5 x_norm: 9.097772654006082\n",
      "Iteration 191 - Grad. Norm.: 0.0040112109528928725 tk: 5 x_norm: 9.114346868548282\n",
      "Iteration 192 - Grad. Norm.: 0.0039991971987649345 tk: 5 x_norm: 9.130870516321844\n",
      "Iteration 193 - Grad. Norm.: 0.003987280309041588 tk: 5 x_norm: 9.147344043694327\n",
      "Iteration 194 - Grad. Norm.: 0.003975458904717568 tk: 5 x_norm: 9.163767890595956\n",
      "Iteration 195 - Grad. Norm.: 0.003963731634572135 tk: 5 x_norm: 9.180142490636797\n",
      "Iteration 196 - Grad. Norm.: 0.003952097174481336 tk: 5 x_norm: 9.196468271221502\n",
      "Iteration 197 - Grad. Norm.: 0.003940554226749706 tk: 5 x_norm: 9.212745653661653\n",
      "Iteration 198 - Grad. Norm.: 0.003929101519460888 tk: 5 x_norm: 9.22897505328586\n",
      "Iteration 199 - Grad. Norm.: 0.003917737805846478 tk: 5 x_norm: 9.245156879547515\n",
      "Iteration 200 - Grad. Norm.: 0.003906461863672607 tk: 5 x_norm: 9.261291536130411\n",
      "Iteration 201 - Grad. Norm.: 0.003895272494643651 tk: 5 x_norm: 9.277379421052132\n",
      "Iteration 202 - Grad. Norm.: 0.003884168523822536 tk: 5 x_norm: 9.293420926765386\n",
      "Iteration 203 - Grad. Norm.: 0.003873148799067174 tk: 5 x_norm: 9.309416440257232\n",
      "Iteration 204 - Grad. Norm.: 0.003862212190482467 tk: 5 x_norm: 9.325366343146303\n",
      "Iteration 205 - Grad. Norm.: 0.003851357589887427 tk: 5 x_norm: 9.341271011778074\n",
      "Iteration 206 - Grad. Norm.: 0.003840583910296955 tk: 5 x_norm: 9.357130817318152\n",
      "Iteration 207 - Grad. Norm.: 0.0038298900854178133 tk: 5 x_norm: 9.372946125843725\n",
      "Iteration 208 - Grad. Norm.: 0.0038192750691583787 tk: 5 x_norm: 9.388717298433141\n",
      "Iteration 209 - Grad. Norm.: 0.003808737835151722 tk: 5 x_norm: 9.404444691253678\n",
      "Iteration 210 - Grad. Norm.: 0.003798277376291648 tk: 5 x_norm: 9.420128655647579\n",
      "Iteration 211 - Grad. Norm.: 0.0037878927042812854 tk: 5 x_norm: 9.43576953821631\n",
      "Iteration 212 - Grad. Norm.: 0.003777582849193835 tk: 5 x_norm: 9.451367680903184\n",
      "Iteration 213 - Grad. Norm.: 0.003767346859045158 tk: 5 x_norm: 9.466923421074299\n",
      "Iteration 214 - Grad. Norm.: 0.003757183799377783 tk: 5 x_norm: 9.482437091597857\n",
      "Iteration 215 - Grad. Norm.: 0.0037470927528560558 tk: 5 x_norm: 9.497909020921941\n",
      "Iteration 216 - Grad. Norm.: 0.0037370728188720584 tk: 5 x_norm: 9.513339533150699\n",
      "Iteration 217 - Grad. Norm.: 0.0037271231131619877 tk: 5 x_norm: 9.528728948119044\n",
      "Iteration 218 - Grad. Norm.: 0.0037172427674327076 tk: 5 x_norm: 9.544077581465874\n",
      "Iteration 219 - Grad. Norm.: 0.003707430928998128 tk: 5 x_norm: 9.559385744705834\n",
      "Iteration 220 - Grad. Norm.: 0.00369768676042518 tk: 5 x_norm: 9.574653745299665\n",
      "Iteration 221 - Grad. Norm.: 0.003688009439189058 tk: 5 x_norm: 9.589881886723186\n",
      "Iteration 222 - Grad. Norm.: 0.0036783981573374775 tk: 5 x_norm: 9.605070468534876\n",
      "Iteration 223 - Grad. Norm.: 0.00366885212116371 tk: 5 x_norm: 9.620219786442187\n",
      "Iteration 224 - Grad. Norm.: 0.003659370550888077 tk: 5 x_norm: 9.635330132366512\n",
      "Iteration 225 - Grad. Norm.: 0.0036499526803477284 tk: 5 x_norm: 9.650401794506912\n",
      "Iteration 226 - Grad. Norm.: 0.0036405977566944386 tk: 5 x_norm: 9.66543505740259\n",
      "Iteration 227 - Grad. Norm.: 0.0036313050401001637 tk: 5 x_norm: 9.680430201994136\n",
      "Iteration 228 - Grad. Norm.: 0.0036220738034701853 tk: 5 x_norm: 9.695387505683604\n",
      "Iteration 229 - Grad. Norm.: 0.00361290333216359 tk: 5 x_norm: 9.71030724239342\n",
      "Iteration 230 - Grad. Norm.: 0.003603792923720884 tk: 5 x_norm: 9.725189682624134\n",
      "Iteration 231 - Grad. Norm.: 0.003594741887598559 tk: 5 x_norm: 9.740035093511064\n",
      "Iteration 232 - Grad. Norm.: 0.003585749544910379 tk: 5 x_norm: 9.75484373887986\n",
      "Iteration 233 - Grad. Norm.: 0.003576815228175211 tk: 5 x_norm: 9.769615879300982\n",
      "Iteration 234 - Grad. Norm.: 0.0035679382810712527 tk: 5 x_norm: 9.78435177214314\n",
      "Iteration 235 - Grad. Norm.: 0.0035591180581964043 tk: 5 x_norm: 9.7990516716257\n",
      "Iteration 236 - Grad. Norm.: 0.0035503539248346832 tk: 5 x_norm: 9.8137158288701\n",
      "Iteration 237 - Grad. Norm.: 0.0035416452567284683 tk: 5 x_norm: 9.828344491950283\n",
      "Iteration 238 - Grad. Norm.: 0.003532991439856431 tk: 5 x_norm: 9.842937905942149\n",
      "Iteration 239 - Grad. Norm.: 0.0035243918702169935 tk: 5 x_norm: 9.857496312972092\n",
      "Iteration 240 - Grad. Norm.: 0.0035158459536171406 tk: 5 x_norm: 9.8720199522646\n",
      "Iteration 241 - Grad. Norm.: 0.0035073531054664897 tk: 5 x_norm: 9.886509060188947\n",
      "Iteration 242 - Grad. Norm.: 0.0034989127505763986 tk: 5 x_norm: 9.900963870305036\n",
      "Iteration 243 - Grad. Norm.: 0.0034905243229640453 tk: 5 x_norm: 9.915384613408317\n",
      "Iteration 244 - Grad. Norm.: 0.003482187265661298 tk: 5 x_norm: 9.929771517573922\n",
      "Iteration 245 - Grad. Norm.: 0.003473901030528257 tk: 5 x_norm: 9.944124808199922\n",
      "Iteration 246 - Grad. Norm.: 0.0034656650780713503 tk: 5 x_norm: 9.958444708049793\n",
      "Iteration 247 - Grad. Norm.: 0.003457478877265842 tk: 5 x_norm: 9.972731437294097\n",
      "Iteration 248 - Grad. Norm.: 0.003449341905382658 tk: 5 x_norm: 9.986985213551353\n",
      "Iteration 249 - Grad. Norm.: 0.003445294909853667 tk: 2.5 x_norm: 9.994094541898372\n",
      "Iteration 250 - Grad. Norm.: 0.0034432767417426333 tk: 1.25 x_norm: 9.997644843128413\n",
      "最小值: 0.095011\t耗时: 26.128632s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "init_x = np.zeros(n)+0.005\n",
    "x_opt_gd, _, _, t_gd = gradient_descent(f=f, f_grad=f_grad, x0=init_x, D=20, t_hat=5, epsilon=1e-6, max_iters=300)\n",
    "optim_val = f(x_opt_gd)\n",
    "print(f'最小值: {optim_val:>2f}\\t耗时: {t_gd:>2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09501128827728388"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_opt_gd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton (Interior Point Methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logarithmic barrier:\n",
    "$$\n",
    "\\phi(x)=-\\log(-g(x)),\\quad g(x)=\\left\\|x\\right\\|_2 - D/2\n",
    "$$\n",
    "$$\n",
    "\\nabla g(x) = x/\\left\\|x\\right\\|_2, \\quad \\nabla^2 g(x)= \\left\\|x\\right\\|_2^{-1}I-\\left\\|x\\right\\|_2^{-3}xx^\\top\n",
    "$$\n",
    "gradient and hessian:\n",
    "$$\n",
    "\\nabla \\phi(x) = \\frac{1}{-g(x)}\\nabla g(x)=\\frac{x/\\left\\|x\\right\\|_2}{D/2-\\left\\|x\\right\\|_2}=\\frac{x}{\\left\\|x\\right\\|_2(D/2-\\left\\|x\\right\\|_2)}\n",
    "$$\n",
    "$$\n",
    "\\nabla^2\\phi(x)=\\frac{1}{g(x)^2}\\nabla g(x)\\nabla g(x)^\\top + \\frac{1}{-g(x)}\\nabla^2 g(x) = \\frac{1}{\\left\\|x\\right\\|_2(D/2-\\left\\|x\\right\\|_2)}I+\\frac{2\\left\\|x\\right\\|_2-D/2}{\\left\\|x\\right\\|_2^3(D/2-\\left\\|x\\right\\|_2)^2}xx^\\top\n",
    "$$\n",
    "central path - for $t>0$:\n",
    "$$\n",
    "\\min_x tf(x)+\\phi(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x,D):\n",
    "    return -np.log(D/2-np.linalg.norm(x,ord=2))\n",
    "\n",
    "def phi_grad(x,D):\n",
    "    x_norm = np.linalg.norm(x,ord=2)\n",
    "    return x/(x_norm*(D/2-x_norm))\n",
    "\n",
    "def phi_hessian(x,D):\n",
    "    x_norm = np.linalg.norm(x,ord=2)\n",
    "    xxT = np.matmul(x[:,None],x[None,:])    # x * xT\n",
    "    return np.eye(x.size)/(x_norm*(D/2-x_norm)) + (2*x_norm-D/2)/(x_norm**3 * (D/2-x_norm)**2)*xxT\n",
    "\n",
    "\n",
    "#* 外部迭代\n",
    "def barrier_method(t_init, f, f_grad, f_hessian, phi, phi_grad, phi_hessian, A, b, x0, D, num_constraints, mu,\n",
    "                        method='newton', epsilon=1e-6, maxIter=20):\n",
    "    xt = x0\n",
    "    t = t_init\n",
    "    duality_gaps = []\n",
    "    t_s = time()\n",
    "    for i in range(maxIter):\n",
    "        xt,num_newton_step = solve_central(f=lambda x:t*f(x)+phi(x,D), \n",
    "                                f_grad=lambda x:t*f_grad(x)+phi_grad(x,D), \n",
    "                                f_hessian=lambda x:t*f_hessian(x)+phi_hessian(x,D),\n",
    "                                x0=xt, D=D, method=method, epsilon=epsilon*1e3)\n",
    "        duality_gaps.extend([num_constraints/t]*num_newton_step)\n",
    "        if num_constraints/t < epsilon:\n",
    "            break\n",
    "        t *= mu\n",
    "    t_e = time()\n",
    "    return xt, t_e-t_s, duality_gaps\n",
    "\n",
    "def solve_central(f, f_grad, f_hessian, x0, D, method='newton', epsilon=1e-6, max_iter=50):\n",
    "    if method == 'newton':\n",
    "        return damped_newton(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=x0, D=D, epsilon=epsilon, max_iter=max_iter)\n",
    "    if method == 'bfgs':\n",
    "        return bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=x0, D=D, epsilon=epsilon, max_iter=max_iter)\n",
    "\n",
    "\n",
    "#* 阻尼牛顿\n",
    "def damped_newton(f, f_grad, f_hessian, x0, D, epsilon=1e-6, max_iter=50):\n",
    "    xk = x0\n",
    "    iter_cnt = 0\n",
    "    for idx in range(max_iter):\n",
    "        iter_cnt += 1\n",
    "        grad = f_grad(xk)\n",
    "        hessian = f_hessian(xk)\n",
    "        dk = -np.linalg.inv(hessian)@grad\n",
    "        decrement = (-grad@dk)**0.5\n",
    "        if decrement**2/2 <= epsilon:\n",
    "            print('** End The Loop - Iter Cnt.:',iter_cnt, 'Decrement:',decrement, 'fval:',f(xk))\n",
    "            return xk, iter_cnt\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=1, alpha=0.1, beta=0.5, D=D, isNewton=True, dk=dk)\n",
    "        print('Iter Cnt.:',iter_cnt, 'Decrement:',decrement, 'fval:',f(xk), 'tk:',tk)\n",
    "        xk += tk*dk\n",
    "    return xk, iter_cnt\n",
    "\n",
    "#* 拟牛顿\n",
    "def bfgs(f, f_grad, f_hessian, x0, D, alpha=0.1, beta=0.5, epsilon=1e-6, max_iter=500):\n",
    "    xk = x0\n",
    "    hessian = f_hessian(x0)\n",
    "    mat_k = np.linalg.inv(hessian) \n",
    "    # mat_k = np.eye(n) \n",
    "    iter_cnt = 0\n",
    "    for idx in range(max_iter):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        dk = -mat_k@grad_k \n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, D, c1=1e-4, c2=0.9)\n",
    "        if tk<0:\n",
    "            return xk, iter_cnt-1\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        # if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon or np.linalg.norm(xk_next)>=D/2-1e-2:\n",
    "            return xk_next, iter_cnt\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        mat_k = update_approximation_bfgs(mat=mat_k, sk=sk, yk=grad_next-grad_k)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt\n",
    "        \n",
    "def update_approximation_bfgs(mat, sk, yk, mat_type='H'):\n",
    "    rhok = 1/(yk@sk)\n",
    "    if mat_type == 'H':\n",
    "        Hkyk = mat@yk\n",
    "        ykTHkyk = yk@Hkyk\n",
    "        HkykskT = Hkyk[:,None]@sk[None,:]\n",
    "        skskT = sk[:,None]@sk[None,:]\n",
    "        mat_new = mat + rhok*((rhok*ykTHkyk+1)*skskT - HkykskT - HkykskT.T)\n",
    "    else:\n",
    "        Bksk = mat@sk\n",
    "        skTBksk = sk@Bksk\n",
    "        mat_new = mat - Bksk[:,None]@Bksk[None,:]/skTBksk + yk[:,None]@yk[None,:]*rhok\n",
    "    return mat_new\n",
    "\n",
    "#* 拟牛顿方法 - 选择步长\n",
    "def wolfe_condition(f, f_grad, xk, pk, D, c1=1e-4, c2=0.9, multiplier=1.2, t0=0, tmax=2):\n",
    "    ### \n",
    "    while (np.linalg.norm(xk+tmax*pk)>=D/2):\n",
    "        tmax /= 2\n",
    "        # print('tmax:',tmax)\n",
    "        if tmax<1e-6:\n",
    "            # print('too small stepsize')\n",
    "            return -1\n",
    "    ###\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "def zoom(f, f_grad, xk, pk, fval, grad, c1, c2, t_lo, t_hi):\n",
    "    while True:\n",
    "        # print(f\"t_lo: {t_lo}\\tt_hi: {t_hi}\")\n",
    "        t = (t_lo+t_hi)/2\n",
    "        xk_next = xk + t*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if fval_next > fval + c1*t*grad@pk or fval_next >= f(xk+t_lo*pk):\n",
    "            t_hi = t\n",
    "        else:\n",
    "            grad_next = f_grad(xk_next)\n",
    "            grad_next_T_pk = grad_next@pk\n",
    "            if np.abs(grad_next_T_pk) <= -c2*grad@pk:\n",
    "                return t\n",
    "            if grad_next_T_pk*(t_hi-t_lo)>=0:\n",
    "                t_hi = t_lo\n",
    "            t_lo = t\n",
    "        if t_lo == t_hi: # 死循环\n",
    "            return -1\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter Cnt.: 1 Decrement: 0.9525578038914555 fval: -1.569533749629282 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6084895374423559 fval: -1.7498984994316462 tk: 0.5\n",
      "Iter Cnt.: 3 Decrement: 0.12992597126245592 fval: -1.8633527068729119 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.02321306437254386 fval: -1.8726656070316618\n",
      "Iter Cnt.: 1 Decrement: 1.0792049141682987 fval: 0.647848502583277 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6100747846231196 fval: -0.08446265646887308 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.23545159529444123 fval: -0.3070194485297655 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.03707000873467996 fval: -0.33731360473380945\n",
      "Iter Cnt.: 1 Decrement: 2.185111949659234 fval: 12.3868314478077 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6861200416531448 fval: 9.862690130100074 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.1074930906532772 fval: 9.60733766618385 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.004044385787036891 fval: 9.601432568933406\n",
      "Iter Cnt.: 1 Decrement: 4.7492332061745826 fval: 101.25102919604771 tk: 0.25\n",
      "Iter Cnt.: 2 Decrement: 1.5063942741053518 fval: 96.55952964013056 tk: 0.5\n",
      "Iter Cnt.: 3 Decrement: 0.2516796064492284 fval: 95.84917578348347 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.01932715163629368 fval: 95.81665806667954\n",
      "Iter Cnt.: 1 Decrement: 7.758057290120473 fval: 945.2189941916731 tk: 0.125\n",
      "Iter Cnt.: 2 Decrement: 0.4660397496437592 fval: 938.9329615227451 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.14794578373910416 fval: 938.8530207878181 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.020506191622691856 fval: 938.8411657445458\n",
      "Iter Cnt.: 1 Decrement: 8.68203280676348 fval: 9355.058817088458 tk: 0.0625\n",
      "Iter Cnt.: 2 Decrement: 3.5343922841032853 fval: 9350.582936507835 tk: 0.25\n",
      "Iter Cnt.: 3 Decrement: 0.45948028913901945 fval: 9348.706638986409 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.21033478846173254 fval: 9348.576998294318 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.0441416320702611 fval: 9348.552201230903\n",
      "Iter Cnt.: 1 Decrement: 8.547191160550971 fval: 93431.25018213892 tk: 0.0625\n",
      "Iter Cnt.: 2 Decrement: 3.4575580534280257 fval: 93426.9136562485 tk: 0.25\n",
      "Iter Cnt.: 3 Decrement: 0.39430599106298314 fval: 93425.05666820492 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.15534873841802843 fval: 93424.96311223789 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.024121528570717442 fval: 93424.9499260611\n",
      "Iter Cnt.: 1 Decrement: 8.757583002532396 fval: 934174.6931805913 tk: 0.0625\n",
      "Iter Cnt.: 2 Decrement: 3.4179025244706693 fval: 934170.1449633552 tk: 0.25\n",
      "Iter Cnt.: 3 Decrement: 0.3569769006429667 fval: 934168.2972183257 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.12741973345093416 fval: 934168.2215044968 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.016234885295921552 fval: 934168.2127568781\n",
      "最小值: 0.093416\t耗时: 4.448850s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "# D=20\n",
    "t_init = 1\n",
    "x0 = np.zeros(n)+0.005\n",
    "x_opt_ipm_damped, t_ipm_damped, duality_gaps_damped = barrier_method(t_init=t_init, f=f, f_grad=f_grad, f_hessian=f_hessian, phi=phi, phi_grad=phi_grad, phi_hessian=phi_hessian, \n",
    "                A=A, b=b, x0=x0, D=20, num_constraints=1, method='newton', mu=10, epsilon=1e-6, maxIter=20)\n",
    "print(f'最小值: {f(x_opt_ipm_damped):>2f}\\t耗时: {t_ipm_damped:>2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.16436546152681786 tk: 1.0 x_norm: 1.895870645724538\n",
      "Iteration 2 - grad_norm: 0.14891025638579403 tk: 1.2 x_norm: 1.6123917628543183\n",
      "Iteration 3 - grad_norm: 0.07502011871157341 tk: 0.5 x_norm: 0.9970644876961946\n",
      "Iteration 4 - grad_norm: 0.05610577996870103 tk: 1.0 x_norm: 1.0941712317097694\n",
      "Iteration 5 - grad_norm: 0.014832457759573174 tk: 1.0 x_norm: 1.3777964276903911\n",
      "Iteration 6 - grad_norm: 0.010120591705236754 tk: 1.0 x_norm: 1.4373307072874\n",
      "Iteration 7 - grad_norm: 0.008785266082478564 tk: 1.0 x_norm: 1.4558377622885526\n",
      "Iteration 8 - grad_norm: 0.006119698831746697 tk: 1.0 x_norm: 1.4713877021416755\n",
      "Iteration 9 - grad_norm: 0.0031432242900151616 tk: 1.0 x_norm: 1.4686542555331454\n",
      "Iteration 10 - grad_norm: 0.0015329498593511898 tk: 1.0 x_norm: 1.4568608633253517\n",
      "Iteration 11 - grad_norm: 0.0012077880680353415 tk: 1.0 x_norm: 1.4511717558898638\n",
      "Iteration 12 - grad_norm: 0.001027187177675304 tk: 1.0 x_norm: 1.4491015164638759\n",
      "Iteration 1 - grad_norm: 0.37897369409781073 tk: 1.0 x_norm: 2.6716247250573217\n",
      "Iteration 2 - grad_norm: 0.19103101073342035 tk: 1.0 x_norm: 3.3781214992859194\n",
      "Iteration 3 - grad_norm: 0.06582550066550386 tk: 1.0 x_norm: 4.022965750889254\n",
      "Iteration 4 - grad_norm: 0.02000640938903534 tk: 1.0 x_norm: 4.279907977435319\n",
      "Iteration 5 - grad_norm: 0.00721373677400255 tk: 1.0 x_norm: 4.340148399787634\n",
      "Iteration 6 - grad_norm: 0.004324818616191462 tk: 1.0 x_norm: 4.339109585529465\n",
      "Iteration 7 - grad_norm: 0.002451388127396861 tk: 1.0 x_norm: 4.324917840136136\n",
      "Iteration 8 - grad_norm: 0.0012405472026928452 tk: 1.0 x_norm: 4.317989629676852\n",
      "Iteration 1 - grad_norm: 0.9369479151549006 tk: 0.5 x_norm: 6.15267366704988\n",
      "Iteration 2 - grad_norm: 0.5634705426245702 tk: 0.5 x_norm: 7.329215158895444\n",
      "Iteration 3 - grad_norm: 0.10944500176601571 tk: 1.0 x_norm: 8.371728209845907\n",
      "Iteration 4 - grad_norm: 0.05280962811543018 tk: 1.0 x_norm: 8.11705849342592\n",
      "Iteration 5 - grad_norm: 0.030415371883788872 tk: 1.0 x_norm: 8.186857560658796\n",
      "Iteration 6 - grad_norm: 0.017508852202837284 tk: 1.0 x_norm: 8.219009126919097\n",
      "Iteration 7 - grad_norm: 0.0075133818810660825 tk: 1.0 x_norm: 8.216061865604626\n",
      "Iteration 8 - grad_norm: 0.001340380112068907 tk: 1.0 x_norm: 8.208845480131444\n",
      "Iteration 1 - grad_norm: 4.322107438530214 tk: 0.125 x_norm: 8.796834487492438\n",
      "Iteration 2 - grad_norm: 3.5118232407719683 tk: 0.125 x_norm: 9.25065685097002\n",
      "Iteration 3 - grad_norm: 2.7713303580466757 tk: 0.125 x_norm: 9.470911011967987\n",
      "Iteration 4 - grad_norm: 1.5470289871640288 tk: 0.25 x_norm: 9.654831243792751\n",
      "Iteration 5 - grad_norm: 0.16764841411077577 tk: 0.5 x_norm: 9.756881970626992\n",
      "Iteration 6 - grad_norm: 0.07257469678303852 tk: 1.0 x_norm: 9.754279962435104\n",
      "Iteration 7 - grad_norm: 0.036219703009681935 tk: 0.5 x_norm: 9.758335600568904\n",
      "Iteration 8 - grad_norm: 0.03340843693411504 tk: 0.5 x_norm: 9.760403428493149\n",
      "Iteration 9 - grad_norm: 0.008991311842119412 tk: 0.5 x_norm: 9.758971567713655\n",
      "Iteration 10 - grad_norm: 0.010167210266545287 tk: 0.5 x_norm: 9.758272034660449\n",
      "Iteration 11 - grad_norm: 0.004981563166981456 tk: 0.5 x_norm: 9.758448716929273\n",
      "Iteration 12 - grad_norm: 0.0013525759945646363 tk: 0.5 x_norm: 9.758683469552656\n",
      "Iteration 13 - grad_norm: 0.0013849749422579277 tk: 0.5 x_norm: 9.758723594700347\n",
      "Iteration 1 - grad_norm: 33.565102325860074 tk: 0.0625 x_norm: 9.863697887592807\n",
      "Iteration 2 - grad_norm: 27.673734162209993 tk: 0.0625 x_norm: 9.922530042982562\n",
      "Iteration 3 - grad_norm: 17.339054623722927 tk: 0.125 x_norm: 9.956491767035814\n",
      "Iteration 4 - grad_norm: 6.245752487869007 tk: 0.25 x_norm: 9.970477307596576\n",
      "Iteration 5 - grad_norm: 1.0520143018555663 tk: 0.5 x_norm: 9.974302196042903\n",
      "Iteration 6 - grad_norm: 0.1512116470555265 tk: 0.25 x_norm: 9.974880861373796\n",
      "Iteration 7 - grad_norm: 0.20836123720456382 tk: 1.0 x_norm: 9.97505837917602\n",
      "Iteration 8 - grad_norm: 0.19179521975429206 tk: 0.25 x_norm: 9.975046582116324\n",
      "Iteration 9 - grad_norm: 0.04708056757578243 tk: 0.5 x_norm: 9.974957892286785\n",
      "Iteration 10 - grad_norm: 0.08991912185314754 tk: 0.5 x_norm: 9.974885820392672\n",
      "Iteration 11 - grad_norm: 0.08057421155201498 tk: 0.25 x_norm: 9.974892136524838\n",
      "Iteration 12 - grad_norm: 0.01006769948033861 tk: 0.5 x_norm: 9.974945316102483\n",
      "Iteration 13 - grad_norm: 0.02662949329134319 tk: 0.5 x_norm: 9.974956612450665\n",
      "Iteration 14 - grad_norm: 0.011437620004993355 tk: 0.5 x_norm: 9.974945218269946\n",
      "Iteration 15 - grad_norm: 0.007220000357902804 tk: 0.5 x_norm: 9.974935427407608\n",
      "Iteration 16 - grad_norm: 0.006460160579116465 tk: 0.5 x_norm: 9.97493640098992\n",
      "Iteration 17 - grad_norm: 0.0027677591015975397 tk: 0.5 x_norm: 9.97494061107029\n",
      "Iteration 18 - grad_norm: 0.002198951839202836 tk: 0.5 x_norm: 9.97494135768509\n",
      "Iteration 19 - grad_norm: 0.0017196036482067336 tk: 0.5 x_norm: 9.974940538911618\n",
      "Iteration 20 - grad_norm: 0.0011119312641601025 tk: 0.5 x_norm: 9.974939804871173\n",
      "Iteration 21 - grad_norm: 0.001009941277284192 tk: 0.5 x_norm: 9.974939651169079\n",
      "Iteration 1 - grad_norm: 322.912326472404 tk: 0.054 x_norm: 9.986764512495123\n",
      "最小值: 0.093416\t耗时: 10.194712s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "# D=20\n",
    "t_init = 1\n",
    "x0 = np.zeros(n)+0.005\n",
    "x_opt_ipm_bfgs, t_ipm_bfgs, duality_gaps_bfgs = barrier_method(t_init=t_init, f=f, f_grad=f_grad, f_hessian=f_hessian, phi=phi, phi_grad=phi_grad, phi_hessian=phi_hessian, \n",
    "                A=A, b=b, x0=x0, D=20, num_constraints=1, method='bfgs', mu=10, epsilon=1e-6, maxIter=20)\n",
    "print(f'最小值: {f(x_opt_ipm_bfgs):>2f}\\t耗时: {t_ipm_bfgs:>2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.647958966233265e-07"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_opt_ipm_bfgs)-f(x_opt_ipm_damped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure damped newton w/o constraints (D=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_norm: 0.18017720987362976 tk: 1 x_norm: 11.165278301280532\n",
      "grad_norm: 0.07430235763542076 tk: 1 x_norm: 18.78206938371404\n",
      "grad_norm: 0.03121190141848213 tk: 1 x_norm: 27.44040477856484\n",
      "grad_norm: 0.012796497780903252 tk: 1 x_norm: 36.77727415441325\n",
      "grad_norm: 0.004894660242605477 tk: 1 x_norm: 45.9257331177887\n",
      "grad_norm: 0.0016353055256952348 tk: 1 x_norm: 54.13674948486485\n",
      "grad_norm: 0.0004390302509728913 tk: 1 x_norm: 60.10520617618396\n",
      "grad_norm: 9.125726807373675e-05 tk: 1 x_norm: 63.53445177996058\n",
      "grad_norm: 2.4768185298650292e-05 tk: 1 x_norm: 65.32760767837152\n",
      "grad_norm: 6.8735057881137935e-06 tk: 1 x_norm: 66.52694639503159\n",
      "grad_norm: 1.2104066887873718e-06 tk: 1 x_norm: 67.1480388626449\n",
      "grad_norm: 9.161945407374053e-08 tk: 1 x_norm: 67.27924417413054\n",
      "最小值: 0.058278\t耗时: 2.071095s\n"
     ]
    }
   ],
   "source": [
    "#* pure阻尼牛顿\n",
    "def pure_damped_newton(f, f_grad, f_hessian, x0, D, epsilon=1e-6, max_iters=100):\n",
    "    func_val_record = []\n",
    "    grad_norm_record = []\n",
    "    xk = x0\n",
    "    t_s = time()\n",
    "    # for idx in trange(max_iters):\n",
    "    for idx in range(max_iters):\n",
    "        grad = f_grad(xk)\n",
    "        # print(np.linalg.norm(grad))\n",
    "        hessian = f_hessian(xk)\n",
    "        # dk = -np.linalg.pinv(hessian)@grad\n",
    "        dk = -np.linalg.inv(hessian)@grad\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=1, alpha=0.1, beta=0.5, D=D, isNewton=True, dk=dk)\n",
    "        xk_next = xk + tk*dk\n",
    "        func_val_record.append(f(xk_next))\n",
    "        # grad_norm_record.append(np.linalg.norm(f_grad(xk_next),ord=2))\n",
    "        grad_next = np.linalg.norm(f_grad(xk_next),ord=2)\n",
    "        grad_norm_record.append(grad_next)\n",
    "        # termination criteria\n",
    "        if grad_next<=epsilon:\n",
    "            break\n",
    "        else:\n",
    "            print('grad_norm:',grad_next,\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        xk = xk_next\n",
    "    t_e = time()\n",
    "    return xk_next, np.asarray(func_val_record), np.asarray(grad_norm_record), t_e-t_s\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "# init_x = np.zeros(n)+0.5\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# D=500 半径足够大，本质上是无约束\n",
    "x_opt_damped, _, _, t_damped = pure_damped_newton(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, D=500, epsilon=1e-8, max_iters=50)\n",
    "print(f'最小值: {f(x_opt_damped):>2f}\\t耗时: {t_damped:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure quasi newton w/o constraints (D=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.18017720987362976 tk: 1.0 x_norm: 11.165278301280532\n",
      "Iteration 2 - grad_norm: 0.11332750219773532 tk: 1.0 x_norm: 15.135338733342227\n",
      "Iteration 3 - grad_norm: 0.0588887609960591 tk: 1.0 x_norm: 21.533528580786278\n",
      "Iteration 4 - grad_norm: 0.03370457453496356 tk: 1.0 x_norm: 28.007182606015498\n",
      "Iteration 5 - grad_norm: 0.018882899036194684 tk: 1.0 x_norm: 35.87806759864599\n",
      "Iteration 6 - grad_norm: 0.010866841039356227 tk: 1.0 x_norm: 44.44864398760293\n",
      "Iteration 7 - grad_norm: 0.006490045202947455 tk: 1.0 x_norm: 53.327850892309826\n",
      "Iteration 8 - grad_norm: 0.004300271670796263 tk: 1.0 x_norm: 61.34159830918189\n",
      "Iteration 9 - grad_norm: 0.0033286798522448226 tk: 1.0 x_norm: 67.46185331330766\n",
      "Iteration 10 - grad_norm: 0.0029130781154975767 tk: 1.0 x_norm: 71.52202526197196\n",
      "Iteration 11 - grad_norm: 0.0026709621915537753 tk: 1.0 x_norm: 74.55496886554727\n",
      "Iteration 12 - grad_norm: 0.0024711794717667194 tk: 1.0 x_norm: 77.01301640920776\n",
      "Iteration 13 - grad_norm: 0.0022580722379469745 tk: 1.0 x_norm: 77.61081896031442\n",
      "Iteration 14 - grad_norm: 0.0019611305006011176 tk: 1.0 x_norm: 74.87516342684319\n",
      "Iteration 15 - grad_norm: 0.001815015621707459 tk: 1.0 x_norm: 69.7557515224829\n",
      "Iteration 16 - grad_norm: 0.0019063176832953225 tk: 1.0 x_norm: 64.42711104360677\n",
      "Iteration 17 - grad_norm: 0.0018763345697549107 tk: 1.0 x_norm: 61.94319651661975\n",
      "Iteration 18 - grad_norm: 0.0017208657496739978 tk: 1.0 x_norm: 61.616147745504676\n",
      "Iteration 19 - grad_norm: 0.001386078159312382 tk: 1.0 x_norm: 62.89409076494536\n",
      "Iteration 20 - grad_norm: 0.0011635196375529271 tk: 1.0 x_norm: 65.10222137327875\n",
      "Iteration 21 - grad_norm: 0.0010389973610115406 tk: 1.0 x_norm: 67.10553071463985\n",
      "Iteration 22 - grad_norm: 0.0009588307486315539 tk: 1.0 x_norm: 68.51480003888817\n",
      "Iteration 23 - grad_norm: 0.0008895651592463788 tk: 1.0 x_norm: 69.19359689165321\n",
      "Iteration 24 - grad_norm: 0.0008186623859286282 tk: 1.0 x_norm: 68.98677847889323\n",
      "Iteration 25 - grad_norm: 0.0007557517882971561 tk: 1.0 x_norm: 67.95288736419309\n",
      "Iteration 26 - grad_norm: 0.0007121398766398684 tk: 1.0 x_norm: 66.85210538322347\n",
      "Iteration 27 - grad_norm: 0.0006733727089787008 tk: 1.0 x_norm: 66.27575818961851\n",
      "Iteration 28 - grad_norm: 0.0006374724502729076 tk: 1.0 x_norm: 66.21837447067597\n",
      "Iteration 29 - grad_norm: 0.0006106643068685932 tk: 1.0 x_norm: 66.6604945874876\n",
      "Iteration 30 - grad_norm: 0.0005820467844169081 tk: 1.0 x_norm: 67.3594410816704\n",
      "Iteration 31 - grad_norm: 0.0005408033541224585 tk: 1.0 x_norm: 67.9537369898761\n",
      "Iteration 32 - grad_norm: 0.000496140086031849 tk: 1.0 x_norm: 68.26786077567732\n",
      "Iteration 33 - grad_norm: 0.00047270438200389266 tk: 1.0 x_norm: 68.15910372645311\n",
      "Iteration 34 - grad_norm: 0.00048387612497401426 tk: 1.0 x_norm: 67.58414815459903\n",
      "Iteration 35 - grad_norm: 0.0004949857444074271 tk: 1.0 x_norm: 66.8415994894382\n",
      "Iteration 36 - grad_norm: 0.0004830587107062824 tk: 1.0 x_norm: 66.2013133539269\n",
      "Iteration 37 - grad_norm: 0.00045357425432472284 tk: 1.0 x_norm: 65.76826874617754\n",
      "Iteration 38 - grad_norm: 0.00042773931332716576 tk: 1.0 x_norm: 65.70252926200358\n",
      "Iteration 39 - grad_norm: 0.0004186094679425927 tk: 1.0 x_norm: 65.99003723825544\n",
      "Iteration 40 - grad_norm: 0.00041343858748386523 tk: 1.0 x_norm: 66.35426551052815\n",
      "Iteration 41 - grad_norm: 0.0004022697535017772 tk: 1.0 x_norm: 66.55788455813193\n",
      "Iteration 42 - grad_norm: 0.00038180143129293043 tk: 1.0 x_norm: 66.47462204646745\n",
      "Iteration 43 - grad_norm: 0.0003565784345057724 tk: 1.0 x_norm: 66.05187093719077\n",
      "Iteration 44 - grad_norm: 0.00034110264322729904 tk: 1.0 x_norm: 65.52533249507341\n",
      "Iteration 45 - grad_norm: 0.0003426694582493172 tk: 1.0 x_norm: 65.08158142810443\n",
      "Iteration 46 - grad_norm: 0.0003684732813880633 tk: 1.0 x_norm: 64.85402514554046\n",
      "Iteration 47 - grad_norm: 0.00040412997955255087 tk: 1.0 x_norm: 64.88151195654284\n",
      "Iteration 48 - grad_norm: 0.00042213333317814175 tk: 1.0 x_norm: 65.14413768046361\n",
      "Iteration 49 - grad_norm: 0.00040287318633914587 tk: 1.0 x_norm: 65.44287737547742\n",
      "Iteration 50 - grad_norm: 0.0003666344055566153 tk: 1.0 x_norm: 65.62255594844466\n",
      "Iteration 51 - grad_norm: 0.00033128011177081985 tk: 1.0 x_norm: 65.62718310996702\n",
      "Iteration 52 - grad_norm: 0.00030289212583518816 tk: 1.0 x_norm: 65.45906510939469\n",
      "Iteration 53 - grad_norm: 0.00028180582867004176 tk: 1.0 x_norm: 65.20941241610925\n",
      "Iteration 54 - grad_norm: 0.00026894200452419345 tk: 1.0 x_norm: 65.00170960963544\n",
      "Iteration 55 - grad_norm: 0.00027781629094515296 tk: 1.0 x_norm: 64.95305740063749\n",
      "Iteration 56 - grad_norm: 0.0002967967177199764 tk: 1.0 x_norm: 65.02718560645997\n",
      "Iteration 57 - grad_norm: 0.00031281611824603167 tk: 1.0 x_norm: 65.17260428963303\n",
      "Iteration 58 - grad_norm: 0.00030969535289721744 tk: 1.0 x_norm: 65.28106275216449\n",
      "Iteration 59 - grad_norm: 0.00029124217321438797 tk: 1.0 x_norm: 65.28609374185417\n",
      "Iteration 60 - grad_norm: 0.0002709063017345956 tk: 1.0 x_norm: 65.1845188321373\n",
      "Iteration 61 - grad_norm: 0.00025693696164495684 tk: 1.0 x_norm: 65.03668367200459\n",
      "Iteration 62 - grad_norm: 0.00025143613901653023 tk: 1.0 x_norm: 64.90198743368664\n",
      "Iteration 63 - grad_norm: 0.00025720154144584034 tk: 1.0 x_norm: 64.8564570444718\n",
      "Iteration 64 - grad_norm: 0.0002721671706573624 tk: 1.0 x_norm: 64.95854253365678\n",
      "Iteration 65 - grad_norm: 0.00028435146169293856 tk: 1.0 x_norm: 65.16643596355497\n",
      "Iteration 66 - grad_norm: 0.00028290856587634823 tk: 1.0 x_norm: 65.3946086088038\n",
      "Iteration 67 - grad_norm: 0.00026580143747703433 tk: 1.0 x_norm: 65.53589472783098\n",
      "Iteration 68 - grad_norm: 0.00024012384964942608 tk: 1.0 x_norm: 65.53277574317818\n",
      "Iteration 69 - grad_norm: 0.00021757052632545988 tk: 1.0 x_norm: 65.37316764075369\n",
      "Iteration 70 - grad_norm: 0.0002061136369235042 tk: 1.0 x_norm: 65.13992753734306\n",
      "Iteration 71 - grad_norm: 0.000208200724182249 tk: 1.0 x_norm: 64.92712022591911\n",
      "Iteration 72 - grad_norm: 0.00022739214701570309 tk: 1.0 x_norm: 64.82108164299814\n",
      "Iteration 73 - grad_norm: 0.00025293698658785074 tk: 1.0 x_norm: 64.84510423938877\n",
      "Iteration 74 - grad_norm: 0.00027095668768186104 tk: 1.0 x_norm: 64.9423115506049\n",
      "Iteration 75 - grad_norm: 0.000271633251287112 tk: 1.0 x_norm: 65.03832823818831\n",
      "Iteration 76 - grad_norm: 0.0002553217488570924 tk: 1.0 x_norm: 65.06149701335129\n",
      "Iteration 77 - grad_norm: 0.00023183958088104114 tk: 1.0 x_norm: 64.9922623512936\n",
      "Iteration 78 - grad_norm: 0.00021097538776471464 tk: 1.0 x_norm: 64.87230843398723\n",
      "Iteration 79 - grad_norm: 0.000197107215238403 tk: 1.0 x_norm: 64.77244395746897\n",
      "Iteration 80 - grad_norm: 0.00019210223526650816 tk: 1.0 x_norm: 64.76963373929584\n",
      "Iteration 81 - grad_norm: 0.00019972055112803814 tk: 1.0 x_norm: 64.92306844224667\n",
      "Iteration 82 - grad_norm: 0.0002142167737529503 tk: 1.0 x_norm: 65.175004040805\n",
      "Iteration 83 - grad_norm: 0.00022406160302169143 tk: 1.0 x_norm: 65.42888040984931\n",
      "Iteration 84 - grad_norm: 0.00022310761362967336 tk: 1.0 x_norm: 65.6168710303731\n",
      "Iteration 85 - grad_norm: 0.00021105414748008206 tk: 1.0 x_norm: 65.69602435511749\n",
      "Iteration 86 - grad_norm: 0.00019287647373586965 tk: 1.0 x_norm: 65.66360448702385\n",
      "Iteration 87 - grad_norm: 0.00017796377473094094 tk: 1.0 x_norm: 65.5666691885147\n",
      "Iteration 88 - grad_norm: 0.00017126011443343236 tk: 1.0 x_norm: 65.50175304465651\n",
      "Iteration 89 - grad_norm: 0.00017215574140614422 tk: 1.0 x_norm: 65.53715261627754\n",
      "Iteration 90 - grad_norm: 0.00018277664694540675 tk: 1.0 x_norm: 65.68685059428456\n",
      "Iteration 91 - grad_norm: 0.00019827392391654535 tk: 1.0 x_norm: 65.90339209759593\n",
      "Iteration 92 - grad_norm: 0.00020989597979334277 tk: 1.0 x_norm: 66.13717475352544\n",
      "Iteration 93 - grad_norm: 0.00020955073909552577 tk: 1.0 x_norm: 66.29896973244166\n",
      "Iteration 94 - grad_norm: 0.0001973481278317285 tk: 1.0 x_norm: 66.31094391087183\n",
      "Iteration 95 - grad_norm: 0.000181264649539806 tk: 1.0 x_norm: 66.21111011098965\n",
      "Iteration 96 - grad_norm: 0.00016752563817009302 tk: 1.0 x_norm: 66.08169960137703\n",
      "Iteration 97 - grad_norm: 0.0001603997096920889 tk: 1.0 x_norm: 66.01364465386418\n",
      "Iteration 98 - grad_norm: 0.00016003278184955204 tk: 1.0 x_norm: 66.09693483692287\n",
      "Iteration 99 - grad_norm: 0.00016381451401201923 tk: 1.0 x_norm: 66.36677253938824\n",
      "Iteration 100 - grad_norm: 0.0001687814390948431 tk: 1.0 x_norm: 66.72061283286416\n",
      "Iteration 101 - grad_norm: 0.000173365873496343 tk: 1.0 x_norm: 67.05128008227862\n",
      "Iteration 102 - grad_norm: 0.0001775727859368129 tk: 1.0 x_norm: 67.28823194891385\n",
      "Iteration 103 - grad_norm: 0.0001805264634599637 tk: 1.0 x_norm: 67.34314446442674\n",
      "Iteration 104 - grad_norm: 0.00017867831009058305 tk: 1.0 x_norm: 67.20722814670413\n",
      "Iteration 105 - grad_norm: 0.00017039854943053852 tk: 1.0 x_norm: 66.97191825608355\n",
      "Iteration 106 - grad_norm: 0.00015945260180363937 tk: 1.0 x_norm: 66.75968437120379\n",
      "Iteration 107 - grad_norm: 0.0001505236267637822 tk: 1.0 x_norm: 66.64894059909466\n",
      "Iteration 108 - grad_norm: 0.0001468766374922231 tk: 1.0 x_norm: 66.71410581943866\n",
      "Iteration 109 - grad_norm: 0.0001453885181544397 tk: 1.0 x_norm: 66.93248852727353\n",
      "Iteration 110 - grad_norm: 0.00014195981830332103 tk: 1.0 x_norm: 67.18138011875712\n",
      "Iteration 111 - grad_norm: 0.00013763134239294766 tk: 1.0 x_norm: 67.34483620621606\n",
      "Iteration 112 - grad_norm: 0.000135115769356685 tk: 1.0 x_norm: 67.34182921580653\n",
      "Iteration 113 - grad_norm: 0.00013375848075421317 tk: 1.0 x_norm: 67.1772212318783\n",
      "Iteration 114 - grad_norm: 0.0001306595143205679 tk: 1.0 x_norm: 66.94785099106718\n",
      "Iteration 115 - grad_norm: 0.00012595780911159017 tk: 1.0 x_norm: 66.78731438671275\n",
      "Iteration 116 - grad_norm: 0.00012293008870478453 tk: 1.0 x_norm: 66.78700894681047\n",
      "Iteration 117 - grad_norm: 0.00012274225107835086 tk: 1.0 x_norm: 66.9732308878052\n",
      "Iteration 118 - grad_norm: 0.00012170517688985373 tk: 1.0 x_norm: 67.25023958962603\n",
      "Iteration 119 - grad_norm: 0.0001176972107543347 tk: 1.0 x_norm: 67.51083874275601\n",
      "Iteration 120 - grad_norm: 0.00011173988097871811 tk: 1.0 x_norm: 67.67862433600132\n",
      "Iteration 121 - grad_norm: 0.00010610875292069336 tk: 1.0 x_norm: 67.69274707104832\n",
      "Iteration 122 - grad_norm: 0.00010196793766571017 tk: 1.0 x_norm: 67.55101650273728\n",
      "BFGS - 迭代次数: 123\t最小值: 0.058483\t耗时: 15.513544s\n"
     ]
    }
   ],
   "source": [
    "#* 拟牛顿\n",
    "def quasi_newton_bfgs(f, f_grad, f_hessian, x0, mat_type='H', alpha=0.1, beta=0.5, epsilon=1e-6, max_iters=500):\n",
    "    assert mat_type in ['H','B']\n",
    "    xk = x0\n",
    "    hessian = f_hessian(x0)\n",
    "    mat_k = np.linalg.inv(hessian) if mat_type=='H' else hessian\n",
    "    iter_cnt = 0\n",
    "    t_s = time()\n",
    "    for idx in range(max_iters):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        dk = -mat_k@grad_k if mat_type=='H' else -np.linalg.inv(mat_k)@grad_k\n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, c1=1e-4, c2=0.9)\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "            return xk_next, iter_cnt, time()-t_s\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        mat_k = update_approximation_bfgs(mat=mat_k, sk=sk, yk=grad_next-grad_k, mat_type=mat_type)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt, time()-t_s\n",
    "\n",
    "def wolfe_condition(f, f_grad, xk, pk, c1=1e-4, c2=0.9, multiplier=1.25, t0=0, tmax=2):\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "np.random.seed(1000)\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# 使用 quasi-newton 求解无约束问题\n",
    "# quasi_newton_bfgs(f, f_grad, f_hessian, x0, mat_type='H', alpha=0.1, beta=0.5, epsilon=1e-6, maxIter=100):\n",
    "x_opt_bfgs, iter_cnt_bfgs, t_bfgs = quasi_newton_bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, mat_type='H', epsilon=1e-4, max_iters=200)\n",
    "# x_opt_bfgs, iter_cnt_bfgs, t_bfgs = quasi_newton_bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, mat_type='B', epsilon=1e-8, max_iters=200)\n",
    "print(f'BFGS - 迭代次数: {iter_cnt_bfgs}\\t最小值: {f(x_opt_bfgs):>2f}\\t耗时: {t_bfgs:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure l-bfgs w/o constraints (D=500) ***aborted***\n",
    "\n",
    "Not very appropriate for IPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.2874976403305301 tk: 1.0 x_norm: 0.6034130174696074\n",
      "Iteration 2 - grad_norm: 0.2838872583169178 tk: 1.25 x_norm: 0.6295060924456327\n",
      "Iteration 3 - grad_norm: 0.28242345968900984 tk: 3.0517578125 x_norm: 0.6542786523350346\n",
      "Iteration 4 - grad_norm: 0.28217543253346206 tk: 3.814697265625 x_norm: 0.6749511140438361\n",
      "Iteration 5 - grad_norm: 0.2821953930122363 tk: 5.9604644775390625 x_norm: 0.6953605998289375\n",
      "Iteration 6 - grad_norm: 0.28217480273736845 tk: 5.9604644775390625 x_norm: 0.7144840093383312\n",
      "Iteration 7 - grad_norm: 0.2821900982989895 tk: 7.450580596923828 x_norm: 0.7425670327511641\n",
      "Iteration 8 - grad_norm: 0.2820953597561407 tk: 11.641532182693481 x_norm: 0.782945591576831\n",
      "Iteration 9 - grad_norm: 0.28219147481740264 tk: 7.450580596923828 x_norm: 0.8100803607423993\n",
      "Iteration 10 - grad_norm: 0.282316987039029 tk: 1.5625 x_norm: 0.8507287723275746\n",
      "Iteration 11 - grad_norm: 0.2825591070321152 tk: 1.0 x_norm: 0.9281565145082193\n",
      "Iteration 12 - grad_norm: 0.28284588710723013 tk: 1.0 x_norm: 1.016427673000379\n",
      "Iteration 13 - grad_norm: 0.28318353701809906 tk: 1.0 x_norm: 1.1146069916278616\n",
      "Iteration 14 - grad_norm: 0.2835713586151958 tk: 1.0 x_norm: 1.2202781596444714\n",
      "Iteration 15 - grad_norm: 0.28400950195151586 tk: 1.0 x_norm: 1.3318416238473683\n",
      "L-BFGS - 迭代次数: 15\t最小值: 0.417851\t耗时: 11.990113s\n"
     ]
    }
   ],
   "source": [
    "class Container():\n",
    "    def __init__(self,numStore,dim) -> None:\n",
    "        self.ss = [np.zeros(dim) for i in range(numStore)]\n",
    "        self.ys = [np.zeros(dim) for i in range(numStore)]\n",
    "        self.cur_idx = 0\n",
    "        self.numStore = numStore\n",
    "        self.dim = dim\n",
    "    def update_container(self,sk,yk):\n",
    "        self.ss[self.cur_idx] = sk\n",
    "        self.ys[self.cur_idx] = yk\n",
    "        self.cur_idx = (self.cur_idx+1)%self.numStore\n",
    "    def cal_descent_direction(self,grad):\n",
    "        # self.cur_idx # last index\n",
    "        start_idx = self.cur_idx-1 # first index\n",
    "        for i in range(start_idx,start_idx-self.numStore,-1):\n",
    "            ysTss = self.ss[i]@self.ys[i]\n",
    "            alpha_i = (self.ss[i]@grad)/ysTss if ysTss != 0 else 0\n",
    "            grad -= alpha_i*self.ys[i]\n",
    "        r = np.eye(self.dim)@grad\n",
    "        for i in range(self.cur_idx, self.cur_idx+self.numStore):\n",
    "            trueId = i if i<self.numStore else i-self.numStore\n",
    "            ysTss = self.ss[trueId]@self.ys[trueId]\n",
    "            alpha_i = 0\n",
    "            beta = 0\n",
    "            if ysTss!=0:\n",
    "                alpha_i = (self.ss[trueId]@grad)/ysTss\n",
    "                beta = (self.ys[trueId]@r)/ysTss\n",
    "            r = r + (alpha_i-beta)*self.ss[trueId]\n",
    "        return r\n",
    "        \n",
    "\n",
    "#* 拟牛顿\n",
    "def quasi_newton_lbfgs(f, f_grad, f_hessian, x0, m=15, epsilon=1e-6, max_iters=500):\n",
    "    xk = x0\n",
    "    # hessian = f_hessian(x0)\n",
    "    # mat_k = np.linalg.inv(hessian) \n",
    "    # mat_k = np.eye(n)\n",
    "    dk = -f_grad(xk)\n",
    "    iter_cnt = 0\n",
    "    container = Container(numStore=m, dim=xk.size)\n",
    "    t_s = time()\n",
    "    for idx in range(max_iters):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        # dk = -mat_k@grad_k \n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, c1=1e-4, c2=0.9)\n",
    "        if tk<0:\n",
    "            return xk, iter_cnt-1, time()-t_s\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "            return xk_next, iter_cnt, time()-t_s\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        # mat_k = update_approximation_lbfgs(mat=mat_k, sk=sk, yk=grad_next-grad_k)\n",
    "        container.update_container(sk=sk, yk=grad_next-grad_k)\n",
    "        dk = -container.cal_descent_direction(grad_next)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt, time()-t_s\n",
    "\n",
    "def wolfe_condition(f, f_grad, xk, pk, c1=1e-4, c2=0.9, multiplier=1.25, t0=0, tmax=2):\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "\n",
    "np.random.seed(1000)\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# 使用 quasi-newton 求解无约束问题\n",
    "x_opt_lbfgs, iter_cnt_lbfgs, t_lbfgs = quasi_newton_lbfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, m=15, epsilon=1e-3, max_iters=200)\n",
    "print(f'L-BFGS - 迭代次数: {iter_cnt_lbfgs}\\t最小值: {f(x_opt_lbfgs):>2f}\\t耗时: {t_lbfgs:>2f}s')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected gradient methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_x \\quad \\frac{1}{m}\\sum_{i=1}^{m}&\\log\\left( 1 + \\exp(-b_i x^\\top a_i)\\right) + \\frac{1}{100m}\\left\\|x\\right\\|^2   \\\\\n",
    "\\text{subject to }\\quad\\quad &\\left\\|x\\right\\|_2 \\le D/2\n",
    "\\end{aligned}\n",
    "$$\n",
    "feasible set is a L2 ball $\\mathcal{X} = \\{x:\\left\\|x\\right\\|_2 \\le D/2\\}$\n",
    "\n",
    "projected gradient method\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{k+1} &= x_k - \\alpha_k p_k\\\\\n",
    "x_{k+1} &= \\Pi_\\mathcal{X}(y_{k+1})\\\\\n",
    "\\Pi_\\mathcal{X}(x) &= \\left\\{\\begin{matrix}\n",
    "x & \\left\\|x\\right\\|_2 \\le D/2  \\\\\n",
    "\\frac{D}{2\\left\\|x\\right\\|_2}x &  \\left\\|x\\right\\|_2 > D/2\\\\\n",
    "\\end{matrix}\\right.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Grad. Norm.: 0.052109680543728935 Norm. Diff.: 3.2435150931206587 tk: 5 x_norm: 3.1948158750857765\n",
      "Iteration 1 - Grad. Norm.: 0.03874538795695223 Norm. Diff.: 0.2605484027186447 tk: 5 x_norm: 3.3007664609434073\n",
      "Iteration 2 - Grad. Norm.: 0.03247236111795165 Norm. Diff.: 0.1937269397847612 tk: 5 x_norm: 3.39832364755234\n",
      "Iteration 3 - Grad. Norm.: 0.028707614758793267 Norm. Diff.: 0.16236180558975827 tk: 5 x_norm: 3.488927444461727\n",
      "Iteration 4 - Grad. Norm.: 0.026137955548665787 Norm. Diff.: 0.14353807379396633 tk: 5 x_norm: 3.573880284211031\n",
      "Iteration 5 - Grad. Norm.: 0.024238255924168794 Norm. Diff.: 0.13068977774332896 tk: 5 x_norm: 3.654164258131663\n",
      "Iteration 6 - Grad. Norm.: 0.02275515238979999 Norm. Diff.: 0.12119127962084399 tk: 5 x_norm: 3.730525846831219\n",
      "Iteration 7 - Grad. Norm.: 0.021550498078459098 Norm. Diff.: 0.11377576194899999 tk: 5 x_norm: 3.803544343824972\n",
      "Iteration 8 - Grad. Norm.: 0.02054217078675201 Norm. Diff.: 0.10775249039229548 tk: 5 x_norm: 3.873677891009221\n",
      "Iteration 9 - Grad. Norm.: 0.01967811317099874 Norm. Diff.: 0.10271085393376006 tk: 5 x_norm: 3.9412945794501484\n",
      "Iteration 10 - Grad. Norm.: 0.018923649545407387 Norm. Diff.: 0.09839056585499374 tk: 5 x_norm: 4.006694049320368\n",
      "Iteration 11 - Grad. Norm.: 0.018254755248512777 Norm. Diff.: 0.09461824772703696 tk: 5 x_norm: 4.070122926376995\n",
      "Iteration 12 - Grad. Norm.: 0.017654243223426912 Norm. Diff.: 0.09127377624256387 tk: 5 x_norm: 4.131786136275055\n",
      "Iteration 13 - Grad. Norm.: 0.017109485628512194 Norm. Diff.: 0.08827121611713455 tk: 5 x_norm: 4.1918553780763546\n",
      "Iteration 14 - Grad. Norm.: 0.01661099162910515 Norm. Diff.: 0.08554742814256094 tk: 5 x_norm: 4.250475586797769\n",
      "Iteration 15 - Grad. Norm.: 0.016151486504397905 Norm. Diff.: 0.08305495814552576 tk: 5 x_norm: 4.307769938849529\n",
      "Iteration 16 - Grad. Norm.: 0.015725296588775545 Norm. Diff.: 0.08075743252198951 tk: 5 x_norm: 4.3638437802112815\n",
      "Iteration 17 - Grad. Norm.: 0.015327927455063974 Norm. Diff.: 0.0786264829438777 tk: 5 x_norm: 4.41878774419224\n",
      "Iteration 18 - Grad. Norm.: 0.01495576795345957 Norm. Diff.: 0.07663963727531989 tk: 5 x_norm: 4.472680250221823\n",
      "Iteration 19 - Grad. Norm.: 0.014605878410723272 Norm. Diff.: 0.07477883976729786 tk: 5 x_norm: 4.525589523576644\n",
      "Iteration 20 - Grad. Norm.: 0.014275836426987468 Norm. Diff.: 0.07302939205361637 tk: 5 x_norm: 4.577575239960449\n",
      "Iteration 21 - Grad. Norm.: 0.013963622905490478 Norm. Diff.: 0.07137918213493735 tk: 5 x_norm: 4.628689873249136\n",
      "Iteration 22 - Grad. Norm.: 0.013667536698368668 Norm. Diff.: 0.0698181145274524 tk: 5 x_norm: 4.6789798061940004\n",
      "Iteration 23 - Grad. Norm.: 0.013386129933440693 Norm. Diff.: 0.0683376834918433 tk: 5 x_norm: 4.728486250282938\n",
      "Iteration 24 - Grad. Norm.: 0.013118158498739279 Norm. Diff.: 0.06693064966720347 tk: 5 x_norm: 4.777246010846559\n",
      "Iteration 25 - Grad. Norm.: 0.012862543774248811 Norm. Diff.: 0.06559079249369638 tk: 5 x_norm: 4.825292125878381\n",
      "Iteration 26 - Grad. Norm.: 0.012618342799414947 Norm. Diff.: 0.06431271887124403 tk: 5 x_norm: 4.872654401231959\n",
      "Iteration 27 - Grad. Norm.: 0.012384724827479992 Norm. Diff.: 0.06309171399707474 tk: 5 x_norm: 4.919359860382872\n",
      "Iteration 28 - Grad. Norm.: 0.01216095275544152 Norm. Diff.: 0.06192362413739998 tk: 5 x_norm: 4.9654331234576885\n",
      "Iteration 29 - Grad. Norm.: 0.01194636830344556 Norm. Diff.: 0.060804763777207595 tk: 5 x_norm: 5.010896727490012\n",
      "Iteration 30 - Grad. Norm.: 0.011740380096867469 Norm. Diff.: 0.05973184151722783 tk: 5 x_norm: 5.055771397686673\n",
      "Iteration 31 - Grad. Norm.: 0.011542454009604618 Norm. Diff.: 0.05870190048433734 tk: 5 x_norm: 5.100076277744397\n",
      "Iteration 32 - Grad. Norm.: 0.011352105279463776 Norm. Diff.: 0.057712270048023116 tk: 5 x_norm: 5.1438291258517665\n",
      "Iteration 33 - Grad. Norm.: 0.011168892020605283 Norm. Diff.: 0.056760526397318885 tk: 5 x_norm: 5.187046481870443\n",
      "Iteration 34 - Grad. Norm.: 0.010992409844044392 Norm. Diff.: 0.05584446010302644 tk: 5 x_norm: 5.229743810258475\n",
      "Iteration 35 - Grad. Norm.: 0.01082228736248468 Norm. Diff.: 0.05496204922022196 tk: 5 x_norm: 5.271935622535017\n",
      "Iteration 36 - Grad. Norm.: 0.010658182405517774 Norm. Diff.: 0.054111436812423426 tk: 5 x_norm: 5.313635582457262\n",
      "Iteration 37 - Grad. Norm.: 0.010499778809307676 Norm. Diff.: 0.053290912027588865 tk: 5 x_norm: 5.3548565965613575\n",
      "Iteration 38 - Grad. Norm.: 0.010346783674123578 Norm. Diff.: 0.052498894046538375 tk: 5 x_norm: 5.395610892289318\n",
      "Iteration 39 - Grad. Norm.: 0.010198925005613414 Norm. Diff.: 0.0517339183706179 tk: 5 x_norm: 5.435910085567235\n",
      "Iteration 40 - Grad. Norm.: 0.010055949673115894 Norm. Diff.: 0.050994625028067064 tk: 5 x_norm: 5.475765239403506\n",
      "Iteration 41 - Grad. Norm.: 0.00991762163179917 Norm. Diff.: 0.05027974836557943 tk: 5 x_norm: 5.515186914828691\n",
      "Iteration 42 - Grad. Norm.: 0.009783720365906778 Norm. Diff.: 0.049588108158995864 tk: 5 x_norm: 5.554185215292414\n",
      "Iteration 43 - Grad. Norm.: 0.009654039518584677 Norm. Diff.: 0.04891860182953392 tk: 5 x_norm: 5.592769825460382\n",
      "Iteration 44 - Grad. Norm.: 0.009528385680190871 Norm. Diff.: 0.04827019759292336 tk: 5 x_norm: 5.630950045210301\n",
      "Iteration 45 - Grad. Norm.: 0.00940657731205772 Norm. Diff.: 0.04764192840095437 tk: 5 x_norm: 5.668734819504399\n",
      "Iteration 46 - Grad. Norm.: 0.009288443786697239 Norm. Diff.: 0.047032886560288596 tk: 5 x_norm: 5.706132764714663\n",
      "Iteration 47 - Grad. Norm.: 0.009173824528649512 Norm. Diff.: 0.04644221893348621 tk: 5 x_norm: 5.743152191891231\n",
      "Iteration 48 - Grad. Norm.: 0.009062568242754069 Norm. Diff.: 0.04586912264324751 tk: 5 x_norm: 5.779801127392229\n",
      "Iteration 49 - Grad. Norm.: 0.008954532218712436 Norm. Diff.: 0.04531284121377029 tk: 5 x_norm: 5.816087331232378\n",
      "Iteration 50 - Grad. Norm.: 0.008849581702513225 Norm. Diff.: 0.04477266109356217 tk: 5 x_norm: 5.852018313456026\n",
      "Iteration 51 - Grad. Norm.: 0.008747589326689507 Norm. Diff.: 0.0442479085125661 tk: 5 x_norm: 5.887601348796511\n",
      "Iteration 52 - Grad. Norm.: 0.008648434592534953 Norm. Diff.: 0.04373794663344752 tk: 5 x_norm: 5.922843489846482\n",
      "Iteration 53 - Grad. Norm.: 0.008552003398367881 Norm. Diff.: 0.04324217296267474 tk: 5 x_norm: 5.9577515789322195\n",
      "Iteration 54 - Grad. Norm.: 0.00845818760873844 Norm. Diff.: 0.0427600169918394 tk: 5 x_norm: 5.992332258857884\n",
      "Iteration 55 - Grad. Norm.: 0.008366884660153454 Norm. Diff.: 0.04229093804369218 tk: 5 x_norm: 6.026591982662588\n",
      "Iteration 56 - Grad. Norm.: 0.00827799719946767 Norm. Diff.: 0.04183442330076726 tk: 5 x_norm: 6.060537022513511\n",
      "Iteration 57 - Grad. Norm.: 0.00819143275157915 Norm. Diff.: 0.0413899859973384 tk: 5 x_norm: 6.094173477841201\n",
      "Iteration 58 - Grad. Norm.: 0.008107103413483375 Norm. Diff.: 0.040957163757895695 tk: 5 x_norm: 6.127507282808913\n",
      "Iteration 59 - Grad. Norm.: 0.008024925572098265 Norm. Diff.: 0.04053551706741689 tk: 5 x_norm: 6.160544213195214\n",
      "Iteration 60 - Grad. Norm.: 0.007944819643579409 Norm. Diff.: 0.04012462786049136 tk: 5 x_norm: 6.193289892758523\n",
      "Iteration 61 - Grad. Norm.: 0.007866709832110139 Norm. Diff.: 0.03972409821789706 tk: 5 x_norm: 6.2257497991429025\n",
      "Iteration 62 - Grad. Norm.: 0.007790523906379961 Norm. Diff.: 0.03933354916055067 tk: 5 x_norm: 6.257929269376674\n",
      "Iteration 63 - Grad. Norm.: 0.007716192992164045 Norm. Diff.: 0.038952619531899824 tk: 5 x_norm: 6.289833505008441\n",
      "Iteration 64 - Grad. Norm.: 0.007643651379589124 Norm. Diff.: 0.03858096496082022 tk: 5 x_norm: 6.32146757691937\n",
      "Iteration 65 - Grad. Norm.: 0.007572836343822244 Norm. Diff.: 0.0382182568979456 tk: 5 x_norm: 6.352836429845363\n",
      "Iteration 66 - Grad. Norm.: 0.007503687978050367 Norm. Diff.: 0.03786418171911121 tk: 5 x_norm: 6.383944886638537\n",
      "Iteration 67 - Grad. Norm.: 0.007436149037734645 Norm. Diff.: 0.03751843989025181 tk: 5 x_norm: 6.414797652293544\n",
      "Iteration 68 - Grad. Norm.: 0.007370164795224553 Norm. Diff.: 0.037180745188673245 tk: 5 x_norm: 6.445399317761025\n",
      "Iteration 69 - Grad. Norm.: 0.007305682903906841 Norm. Diff.: 0.036850823976122764 tk: 5 x_norm: 6.4757543635677415\n",
      "Iteration 70 - Grad. Norm.: 0.007242653271142864 Norm. Diff.: 0.03652841451953422 tk: 5 x_norm: 6.505867163260412\n",
      "Iteration 71 - Grad. Norm.: 0.007181027939318518 Norm. Diff.: 0.03621326635571432 tk: 5 x_norm: 6.535741986688262\n",
      "Iteration 72 - Grad. Norm.: 0.007120760974392532 Norm. Diff.: 0.03590513969659258 tk: 5 x_norm: 6.565383003137404\n",
      "Iteration 73 - Grad. Norm.: 0.00706180836138476 Norm. Diff.: 0.0356038048719627 tk: 5 x_norm: 6.594794284328683\n",
      "Iteration 74 - Grad. Norm.: 0.0070041279062951845 Norm. Diff.: 0.03530904180692379 tk: 5 x_norm: 6.623979807289179\n",
      "Iteration 75 - Grad. Norm.: 0.006947679143988687 Norm. Diff.: 0.035020639531475925 tk: 5 x_norm: 6.652943457106436\n",
      "Iteration 76 - Grad. Norm.: 0.006892423251620309 Norm. Diff.: 0.03473839571994344 tk: 5 x_norm: 6.681689029573478\n",
      "Iteration 77 - Grad. Norm.: 0.006838322967211188 Norm. Diff.: 0.03446211625810151 tk: 5 x_norm: 6.710220233731739\n",
      "Iteration 78 - Grad. Norm.: 0.006785342513017727 Norm. Diff.: 0.03419161483605594 tk: 5 x_norm: 6.738540694318314\n",
      "Iteration 79 - Grad. Norm.: 0.006733447523365182 Norm. Diff.: 0.03392671256508867 tk: 5 x_norm: 6.766653954123283\n",
      "Iteration 80 - Grad. Norm.: 0.006682604976643397 Norm. Diff.: 0.033667237616825926 tk: 5 x_norm: 6.79456347626223\n",
      "Iteration 81 - Grad. Norm.: 0.006632783131185928 Norm. Diff.: 0.033413024883216985 tk: 5 x_norm: 6.8222726463686225\n",
      "Iteration 82 - Grad. Norm.: 0.006583951464775524 Norm. Diff.: 0.03316391565592961 tk: 5 x_norm: 6.849784774710303\n",
      "Iteration 83 - Grad. Norm.: 0.006536080617538423 Norm. Diff.: 0.032919757323877585 tk: 5 x_norm: 6.877103098233852\n",
      "Iteration 84 - Grad. Norm.: 0.006489142338007995 Norm. Diff.: 0.03268040308769206 tk: 5 x_norm: 6.904230782540435\n",
      "Iteration 85 - Grad. Norm.: 0.006443109432154281 Norm. Diff.: 0.032445711690039986 tk: 5 x_norm: 6.931170923796241\n",
      "Iteration 86 - Grad. Norm.: 0.006397955715191257 Norm. Diff.: 0.032215547160771414 tk: 5 x_norm: 6.957926550580559\n",
      "Iteration 87 - Grad. Norm.: 0.006353655965987035 Norm. Diff.: 0.031989778575956314 tk: 5 x_norm: 6.984500625674142\n",
      "Iteration 88 - Grad. Norm.: 0.006310185883914801 Norm. Diff.: 0.03176827982993518 tk: 5 x_norm: 7.010896047790501\n",
      "Iteration 89 - Grad. Norm.: 0.006267522047993853 Norm. Diff.: 0.03155092941957407 tk: 5 x_norm: 7.037115653252374\n",
      "Iteration 90 - Grad. Norm.: 0.0062256418781806155 Norm. Diff.: 0.03133761023996934 tk: 5 x_norm: 7.063162217615676\n",
      "Iteration 91 - Grad. Norm.: 0.006184523598679103 Norm. Diff.: 0.0311282093909031 tk: 5 x_norm: 7.0890384572429275\n",
      "Iteration 92 - Grad. Norm.: 0.006144146203149559 Norm. Diff.: 0.030922617993395508 tk: 5 x_norm: 7.114747030828146\n",
      "Iteration 93 - Grad. Norm.: 0.006104489421701856 Norm. Diff.: 0.030720731015747803 tk: 5 x_norm: 7.140290540875032\n",
      "Iteration 94 - Grad. Norm.: 0.006065533689568208 Norm. Diff.: 0.03052244710850927 tk: 5 x_norm: 7.1656715351301425\n",
      "Iteration 95 - Grad. Norm.: 0.0060272601173566075 Norm. Diff.: 0.03032766844784107 tk: 5 x_norm: 7.190892507972765\n",
      "Iteration 96 - Grad. Norm.: 0.0059896504627929064 Norm. Diff.: 0.030136300586783023 tk: 5 x_norm: 7.215955901762982\n",
      "Iteration 97 - Grad. Norm.: 0.005952687103865626 Norm. Diff.: 0.02994825231396458 tk: 5 x_norm: 7.2408641081494585\n",
      "Iteration 98 - Grad. Norm.: 0.005916353013292988 Norm. Diff.: 0.029763435519328127 tk: 5 x_norm: 7.2656194693383656\n",
      "Iteration 99 - Grad. Norm.: 0.005880631734237009 Norm. Diff.: 0.029581765066464892 tk: 5 x_norm: 7.290224279324753\n",
      "Iteration 100 - Grad. Norm.: 0.005845507357194208 Norm. Diff.: 0.029403158671184995 tk: 5 x_norm: 7.3146807850877344\n",
      "Iteration 101 - Grad. Norm.: 0.005810964497996975 Norm. Diff.: 0.029227536785970978 tk: 5 x_norm: 7.338991187750657\n",
      "Iteration 102 - Grad. Norm.: 0.005776988276863809 Norm. Diff.: 0.029054822489984856 tk: 5 x_norm: 7.363157643707488\n",
      "Iteration 103 - Grad. Norm.: 0.005743564298440498 Norm. Diff.: 0.028884941384319053 tk: 5 x_norm: 7.38718226571656\n",
      "Iteration 104 - Grad. Norm.: 0.005710678632777918 Norm. Diff.: 0.028717821492202488 tk: 5 x_norm: 7.4110671239627255\n",
      "Iteration 105 - Grad. Norm.: 0.005678317797195463 Norm. Diff.: 0.028553393163889607 tk: 5 x_norm: 7.434814247089042\n",
      "Iteration 106 - Grad. Norm.: 0.00564646873898221 Norm. Diff.: 0.02839158898597736 tk: 5 x_norm: 7.458425623198962\n",
      "Iteration 107 - Grad. Norm.: 0.005615118818890957 Norm. Diff.: 0.02823234369491105 tk: 5 x_norm: 7.481903200829988\n",
      "Iteration 108 - Grad. Norm.: 0.005584255795382843 Norm. Diff.: 0.02807559409445473 tk: 5 x_norm: 7.505248889899782\n",
      "Iteration 109 - Grad. Norm.: 0.005553867809582866 Norm. Diff.: 0.027921278976914203 tk: 5 x_norm: 7.528464562625599\n",
      "Iteration 110 - Grad. Norm.: 0.005523943370908989 Norm. Diff.: 0.027769339047914312 tk: 5 x_norm: 7.551552054417913\n",
      "Iteration 111 - Grad. Norm.: 0.0054944713433397645 Norm. Diff.: 0.027619716854544892 tk: 5 x_norm: 7.574513164749113\n",
      "Iteration 112 - Grad. Norm.: 0.005465440932287348 Norm. Diff.: 0.027472356716698815 tk: 5 x_norm: 7.597349657998026\n",
      "Iteration 113 - Grad. Norm.: 0.005436841672044922 Norm. Diff.: 0.027327204661436767 tk: 5 x_norm: 7.620063264271106\n",
      "Iteration 114 - Grad. Norm.: 0.0054086634137791205 Norm. Diff.: 0.02718420836022453 tk: 5 x_norm: 7.64265568020099\n",
      "Iteration 115 - Grad. Norm.: 0.005380896314039965 Norm. Diff.: 0.02704331706889565 tk: 5 x_norm: 7.6651285697231835\n",
      "Iteration 116 - Grad. Norm.: 0.005353530823762216 Norm. Diff.: 0.026904481570199798 tk: 5 x_norm: 7.687483564831559\n",
      "Iteration 117 - Grad. Norm.: 0.005326557677733729 Norm. Diff.: 0.026767654118811112 tk: 5 x_norm: 7.709722266313349\n",
      "Iteration 118 - Grad. Norm.: 0.00529996788450755 Norm. Diff.: 0.026632788388668657 tk: 5 x_norm: 7.7318462444642835\n",
      "Iteration 119 - Grad. Norm.: 0.005273752716736079 Norm. Diff.: 0.02649983942253774 tk: 5 x_norm: 7.753857039784492\n",
      "Iteration 120 - Grad. Norm.: 0.005247903701906571 Norm. Diff.: 0.026368763583680337 tk: 5 x_norm: 7.775756163655808\n",
      "Iteration 121 - Grad. Norm.: 0.005222412613458654 Norm. Diff.: 0.0262395185095329 tk: 5 x_norm: 7.797545099001002\n",
      "Iteration 122 - Grad. Norm.: 0.0051972714622654 Norm. Diff.: 0.026112063067293207 tk: 5 x_norm: 7.81922530092558\n",
      "Iteration 123 - Grad. Norm.: 0.005172472488460624 Norm. Diff.: 0.025986357311326997 tk: 5 x_norm: 7.840798197342625\n",
      "Iteration 124 - Grad. Norm.: 0.005148008153596033 Norm. Diff.: 0.02586236244230311 tk: 5 x_norm: 7.8622651895812625\n",
      "Iteration 125 - Grad. Norm.: 0.005123871133112667 Norm. Diff.: 0.025740040767980154 tk: 5 x_norm: 7.883627652979195\n",
      "Iteration 126 - Grad. Norm.: 0.005100054309111971 Norm. Diff.: 0.025619355665563368 tk: 5 x_norm: 7.904886937459868\n",
      "Iteration 127 - Grad. Norm.: 0.005076550763412619 Norm. Diff.: 0.02550027154555984 tk: 5 x_norm: 7.926044368094676\n",
      "Iteration 128 - Grad. Norm.: 0.005053353770879928 Norm. Diff.: 0.02538275381706305 tk: 5 x_norm: 7.947101245650691\n",
      "Iteration 129 - Grad. Norm.: 0.005030456793015412 Norm. Diff.: 0.025266768854399612 tk: 5 x_norm: 7.968058847124352\n",
      "Iteration 130 - Grad. Norm.: 0.005007853471794668 Norm. Diff.: 0.025152283965077023 tk: 5 x_norm: 7.988918426261543\n",
      "Iteration 131 - Grad. Norm.: 0.004985537623742439 Norm. Diff.: 0.025039267358973338 tk: 5 x_norm: 8.009681214064443\n",
      "Iteration 132 - Grad. Norm.: 0.00496350323423421 Norm. Diff.: 0.024927688118712182 tk: 5 x_norm: 8.030348419285577\n",
      "Iteration 133 - Grad. Norm.: 0.00494174445201434 Norm. Diff.: 0.024817516171171027 tk: 5 x_norm: 8.050921228909427\n",
      "Iteration 134 - Grad. Norm.: 0.004920255583921091 Norm. Diff.: 0.02470872226007168 tk: 5 x_norm: 8.071400808621982\n",
      "Iteration 135 - Grad. Norm.: 0.004899031089809628 Norm. Diff.: 0.02460127791960552 tk: 5 x_norm: 8.091788303268565\n",
      "Iteration 136 - Grad. Norm.: 0.004878065577664277 Norm. Diff.: 0.024495155449048164 tk: 5 x_norm: 8.112084837300293\n",
      "Iteration 137 - Grad. Norm.: 0.0048573537988919 Norm. Diff.: 0.02439032788832139 tk: 5 x_norm: 8.132291515209515\n",
      "Iteration 138 - Grad. Norm.: 0.0048368906437886855 Norm. Diff.: 0.02428676899445943 tk: 5 x_norm: 8.152409421954514\n",
      "Iteration 139 - Grad. Norm.: 0.004816671137172886 Norm. Diff.: 0.024184453218943403 tk: 5 x_norm: 8.1724396233738\n",
      "Iteration 140 - Grad. Norm.: 0.004796690434176531 Norm. Diff.: 0.024083355685864474 tk: 5 x_norm: 8.192383166590304\n",
      "Iteration 141 - Grad. Norm.: 0.004776943816189436 Norm. Diff.: 0.023983452170882682 tk: 5 x_norm: 8.212241080405732\n",
      "Iteration 142 - Grad. Norm.: 0.004757426686949174 Norm. Diff.: 0.02388471908094717 tk: 5 x_norm: 8.232014375685388\n",
      "Iteration 143 - Grad. Norm.: 0.004738134568770871 Norm. Diff.: 0.023787133434745852 tk: 5 x_norm: 8.251704045733728\n",
      "Iteration 144 - Grad. Norm.: 0.004719063098911212 Norm. Diff.: 0.02369067284385434 tk: 5 x_norm: 8.271311066660848\n",
      "Iteration 145 - Grad. Norm.: 0.004700208026061033 Norm. Diff.: 0.02359531549455605 tk: 5 x_norm: 8.290836397740293\n",
      "Iteration 146 - Grad. Norm.: 0.004681565206961339 Norm. Diff.: 0.023501040130305167 tk: 5 x_norm: 8.310280981758263\n",
      "Iteration 147 - Grad. Norm.: 0.0046631306031377695 Norm. Diff.: 0.023407826034806744 tk: 5 x_norm: 8.329645745354577\n",
      "Iteration 148 - Grad. Norm.: 0.0046449002777486995 Norm. Diff.: 0.023315653015688834 tk: 5 x_norm: 8.348931599355577\n",
      "Iteration 149 - Grad. Norm.: 0.004626870392542518 Norm. Diff.: 0.02322450138874355 tk: 5 x_norm: 8.368139439099176\n",
      "Iteration 150 - Grad. Norm.: 0.004609037204919693 Norm. Diff.: 0.023134351962712657 tk: 5 x_norm: 8.387270144752287\n",
      "Iteration 151 - Grad. Norm.: 0.004591397065095509 Norm. Diff.: 0.023045186024598423 tk: 5 x_norm: 8.406324581620847\n",
      "Iteration 152 - Grad. Norm.: 0.004573946413359559 Norm. Diff.: 0.022956985325477564 tk: 5 x_norm: 8.425303600452606\n",
      "Iteration 153 - Grad. Norm.: 0.004556681777428187 Norm. Diff.: 0.022869732066797793 tk: 5 x_norm: 8.444208037732885\n",
      "Iteration 154 - Grad. Norm.: 0.004539599769886302 Norm. Diff.: 0.02278340888714096 tk: 5 x_norm: 8.463038715973518\n",
      "Iteration 155 - Grad. Norm.: 0.00452269708571512 Norm. Diff.: 0.022697998849431487 tk: 5 x_norm: 8.481796443995103\n",
      "Iteration 156 - Grad. Norm.: 0.004505970499902511 Norm. Diff.: 0.022613485428575664 tk: 5 x_norm: 8.500482017202804\n",
      "Iteration 157 - Grad. Norm.: 0.004489416865132891 Norm. Diff.: 0.022529852499512535 tk: 5 x_norm: 8.519096217855822\n",
      "Iteration 158 - Grad. Norm.: 0.004473033109553527 Norm. Diff.: 0.022447084325664415 tk: 5 x_norm: 8.537639815330706\n",
      "Iteration 159 - Grad. Norm.: 0.0044568162346144986 Norm. Diff.: 0.02236516554776766 tk: 5 x_norm: 8.556113566378716\n",
      "Iteration 160 - Grad. Norm.: 0.004440763312979452 Norm. Diff.: 0.022284081173072536 tk: 5 x_norm: 8.574518215377303\n",
      "Iteration 161 - Grad. Norm.: 0.004424871486504592 Norm. Diff.: 0.022203816564897224 tk: 5 x_norm: 8.592854494575944\n",
      "Iteration 162 - Grad. Norm.: 0.0044091379642833385 Norm. Diff.: 0.02212435743252299 tk: 5 x_norm: 8.611123124336434\n",
      "Iteration 163 - Grad. Norm.: 0.004393560020754221 Norm. Diff.: 0.022045689821416695 tk: 5 x_norm: 8.62932481336775\n",
      "Iteration 164 - Grad. Norm.: 0.004378134993869741 Norm. Diff.: 0.021967800103771133 tk: 5 x_norm: 8.647460258955713\n",
      "Iteration 165 - Grad. Norm.: 0.0043628602833239135 Norm. Diff.: 0.021890674969348663 tk: 5 x_norm: 8.665530147187466\n",
      "Iteration 166 - Grad. Norm.: 0.004347733348836403 Norm. Diff.: 0.021814301416619535 tk: 5 x_norm: 8.683535153170999\n",
      "Iteration 167 - Grad. Norm.: 0.00433275170849119 Norm. Diff.: 0.02173866674418197 tk: 5 x_norm: 8.70147594124977\n",
      "Iteration 168 - Grad. Norm.: 0.004317912937127823 Norm. Diff.: 0.02166375854245593 tk: 5 x_norm: 8.719353165212606\n",
      "Iteration 169 - Grad. Norm.: 0.004303214664783347 Norm. Diff.: 0.021589564685639134 tk: 5 x_norm: 8.737167468498956\n",
      "Iteration 170 - Grad. Norm.: 0.004288654575183136 Norm. Diff.: 0.021516073323916734 tk: 5 x_norm: 8.754919484399625\n",
      "Iteration 171 - Grad. Norm.: 0.004274230404278877 Norm. Diff.: 0.021443272875915652 tk: 5 x_norm: 8.772609836253098\n",
      "Iteration 172 - Grad. Norm.: 0.004259939938832053 Norm. Diff.: 0.021371152021394377 tk: 5 x_norm: 8.79023913763761\n",
      "Iteration 173 - Grad. Norm.: 0.004245781015041309 Norm. Diff.: 0.02129969969416024 tk: 5 x_norm: 8.807807992558946\n",
      "Iteration 174 - Grad. Norm.: 0.004231751517212209 Norm. Diff.: 0.02122890507520653 tk: 5 x_norm: 8.82531699563423\n",
      "Iteration 175 - Grad. Norm.: 0.004217849376467856 Norm. Diff.: 0.02115875758606108 tk: 5 x_norm: 8.842766732271683\n",
      "Iteration 176 - Grad. Norm.: 0.004204072569498997 Norm. Diff.: 0.021089246882339263 tk: 5 x_norm: 8.860157778846485\n",
      "Iteration 177 - Grad. Norm.: 0.004190419117352251 Norm. Diff.: 0.021020362847495012 tk: 5 x_norm: 8.877490702872857\n",
      "Iteration 178 - Grad. Norm.: 0.0041768870842551304 Norm. Diff.: 0.020952095586761275 tk: 5 x_norm: 8.894766063172419\n",
      "Iteration 179 - Grad. Norm.: 0.004163474576476612 Norm. Diff.: 0.02088443542127569 tk: 5 x_norm: 8.911984410038947\n",
      "Iteration 180 - Grad. Norm.: 0.00415017974122207 Norm. Diff.: 0.020817372882383053 tk: 5 x_norm: 8.929146285399577\n",
      "Iteration 181 - Grad. Norm.: 0.004137000765561341 Norm. Diff.: 0.020750898706110334 tk: 5 x_norm: 8.946252222972568\n",
      "Iteration 182 - Grad. Norm.: 0.0041239358753888955 Norm. Diff.: 0.02068500382780678 tk: 5 x_norm: 8.963302748421711\n",
      "Iteration 183 - Grad. Norm.: 0.004110983334414956 Norm. Diff.: 0.02061967937694445 tk: 5 x_norm: 8.98029837950742\n",
      "Iteration 184 - Grad. Norm.: 0.0040981414431865764 Norm. Diff.: 0.02055491667207484 tk: 5 x_norm: 8.997239626234652\n",
      "Iteration 185 - Grad. Norm.: 0.004085408538137675 Norm. Diff.: 0.020490707215932866 tk: 5 x_norm: 9.014126990997669\n",
      "Iteration 186 - Grad. Norm.: 0.004072782990667028 Norm. Diff.: 0.02042704269068838 tk: 5 x_norm: 9.030960968721734\n",
      "Iteration 187 - Grad. Norm.: 0.004060263206243351 Norm. Diff.: 0.02036391495333513 tk: 5 x_norm: 9.047742047001842\n",
      "Iteration 188 - Grad. Norm.: 0.004047847623536561 Norm. Diff.: 0.02030131603121674 tk: 5 x_norm: 9.064470706238508\n",
      "Iteration 189 - Grad. Norm.: 0.004035534713574314 Norm. Diff.: 0.020239238117682735 tk: 5 x_norm: 9.081147419770717\n",
      "Iteration 190 - Grad. Norm.: 0.0040233229789230765 Norm. Diff.: 0.02017767356787155 tk: 5 x_norm: 9.097772654006082\n",
      "Iteration 191 - Grad. Norm.: 0.0040112109528928725 Norm. Diff.: 0.020116614894615352 tk: 5 x_norm: 9.114346868548282\n",
      "Iteration 192 - Grad. Norm.: 0.0039991971987649345 Norm. Diff.: 0.020056054764464395 tk: 5 x_norm: 9.130870516321844\n",
      "Iteration 193 - Grad. Norm.: 0.003987280309041588 Norm. Diff.: 0.019995985993824707 tk: 5 x_norm: 9.147344043694327\n",
      "Iteration 194 - Grad. Norm.: 0.003975458904717568 Norm. Diff.: 0.019936401545207896 tk: 5 x_norm: 9.163767890595956\n",
      "Iteration 195 - Grad. Norm.: 0.003963731634572135 Norm. Diff.: 0.019877294523587875 tk: 5 x_norm: 9.180142490636797\n",
      "Iteration 196 - Grad. Norm.: 0.003952097174481336 Norm. Diff.: 0.019818658172860736 tk: 5 x_norm: 9.196468271221502\n",
      "Iteration 197 - Grad. Norm.: 0.003940554226749706 Norm. Diff.: 0.01976048587240669 tk: 5 x_norm: 9.212745653661653\n",
      "Iteration 198 - Grad. Norm.: 0.003929101519460888 Norm. Diff.: 0.019702771133748492 tk: 5 x_norm: 9.22897505328586\n",
      "Iteration 199 - Grad. Norm.: 0.003917737805846478 Norm. Diff.: 0.019645507597304466 tk: 5 x_norm: 9.245156879547515\n",
      "Iteration 200 - Grad. Norm.: 0.003906461863672607 Norm. Diff.: 0.019588689029232432 tk: 5 x_norm: 9.261291536130411\n",
      "Iteration 201 - Grad. Norm.: 0.003895272494643651 Norm. Diff.: 0.019532309318363066 tk: 5 x_norm: 9.277379421052132\n",
      "Iteration 202 - Grad. Norm.: 0.003884168523822536 Norm. Diff.: 0.01947636247321824 tk: 5 x_norm: 9.293420926765386\n",
      "Iteration 203 - Grad. Norm.: 0.003873148799067174 Norm. Diff.: 0.01942084261911262 tk: 5 x_norm: 9.309416440257232\n",
      "Iteration 204 - Grad. Norm.: 0.003862212190482467 Norm. Diff.: 0.01936574399533585 tk: 5 x_norm: 9.325366343146303\n",
      "Iteration 205 - Grad. Norm.: 0.003851357589887427 Norm. Diff.: 0.01931106095241228 tk: 5 x_norm: 9.341271011778074\n",
      "Iteration 206 - Grad. Norm.: 0.003840583910296955 Norm. Diff.: 0.01925678794943711 tk: 5 x_norm: 9.357130817318152\n",
      "Iteration 207 - Grad. Norm.: 0.0038298900854178133 Norm. Diff.: 0.01920291955148479 tk: 5 x_norm: 9.372946125843725\n",
      "Iteration 208 - Grad. Norm.: 0.0038192750691583787 Norm. Diff.: 0.019149450427089116 tk: 5 x_norm: 9.388717298433141\n",
      "Iteration 209 - Grad. Norm.: 0.003808737835151722 Norm. Diff.: 0.019096375345791845 tk: 5 x_norm: 9.404444691253678\n",
      "Iteration 210 - Grad. Norm.: 0.003798277376291648 Norm. Diff.: 0.019043689175758608 tk: 5 x_norm: 9.420128655647579\n",
      "Iteration 211 - Grad. Norm.: 0.0037878927042812854 Norm. Diff.: 0.018991386881458218 tk: 5 x_norm: 9.43576953821631\n",
      "Iteration 212 - Grad. Norm.: 0.003777582849193835 Norm. Diff.: 0.018939463521406453 tk: 5 x_norm: 9.451367680903184\n",
      "Iteration 213 - Grad. Norm.: 0.003767346859045158 Norm. Diff.: 0.01888791424596918 tk: 5 x_norm: 9.466923421074299\n",
      "Iteration 214 - Grad. Norm.: 0.003757183799377783 Norm. Diff.: 0.018836734295225793 tk: 5 x_norm: 9.482437091597857\n",
      "Iteration 215 - Grad. Norm.: 0.0037470927528560558 Norm. Diff.: 0.018785918996888862 tk: 5 x_norm: 9.497909020921941\n",
      "Iteration 216 - Grad. Norm.: 0.0037370728188720584 Norm. Diff.: 0.018735463764280178 tk: 5 x_norm: 9.513339533150699\n",
      "Iteration 217 - Grad. Norm.: 0.0037271231131619877 Norm. Diff.: 0.018685364094360293 tk: 5 x_norm: 9.528728948119044\n",
      "Iteration 218 - Grad. Norm.: 0.0037172427674327076 Norm. Diff.: 0.01863561556580992 tk: 5 x_norm: 9.544077581465874\n",
      "Iteration 219 - Grad. Norm.: 0.003707430928998128 Norm. Diff.: 0.018586213837163527 tk: 5 x_norm: 9.559385744705834\n",
      "Iteration 220 - Grad. Norm.: 0.00369768676042518 Norm. Diff.: 0.01853715464499069 tk: 5 x_norm: 9.574653745299665\n",
      "Iteration 221 - Grad. Norm.: 0.003688009439189058 Norm. Diff.: 0.018488433802125944 tk: 5 x_norm: 9.589881886723186\n",
      "Iteration 222 - Grad. Norm.: 0.0036783981573374775 Norm. Diff.: 0.018440047195945282 tk: 5 x_norm: 9.605070468534876\n",
      "Iteration 223 - Grad. Norm.: 0.00366885212116371 Norm. Diff.: 0.018391990786687407 tk: 5 x_norm: 9.620219786442187\n",
      "Iteration 224 - Grad. Norm.: 0.003659370550888077 Norm. Diff.: 0.018344260605818542 tk: 5 x_norm: 9.635330132366512\n",
      "Iteration 225 - Grad. Norm.: 0.0036499526803477284 Norm. Diff.: 0.018296852754440346 tk: 5 x_norm: 9.650401794506912\n",
      "Iteration 226 - Grad. Norm.: 0.0036405977566944386 Norm. Diff.: 0.01824976340173871 tk: 5 x_norm: 9.66543505740259\n",
      "Iteration 227 - Grad. Norm.: 0.0036313050401001637 Norm. Diff.: 0.0182029887834722 tk: 5 x_norm: 9.680430201994136\n",
      "Iteration 228 - Grad. Norm.: 0.0036220738034701853 Norm. Diff.: 0.01815652520050089 tk: 5 x_norm: 9.695387505683604\n",
      "Iteration 229 - Grad. Norm.: 0.00361290333216359 Norm. Diff.: 0.01811036901735097 tk: 5 x_norm: 9.71030724239342\n",
      "Iteration 230 - Grad. Norm.: 0.003603792923720884 Norm. Diff.: 0.01806451666081794 tk: 5 x_norm: 9.725189682624134\n",
      "Iteration 231 - Grad. Norm.: 0.003594741887598559 Norm. Diff.: 0.018018964618604447 tk: 5 x_norm: 9.740035093511064\n",
      "Iteration 232 - Grad. Norm.: 0.003585749544910379 Norm. Diff.: 0.01797370943799279 tk: 5 x_norm: 9.75484373887986\n",
      "Iteration 233 - Grad. Norm.: 0.003576815228175211 Norm. Diff.: 0.017928747724551875 tk: 5 x_norm: 9.769615879300982\n",
      "Iteration 234 - Grad. Norm.: 0.0035679382810712527 Norm. Diff.: 0.017884076140876046 tk: 5 x_norm: 9.78435177214314\n",
      "Iteration 235 - Grad. Norm.: 0.0035591180581964043 Norm. Diff.: 0.017839691405356295 tk: 5 x_norm: 9.7990516716257\n",
      "Iteration 236 - Grad. Norm.: 0.0035503539248346832 Norm. Diff.: 0.01779559029098202 tk: 5 x_norm: 9.8137158288701\n",
      "Iteration 237 - Grad. Norm.: 0.0035416452567284683 Norm. Diff.: 0.01775176962417343 tk: 5 x_norm: 9.828344491950283\n",
      "Iteration 238 - Grad. Norm.: 0.003532991439856431 Norm. Diff.: 0.017708226283642364 tk: 5 x_norm: 9.842937905942149\n",
      "Iteration 239 - Grad. Norm.: 0.0035243918702169935 Norm. Diff.: 0.017664957199282152 tk: 5 x_norm: 9.857496312972092\n",
      "Iteration 240 - Grad. Norm.: 0.0035158459536171406 Norm. Diff.: 0.017621959351084897 tk: 5 x_norm: 9.8720199522646\n",
      "Iteration 241 - Grad. Norm.: 0.0035073531054664897 Norm. Diff.: 0.01757922976808577 tk: 5 x_norm: 9.886509060188947\n",
      "Iteration 242 - Grad. Norm.: 0.0034989127505763986 Norm. Diff.: 0.017536765527332486 tk: 5 x_norm: 9.900963870305036\n",
      "Iteration 243 - Grad. Norm.: 0.0034905243229640453 Norm. Diff.: 0.017494563752882043 tk: 5 x_norm: 9.915384613408317\n",
      "Iteration 244 - Grad. Norm.: 0.003482187265661298 Norm. Diff.: 0.017452621614820214 tk: 5 x_norm: 9.929771517573922\n",
      "Iteration 245 - Grad. Norm.: 0.003473901030528257 Norm. Diff.: 0.017410936328306512 tk: 5 x_norm: 9.944124808199922\n",
      "Iteration 246 - Grad. Norm.: 0.0034656650780713503 Norm. Diff.: 0.01736950515264122 tk: 5 x_norm: 9.958444708049793\n",
      "Iteration 247 - Grad. Norm.: 0.003457478877265842 Norm. Diff.: 0.017328325390356728 tk: 5 x_norm: 9.972731437294097\n",
      "Iteration 248 - Grad. Norm.: 0.003449341905382658 Norm. Diff.: 0.017287394386329112 tk: 5 x_norm: 9.986985213551353\n",
      "Iteration 249 - Grad. Norm.: 0.003441841102312813 Norm. Diff.: 0.01626604561126572 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 250 - Grad. Norm.: 0.0034408773733205717 Norm. Diff.: 0.009719738044542441 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 251 - Grad. Norm.: 0.003440242862802498 Norm. Diff.: 0.009615630194562488 tk: 5 x_norm: 10.0\n",
      "Iteration 252 - Grad. Norm.: 0.003439900883057544 Norm. Diff.: 0.00951544083275731 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 253 - Grad. Norm.: 0.0034398186060003244 Norm. Diff.: 0.009418886745212319 tk: 5 x_norm: 10.0\n",
      "Iteration 254 - Grad. Norm.: 0.003439966684394011 Norm. Diff.: 0.009325707376818319 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 255 - Grad. Norm.: 0.0034403189059988 Norm. Diff.: 0.009235663191560481 tk: 5 x_norm: 10.0\n",
      "Iteration 256 - Grad. Norm.: 0.00344085187855021 Norm. Diff.: 0.009148534112683116 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 257 - Grad. Norm.: 0.0034415447434504604 Norm. Diff.: 0.00906411804599265 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 258 - Grad. Norm.: 0.0034423789160708423 Norm. Diff.: 0.008982229487882621 tk: 5 x_norm: 10.0\n",
      "Iteration 259 - Grad. Norm.: 0.003443337850612326 Norm. Diff.: 0.00890269821828513 tk: 5 x_norm: 10.0\n",
      "Iteration 260 - Grad. Norm.: 0.003444406827546019 Norm. Diff.: 0.008825368077618268 tk: 5 x_norm: 10.0\n",
      "Iteration 261 - Grad. Norm.: 0.0034455727617467326 Norm. Diff.: 0.008750095825897364 tk: 5 x_norm: 10.0\n",
      "Iteration 262 - Grad. Norm.: 0.0034468240295363653 Norm. Diff.: 0.008676750081478847 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 263 - Grad. Norm.: 0.0034481503129633285 Norm. Diff.: 0.008605210336372226 tk: 5 x_norm: 10.0\n",
      "Iteration 264 - Grad. Norm.: 0.003449542459757103 Norm. Diff.: 0.008535366044681796 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 265 - Grad. Norm.: 0.0034509923575096312 Norm. Diff.: 0.008467115780480506 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 266 - Grad. Norm.: 0.0034524928207457914 Norm. Diff.: 0.008400366461269541 tk: 5 x_norm: 10.0\n",
      "Iteration 267 - Grad. Norm.: 0.003454037489652251 Norm. Diff.: 0.00833503263311422 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 268 - Grad. Norm.: 0.003455620739336101 Norm. Diff.: 0.008271035813542493 tk: 5 x_norm: 10.0\n",
      "Iteration 269 - Grad. Norm.: 0.0034572375985815877 Norm. Diff.: 0.008208303888359986 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 270 - Grad. Norm.: 0.0034588836771641193 Norm. Diff.: 0.008146770558625348 tk: 5 x_norm: 10.0\n",
      "Iteration 271 - Grad. Norm.: 0.0034605551008656545 Norm. Diff.: 0.008086374834169692 tk: 5 x_norm: 10.0\n",
      "Iteration 272 - Grad. Norm.: 0.0034622484534144776 Norm. Diff.: 0.0080270605701887 tk: 5 x_norm: 10.0\n",
      "Iteration 273 - Grad. Norm.: 0.0034639607246450222 Norm. Diff.: 0.007968776043613802 tk: 5 x_norm: 10.0\n",
      "Iteration 274 - Grad. Norm.: 0.0034656892642406702 Norm. Diff.: 0.007911473566145055 tk: 5 x_norm: 10.0\n",
      "Iteration 275 - Grad. Norm.: 0.003467431740483703 Norm. Diff.: 0.007855109131008566 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 276 - Grad. Norm.: 0.0034691861034930336 Norm. Diff.: 0.007799642090695152 tk: 5 x_norm: 10.0\n",
      "Iteration 277 - Grad. Norm.: 0.0034709505524814052 Norm. Diff.: 0.0077450348631113745 tk: 5 x_norm: 10.0\n",
      "Iteration 278 - Grad. Norm.: 0.0034727235066105405 Norm. Diff.: 0.0076912526637599645 tk: 5 x_norm: 10.0\n",
      "Iteration 279 - Grad. Norm.: 0.003474503579065072 Norm. Diff.: 0.007638263261734248 tk: 5 x_norm: 10.0\n",
      "Iteration 280 - Grad. Norm.: 0.00347628955400431 Norm. Diff.: 0.0075860367574828716 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 281 - Grad. Norm.: 0.0034780803660859577 Norm. Diff.: 0.007534545380453414 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 282 - Grad. Norm.: 0.003479875082286956 Norm. Diff.: 0.007483763304875148 tk: 5 x_norm: 10.0\n",
      "Iteration 283 - Grad. Norm.: 0.003481672885775184 Norm. Diff.: 0.007433666482085618 tk: 5 x_norm: 10.0\n",
      "Iteration 284 - Grad. Norm.: 0.003483473061611185 Norm. Diff.: 0.00738423248792654 tk: 5 x_norm: 10.0\n",
      "Iteration 285 - Grad. Norm.: 0.003485274984082135 Norm. Diff.: 0.007335440383871018 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 286 - Grad. Norm.: 0.0034870781054909717 Norm. Diff.: 0.007287270590645803 tk: 5 x_norm: 10.0\n",
      "Iteration 287 - Grad. Norm.: 0.003488881946242073 Norm. Diff.: 0.0072397047732272275 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 288 - Grad. Norm.: 0.003490686086081808 Norm. Diff.: 0.007192725736179038 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 289 - Grad. Norm.: 0.0034924901563671445 Norm. Diff.: 0.007146317328398743 tk: 5 x_norm: 10.0\n",
      "Iteration 290 - Grad. Norm.: 0.003494293833248861 Norm. Diff.: 0.007100464356414969 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 291 - Grad. Norm.: 0.00349609683166832 Norm. Diff.: 0.007055152505458659 tk: 5 x_norm: 10.0\n",
      "Iteration 292 - Grad. Norm.: 0.003497898900077076 Norm. Diff.: 0.007010368267598269 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 293 - Grad. Norm.: 0.003499699815798805 Norm. Diff.: 0.006966098876294302 tk: 5 x_norm: 10.0\n",
      "Iteration 294 - Grad. Norm.: 0.003501499380961343 Norm. Diff.: 0.0069223322467888175 tk: 5 x_norm: 10.0\n",
      "Iteration 295 - Grad. Norm.: 0.0035032974189344837 Norm. Diff.: 0.006879056921793488 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 296 - Grad. Norm.: 0.0035050937712162787 Norm. Diff.: 0.006836262021993102 tk: 5 x_norm: 10.0\n",
      "Iteration 297 - Grad. Norm.: 0.003506888294716552 Norm. Diff.: 0.0067939372009271784 tk: 5 x_norm: 10.0\n",
      "Iteration 298 - Grad. Norm.: 0.003508680859392205 Norm. Diff.: 0.006752072603845379 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 299 - Grad. Norm.: 0.003510471346193432 Norm. Diff.: 0.006710658830177238 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 300 - Grad. Norm.: 0.0035122596452849377 Norm. Diff.: 0.006669686899285962 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 301 - Grad. Norm.: 0.0035140456545097324 Norm. Diff.: 0.00662914821920772 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 302 - Grad. Norm.: 0.00351582927806695 Norm. Diff.: 0.006589034558103234 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 303 - Grad. Norm.: 0.0035176104253781 Norm. Diff.: 0.006549338018178924 tk: 5 x_norm: 10.0\n",
      "Iteration 304 - Grad. Norm.: 0.003519389010119076 Norm. Diff.: 0.0065100510118494175 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 305 - Grad. Norm.: 0.0035211649493979134 Norm. Diff.: 0.006471166239944672 tk: 5 x_norm: 10.0\n",
      "Iteration 306 - Grad. Norm.: 0.003522938163060141 Norm. Diff.: 0.006432676671770818 tk: 5 x_norm: 10.0\n",
      "Iteration 307 - Grad. Norm.: 0.0035247085731060497 Norm. Diff.: 0.006394575526863088 tk: 5 x_norm: 10.0\n",
      "Iteration 308 - Grad. Norm.: 0.003526476103205588 Norm. Diff.: 0.006356856258276178 tk: 5 x_norm: 10.0\n",
      "Iteration 309 - Grad. Norm.: 0.003528240678298601 Norm. Diff.: 0.006319512537276613 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 310 - Grad. Norm.: 0.0035300022242690686 Norm. Diff.: 0.0062825382393090256 tk: 5 x_norm: 10.0\n",
      "Iteration 311 - Grad. Norm.: 0.0035317606676837877 Norm. Diff.: 0.006245927431125977 tk: 5 x_norm: 10.0\n",
      "Iteration 312 - Grad. Norm.: 0.0035335159355865836 Norm. Diff.: 0.006209674358976436 tk: 5 x_norm: 10.0\n",
      "Iteration 313 - Grad. Norm.: 0.003535267955340597 Norm. Diff.: 0.006173773437759491 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 314 - Grad. Norm.: 0.003537016654511691 Norm. Diff.: 0.006138219241059523 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 315 - Grad. Norm.: 0.0035387619607871496 Norm. Diff.: 0.006103006491982445 tk: 5 x_norm: 10.0\n",
      "Iteration 316 - Grad. Norm.: 0.0035405038019243047 Norm. Diff.: 0.006068130054726818 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 317 - Grad. Norm.: 0.003542242105724487 Norm. Diff.: 0.006033584926822915 tk: 5 x_norm: 10.0\n",
      "Iteration 318 - Grad. Norm.: 0.0035439768000282814 Norm. Diff.: 0.005999366231981521 tk: 5 x_norm: 10.0\n",
      "Iteration 319 - Grad. Norm.: 0.003545707812728521 Norm. Diff.: 0.00596546921350247 tk: 5 x_norm: 10.0\n",
      "Iteration 320 - Grad. Norm.: 0.0035474350717977442 Norm. Diff.: 0.005931889228191945 tk: 5 x_norm: 10.0\n",
      "Iteration 321 - Grad. Norm.: 0.0035491585053276267 Norm. Diff.: 0.005898621740745199 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 322 - Grad. Norm.: 0.00355087804157784 Norm. Diff.: 0.005865662318556632 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 323 - Grad. Norm.: 0.0035525936090323455 Norm. Diff.: 0.005833006626918116 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 324 - Grad. Norm.: 0.003554305136461246 Norm. Diff.: 0.0058006504245755345 tk: 5 x_norm: 10.0\n",
      "Iteration 325 - Grad. Norm.: 0.0035560125529868303 Norm. Diff.: 0.00576858955961021 tk: 5 x_norm: 10.0\n",
      "Iteration 326 - Grad. Norm.: 0.0035577157881522092 Norm. Diff.: 0.005736819965621851 tk: 5 x_norm: 10.0\n",
      "Iteration 327 - Grad. Norm.: 0.0035594147719916554 Norm. Diff.: 0.005705337658181526 tk: 5 x_norm: 10.0\n",
      "Iteration 328 - Grad. Norm.: 0.003561109435101521 Norm. Diff.: 0.005674138731539945 tk: 5 x_norm: 10.0\n",
      "Iteration 329 - Grad. Norm.: 0.003562799708710926 Norm. Diff.: 0.005643219355563027 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 330 - Grad. Norm.: 0.003564485524751625 Norm. Diff.: 0.005612575772880252 tk: 5 x_norm: 10.0\n",
      "Iteration 331 - Grad. Norm.: 0.0035661668159262315 Norm. Diff.: 0.005582204296225964 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 332 - Grad. Norm.: 0.0035678435157746654 Norm. Diff.: 0.00555210130595886 tk: 5 x_norm: 10.0\n",
      "Iteration 333 - Grad. Norm.: 0.0035695155587381045 Norm. Diff.: 0.005522263247745451 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 334 - Grad. Norm.: 0.0035711828802202643 Norm. Diff.: 0.0054926866303922775 tk: 5 x_norm: 10.0\n",
      "Iteration 335 - Grad. Norm.: 0.00357284541664584 Norm. Diff.: 0.005463368023817759 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 336 - Grad. Norm.: 0.003574503105515729 Norm. Diff.: 0.005434304057149531 tk: 5 x_norm: 10.0\n",
      "Iteration 337 - Grad. Norm.: 0.0035761558854590864 Norm. Diff.: 0.005405491416939728 tk: 5 x_norm: 10.0\n",
      "Iteration 338 - Grad. Norm.: 0.0035778036962819293 Norm. Diff.: 0.005376926845487307 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 339 - Grad. Norm.: 0.003579446479012436 Norm. Diff.: 0.005348607139258528 tk: 5 x_norm: 10.0\n",
      "Iteration 340 - Grad. Norm.: 0.0035810841759427335 Norm. Diff.: 0.005320529147400581 tk: 5 x_norm: 10.0\n",
      "Iteration 341 - Grad. Norm.: 0.0035827167306672876 Norm. Diff.: 0.005292689770336882 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 342 - Grad. Norm.: 0.0035843440881178864 Norm. Diff.: 0.005265085958442421 tk: 5 x_norm: 10.0\n",
      "Iteration 343 - Grad. Norm.: 0.003585966194595249 Norm. Diff.: 0.005237714710788119 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 344 - Grad. Norm.: 0.0035875829977973653 Norm. Diff.: 0.005210573073953496 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 345 - Grad. Norm.: 0.0035891944468445874 Norm. Diff.: 0.005183658140898801 tk: 5 x_norm: 10.0\n",
      "Iteration 346 - Grad. Norm.: 0.0035908004923016425 Norm. Diff.: 0.005156967049894153 tk: 5 x_norm: 10.0\n",
      "Iteration 347 - Grad. Norm.: 0.003592401086196505 Norm. Diff.: 0.005130496983501265 tk: 5 x_norm: 10.0\n",
      "Iteration 348 - Grad. Norm.: 0.0035939961820365012 Norm. Diff.: 0.005104245167602328 tk: 5 x_norm: 10.0\n",
      "Iteration 349 - Grad. Norm.: 0.0035955857348214195 Norm. Diff.: 0.005078208870475111 tk: 5 x_norm: 10.0\n",
      "Iteration 350 - Grad. Norm.: 0.0035971697010540693 Norm. Diff.: 0.005052385401907479 tk: 5 x_norm: 10.0\n",
      "Iteration 351 - Grad. Norm.: 0.003598748038748141 Norm. Diff.: 0.005026772112352348 tk: 5 x_norm: 10.0\n",
      "Iteration 352 - Grad. Norm.: 0.0036003207074336817 Norm. Diff.: 0.005001366392117193 tk: 5 x_norm: 10.0\n",
      "Iteration 353 - Grad. Norm.: 0.003601887668160134 Norm. Diff.: 0.004976165670586745 tk: 5 x_norm: 10.0\n",
      "Iteration 354 - Grad. Norm.: 0.003603448883497207 Norm. Diff.: 0.0049511674154765015 tk: 5 x_norm: 10.0\n",
      "Iteration 355 - Grad. Norm.: 0.0036050043175336268 Norm. Diff.: 0.004926369132115029 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 356 - Grad. Norm.: 0.003606553935873855 Norm. Diff.: 0.004901768362752778 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 357 - Grad. Norm.: 0.003608097705632983 Norm. Diff.: 0.004877362685895454 tk: 5 x_norm: 10.0\n",
      "Iteration 358 - Grad. Norm.: 0.0036096355954297723 Norm. Diff.: 0.004853149715660745 tk: 5 x_norm: 10.0\n",
      "Iteration 359 - Grad. Norm.: 0.003611167575378126 Norm. Diff.: 0.0048291271011570895 tk: 5 x_norm: 10.0\n",
      "Iteration 360 - Grad. Norm.: 0.0036126936170769474 Norm. Diff.: 0.0048052925258815285 tk: 5 x_norm: 10.0\n",
      "Iteration 361 - Grad. Norm.: 0.003614213693598574 Norm. Diff.: 0.004781643707137635 tk: 5 x_norm: 10.0\n",
      "Iteration 362 - Grad. Norm.: 0.003615727779475864 Norm. Diff.: 0.0047581783954699355 tk: 5 x_norm: 10.0\n",
      "Iteration 363 - Grad. Norm.: 0.003617235850688007 Norm. Diff.: 0.004734894374115868 tk: 5 x_norm: 10.0\n",
      "Iteration 364 - Grad. Norm.: 0.003618737884645231 Norm. Diff.: 0.004711789458472919 tk: 5 x_norm: 10.0\n",
      "Iteration 365 - Grad. Norm.: 0.003620233860172396 Norm. Diff.: 0.004688861495579948 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 366 - Grad. Norm.: 0.0036217237574915832 Norm. Diff.: 0.004666108363613137 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 367 - Grad. Norm.: 0.003623207558203899 Norm. Diff.: 0.004643527971393816 tk: 5 x_norm: 10.0\n",
      "Iteration 368 - Grad. Norm.: 0.003624685245270325 Norm. Diff.: 0.004621118257909121 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 369 - Grad. Norm.: 0.0036261568029919283 Norm. Diff.: 0.004598877191844663 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 370 - Grad. Norm.: 0.003627622216989349 Norm. Diff.: 0.004576802771126086 tk: 5 x_norm: 10.0\n",
      "Iteration 371 - Grad. Norm.: 0.0036290814741816945 Norm. Diff.: 0.004554893022473524 tk: 5 x_norm: 10.0\n",
      "Iteration 372 - Grad. Norm.: 0.003630534562764912 Norm. Diff.: 0.0045331460009644204 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 373 - Grad. Norm.: 0.0036319814721896913 Norm. Diff.: 0.004511559789604891 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 374 - Grad. Norm.: 0.0036334221931389557 Norm. Diff.: 0.004490132498911827 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 375 - Grad. Norm.: 0.003634856717504957 Norm. Diff.: 0.00446886226650188 tk: 5 x_norm: 10.0\n",
      "Iteration 376 - Grad. Norm.: 0.0036362850383661145 Norm. Diff.: 0.004447747256688787 tk: 5 x_norm: 10.0\n",
      "Iteration 377 - Grad. Norm.: 0.0036377071499635793 Norm. Diff.: 0.00442678566008903 tk: 5 x_norm: 10.0\n",
      "Iteration 378 - Grad. Norm.: 0.0036391230476775826 Norm. Diff.: 0.00440597569323366 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 379 - Grad. Norm.: 0.0036405327280036475 Norm. Diff.: 0.004385315598187924 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 380 - Grad. Norm.: 0.003641936188528666 Norm. Diff.: 0.004364803642177737 tk: 5 x_norm: 10.0\n",
      "Iteration 381 - Grad. Norm.: 0.003643333427906844 Norm. Diff.: 0.004344438117221648 tk: 5 x_norm: 10.0\n",
      "Iteration 382 - Grad. Norm.: 0.003644724445835709 Norm. Diff.: 0.00432421733977008 tk: 5 x_norm: 10.0\n",
      "Iteration 383 - Grad. Norm.: 0.003646109243032016 Norm. Diff.: 0.004304139650349876 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 384 - Grad. Norm.: 0.003647487821207697 Norm. Diff.: 0.0042842034132153815 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 385 - Grad. Norm.: 0.0036488601830458904 Norm. Diff.: 0.0042644070160035635 tk: 5 x_norm: 10.0\n",
      "Iteration 386 - Grad. Norm.: 0.003650226332177036 Norm. Diff.: 0.004244748869396508 tk: 5 x_norm: 10.0\n",
      "Iteration 387 - Grad. Norm.: 0.003651586273155013 Norm. Diff.: 0.004225227406787341 tk: 5 x_norm: 10.0\n",
      "Iteration 388 - Grad. Norm.: 0.003652940011433541 Norm. Diff.: 0.004205841083952977 tk: 5 x_norm: 10.0\n",
      "Iteration 389 - Grad. Norm.: 0.003654287553342616 Norm. Diff.: 0.004186588378729428 tk: 5 x_norm: 10.0\n",
      "Iteration 390 - Grad. Norm.: 0.0036556289060651484 Norm. Diff.: 0.004167467790694377 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 391 - Grad. Norm.: 0.0036569640776138333 Norm. Diff.: 0.004148477840853493 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 392 - Grad. Norm.: 0.0036582930768081947 Norm. Diff.: 0.004129617071330026 tk: 5 x_norm: 10.0\n",
      "Iteration 393 - Grad. Norm.: 0.003659615913251846 Norm. Diff.: 0.004110884045060659 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 394 - Grad. Norm.: 0.003660932597310037 Norm. Diff.: 0.00409227734549454 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 395 - Grad. Norm.: 0.003662243140087391 Norm. Diff.: 0.004073795576297714 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 396 - Grad. Norm.: 0.0036635475534059973 Norm. Diff.: 0.004055437361059401 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 397 - Grad. Norm.: 0.0036648458497836583 Norm. Diff.: 0.0040372013430049905 tk: 5 x_norm: 10.0\n",
      "Iteration 398 - Grad. Norm.: 0.0036661380424125797 Norm. Diff.: 0.0040190861847115384 tk: 5 x_norm: 10.0\n",
      "Iteration 399 - Grad. Norm.: 0.003667424145138186 Norm. Diff.: 0.004001090567826994 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 400 - Grad. Norm.: 0.0036687041724383835 Norm. Diff.: 0.00398321319279406 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 401 - Grad. Norm.: 0.0036699781394030663 Norm. Diff.: 0.003965452778577166 tk: 5 x_norm: 10.0\n",
      "Iteration 402 - Grad. Norm.: 0.0036712460617139386 Norm. Diff.: 0.003947808062393051 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 403 - Grad. Norm.: 0.0036725079556246777 Norm. Diff.: 0.003930277799446038 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 404 - Grad. Norm.: 0.0036737638379414623 Norm. Diff.: 0.003912860762664997 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 405 - Grad. Norm.: 0.0036750137260037194 Norm. Diff.: 0.0038955557424459773 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 406 - Grad. Norm.: 0.003676257637665311 Norm. Diff.: 0.0038783615463957575 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 407 - Grad. Norm.: 0.0036774955912760574 Norm. Diff.: 0.0038612769990820327 tk: 5 x_norm: 10.0\n",
      "Iteration 408 - Grad. Norm.: 0.0036787276056635164 Norm. Diff.: 0.0038443009417834114 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 409 - Grad. Norm.: 0.003679953700115179 Norm. Diff.: 0.0038274322322455917 tk: 5 x_norm: 10.0\n",
      "Iteration 410 - Grad. Norm.: 0.0036811738943609684 Norm. Diff.: 0.0038106697444386605 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 411 - Grad. Norm.: 0.003682388208556117 Norm. Diff.: 0.003794012368319631 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 412 - Grad. Norm.: 0.0036835966632643266 Norm. Diff.: 0.003777459009596392 tk: 5 x_norm: 10.0\n",
      "Iteration 413 - Grad. Norm.: 0.0036847992794413436 Norm. Diff.: 0.003761008589495387 tk: 5 x_norm: 10.0\n",
      "Iteration 414 - Grad. Norm.: 0.003685996078418794 Norm. Diff.: 0.003744660044532954 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 415 - Grad. Norm.: 0.0036871870818884485 Norm. Diff.: 0.0037284123262891536 tk: 5 x_norm: 10.0\n",
      "Iteration 416 - Grad. Norm.: 0.0036883723118867227 Norm. Diff.: 0.0037122644011842354 tk: 5 x_norm: 10.0\n",
      "Iteration 417 - Grad. Norm.: 0.0036895517907795973 Norm. Diff.: 0.0036962152502591893 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 418 - Grad. Norm.: 0.0036907255412478297 Norm. Diff.: 0.003680263868958349 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 419 - Grad. Norm.: 0.003691893586272521 Norm. Diff.: 0.003664409266914888 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 420 - Grad. Norm.: 0.0036930559491209626 Norm. Diff.: 0.003648650467739769 tk: 5 x_norm: 10.0\n",
      "Iteration 421 - Grad. Norm.: 0.003694212653332889 Norm. Diff.: 0.003632986508813289 tk: 5 x_norm: 10.0\n",
      "Iteration 422 - Grad. Norm.: 0.003695363722706943 Norm. Diff.: 0.0036174164410794686 tk: 5 x_norm: 10.0\n",
      "Iteration 423 - Grad. Norm.: 0.003696509181287586 Norm. Diff.: 0.003601939328842349 tk: 5 x_norm: 10.0\n",
      "Iteration 424 - Grad. Norm.: 0.0036976490533522166 Norm. Diff.: 0.00358655424956708 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 425 - Grad. Norm.: 0.00369878336339865 Norm. Diff.: 0.003571260293681377 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 426 - Grad. Norm.: 0.0036999121361328984 Norm. Diff.: 0.003556056564381588 tk: 5 x_norm: 10.0\n",
      "Iteration 427 - Grad. Norm.: 0.003701035396457252 Norm. Diff.: 0.0035409421774398047 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 428 - Grad. Norm.: 0.0037021531694586855 Norm. Diff.: 0.003525916261015048 tk: 5 x_norm: 10.0\n",
      "Iteration 429 - Grad. Norm.: 0.003703265480397487 Norm. Diff.: 0.003510977955466266 tk: 5 x_norm: 10.0\n",
      "Iteration 430 - Grad. Norm.: 0.0037043723546962746 Norm. Diff.: 0.003496126413167542 tk: 5 x_norm: 10.0\n",
      "Iteration 431 - Grad. Norm.: 0.0037054738179292117 Norm. Diff.: 0.0034813607983271255 tk: 5 x_norm: 10.0\n",
      "Iteration 432 - Grad. Norm.: 0.0037065698958115728 Norm. Diff.: 0.003466680286807964 tk: 5 x_norm: 10.0\n",
      "Iteration 433 - Grad. Norm.: 0.003707660614189572 Norm. Diff.: 0.003452084065950044 tk: 5 x_norm: 10.0\n",
      "Iteration 434 - Grad. Norm.: 0.003708745999030376 Norm. Diff.: 0.0034375713343976783 tk: 5 x_norm: 10.0\n",
      "Iteration 435 - Grad. Norm.: 0.003709826076412575 Norm. Diff.: 0.0034231413019262733 tk: 5 x_norm: 10.0\n",
      "Iteration 436 - Grad. Norm.: 0.003710900872516713 Norm. Diff.: 0.0034087931892739596 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 437 - Grad. Norm.: 0.0037119704136162257 Norm. Diff.: 0.0033945262279736197 tk: 5 x_norm: 10.0\n",
      "Iteration 438 - Grad. Norm.: 0.0037130347260685317 Norm. Diff.: 0.0033803396601888324 tk: 5 x_norm: 10.0\n",
      "Iteration 439 - Grad. Norm.: 0.0037140938363064684 Norm. Diff.: 0.0033662327385511636 tk: 5 x_norm: 10.0\n",
      "Iteration 440 - Grad. Norm.: 0.00371514777082991 Norm. Diff.: 0.0033522047260001555 tk: 5 x_norm: 10.0\n",
      "Iteration 441 - Grad. Norm.: 0.0037161965561976337 Norm. Diff.: 0.0033382548956255313 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 442 - Grad. Norm.: 0.003717240219019421 Norm. Diff.: 0.0033243825305117403 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 443 - Grad. Norm.: 0.0037182787859484806 Norm. Diff.: 0.003310586923584161 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 444 - Grad. Norm.: 0.0037193122836738993 Norm. Diff.: 0.003296867377458769 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 445 - Grad. Norm.: 0.003720340738913579 Norm. Diff.: 0.0032832232042925413 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 446 - Grad. Norm.: 0.003721364178407117 Norm. Diff.: 0.003269653725636872 tk: 5 x_norm: 10.0\n",
      "Iteration 447 - Grad. Norm.: 0.003722382628909145 Norm. Diff.: 0.0032561582722927247 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 448 - Grad. Norm.: 0.0037233961171827105 Norm. Diff.: 0.003242736184167918 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 449 - Grad. Norm.: 0.003724404669992973 Norm. Diff.: 0.003229386810137545 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 450 - Grad. Norm.: 0.0037254083141009735 Norm. Diff.: 0.0032161095079040643 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 451 - Grad. Norm.: 0.003726407076257785 Norm. Diff.: 0.0032029036438619043 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 452 - Grad. Norm.: 0.003727400983198666 Norm. Diff.: 0.003189768592962578 tk: 5 x_norm: 10.0\n",
      "Iteration 453 - Grad. Norm.: 0.0037283900616375374 Norm. Diff.: 0.0031767037385823345 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 454 - Grad. Norm.: 0.0037293743382615617 Norm. Diff.: 0.003163708472391479 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 455 - Grad. Norm.: 0.0037303538397259845 Norm. Diff.: 0.003150782194226235 tk: 5 x_norm: 10.0\n",
      "Iteration 456 - Grad. Norm.: 0.0037313285926490257 Norm. Diff.: 0.003137924311961324 tk: 5 x_norm: 10.0\n",
      "Iteration 457 - Grad. Norm.: 0.0037322986236071067 Norm. Diff.: 0.0031251342413857247 tk: 5 x_norm: 10.0\n",
      "Iteration 458 - Grad. Norm.: 0.0037332639591300958 Norm. Diff.: 0.0031124114060794997 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 459 - Grad. Norm.: 0.0037342246256968337 Norm. Diff.: 0.003099755237292898 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 460 - Grad. Norm.: 0.0037351806497307135 Norm. Diff.: 0.0030871651738267392 tk: 5 x_norm: 10.0\n",
      "Iteration 461 - Grad. Norm.: 0.0037361320575955348 Norm. Diff.: 0.003074640661914939 tk: 5 x_norm: 10.0\n",
      "Iteration 462 - Grad. Norm.: 0.003737078875591389 Norm. Diff.: 0.003062181155108893 tk: 5 x_norm: 10.0\n",
      "Iteration 463 - Grad. Norm.: 0.0037380211299507782 Norm. Diff.: 0.003049786114163934 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 464 - Grad. Norm.: 0.0037389588468348892 Norm. Diff.: 0.0030374550069256646 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 465 - Grad. Norm.: 0.003739892052329927 Norm. Diff.: 0.0030251873082210376 tk: 5 x_norm: 10.0\n",
      "Iteration 466 - Grad. Norm.: 0.0037408207724436574 Norm. Diff.: 0.003012982499748504 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 467 - Grad. Norm.: 0.0037417450331020717 Norm. Diff.: 0.003000840069970475 tk: 5 x_norm: 10.0\n",
      "Iteration 468 - Grad. Norm.: 0.0037426648601461576 Norm. Diff.: 0.0029887595140089132 tk: 5 x_norm: 10.0\n",
      "Iteration 469 - Grad. Norm.: 0.003743580279328819 Norm. Diff.: 0.002976740333540094 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 470 - Grad. Norm.: 0.003744491316311959 Norm. Diff.: 0.0029647820366929807 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 471 - Grad. Norm.: 0.003745397996663574 Norm. Diff.: 0.002952884137947522 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 472 - Grad. Norm.: 0.0037463003458551103 Norm. Diff.: 0.002941046158036902 tk: 5 x_norm: 10.0\n",
      "Iteration 473 - Grad. Norm.: 0.0037471983892588255 Norm. Diff.: 0.0029292676238478975 tk: 5 x_norm: 10.0\n",
      "Iteration 474 - Grad. Norm.: 0.0037480921521452913 Norm. Diff.: 0.002917548068326502 tk: 5 x_norm: 10.0\n",
      "Iteration 475 - Grad. Norm.: 0.003748981659681075 Norm. Diff.: 0.002905887030381287 tk: 5 x_norm: 10.0\n",
      "Iteration 476 - Grad. Norm.: 0.003749866936926388 Norm. Diff.: 0.0028942840547916565 tk: 5 x_norm: 10.0\n",
      "Iteration 477 - Grad. Norm.: 0.003750748008833006 Norm. Diff.: 0.0028827386921152494 tk: 5 x_norm: 10.0\n",
      "Iteration 478 - Grad. Norm.: 0.00375162490024213 Norm. Diff.: 0.0028712504985974664 tk: 5 x_norm: 10.0\n",
      "Iteration 479 - Grad. Norm.: 0.0037524976358824912 Norm. Diff.: 0.002859819036082845 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 480 - Grad. Norm.: 0.0037533662403684345 Norm. Diff.: 0.0028484438719273294 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 481 - Grad. Norm.: 0.003754230738198151 Norm. Diff.: 0.0028371245789117782 tk: 5 x_norm: 10.0\n",
      "Iteration 482 - Grad. Norm.: 0.0037550911537520097 Norm. Diff.: 0.0028258607351576348 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 483 - Grad. Norm.: 0.003755947511290963 Norm. Diff.: 0.002814651924042078 tk: 5 x_norm: 10.0\n",
      "Iteration 484 - Grad. Norm.: 0.003756799834955007 Norm. Diff.: 0.0028034977341180422 tk: 5 x_norm: 10.0\n",
      "Iteration 485 - Grad. Norm.: 0.0037576481487618194 Norm. Diff.: 0.002792397759030386 tk: 5 x_norm: 10.0\n",
      "Iteration 486 - Grad. Norm.: 0.003758492476605316 Norm. Diff.: 0.002781351597438998 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 487 - Grad. Norm.: 0.0037593328422544994 Norm. Diff.: 0.0027703588529385 tk: 5 x_norm: 10.0\n",
      "Iteration 488 - Grad. Norm.: 0.003760169269352168 Norm. Diff.: 0.0027594191339816547 tk: 5 x_norm: 10.0\n",
      "Iteration 489 - Grad. Norm.: 0.0037610017814138468 Norm. Diff.: 0.002748532053803251 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 490 - Grad. Norm.: 0.00376183040182679 Norm. Diff.: 0.002737697230346194 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 491 - Grad. Norm.: 0.0037626551538488973 Norm. Diff.: 0.002726914286187036 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 492 - Grad. Norm.: 0.003763476060607918 Norm. Diff.: 0.002716182848463327 tk: 5 x_norm: 10.0\n",
      "Iteration 493 - Grad. Norm.: 0.003764293145100545 Norm. Diff.: 0.0027055025488033242 tk: 5 x_norm: 10.0\n",
      "Iteration 494 - Grad. Norm.: 0.003765106430191663 Norm. Diff.: 0.0026948730232559284 tk: 5 x_norm: 10.0\n",
      "Iteration 495 - Grad. Norm.: 0.003765915938613653 Norm. Diff.: 0.0026842939122201674 tk: 5 x_norm: 10.0\n",
      "Iteration 496 - Grad. Norm.: 0.003766721692965684 Norm. Diff.: 0.002673764860379455 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 497 - Grad. Norm.: 0.0037675237157131754 Norm. Diff.: 0.0026632855166329617 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 498 - Grad. Norm.: 0.0037683220291872613 Norm. Diff.: 0.0026528555340311535 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 499 - Grad. Norm.: 0.003769116655584247 Norm. Diff.: 0.0026424745697107587 tk: 5 x_norm: 10.0\n",
      "Iteration 500 - Grad. Norm.: 0.0037699076169652426 Norm. Diff.: 0.0026321422848307025 tk: 5 x_norm: 10.0\n",
      "Iteration 501 - Grad. Norm.: 0.003770694935255752 Norm. Diff.: 0.0026218583445103854 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 502 - Grad. Norm.: 0.003771478632245345 Norm. Diff.: 0.0026116224177672696 tk: 5 x_norm: 10.0\n",
      "Iteration 503 - Grad. Norm.: 0.003772258729587367 Norm. Diff.: 0.0026014341774566923 tk: 5 x_norm: 10.0\n",
      "Iteration 504 - Grad. Norm.: 0.0037730352487987833 Norm. Diff.: 0.002591293300212161 tk: 5 x_norm: 10.0\n",
      "Iteration 505 - Grad. Norm.: 0.0037738082112598577 Norm. Diff.: 0.002581199466387373 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 506 - Grad. Norm.: 0.0037745776382141303 Norm. Diff.: 0.002571152359997138 tk: 5 x_norm: 10.0\n",
      "Iteration 507 - Grad. Norm.: 0.0037753435507682062 Norm. Diff.: 0.0025611516686620556 tk: 5 x_norm: 10.0\n",
      "Iteration 508 - Grad. Norm.: 0.003776105969891793 Norm. Diff.: 0.0025511970835515167 tk: 5 x_norm: 10.0\n",
      "Iteration 509 - Grad. Norm.: 0.00377686491641762 Norm. Diff.: 0.002541288299329799 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 510 - Grad. Norm.: 0.0037776204110414693 Norm. Diff.: 0.0025314250141015857 tk: 5 x_norm: 10.0\n",
      "Iteration 511 - Grad. Norm.: 0.0037783724743222138 Norm. Diff.: 0.002521606929358353 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 512 - Grad. Norm.: 0.003779121126681927 Norm. Diff.: 0.0025118337499272126 tk: 5 x_norm: 10.0\n",
      "Iteration 513 - Grad. Norm.: 0.0037798663884059712 Norm. Diff.: 0.002502105183918119 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 514 - Grad. Norm.: 0.003780608279643203 Norm. Diff.: 0.002492420942674635 tk: 5 x_norm: 10.0\n",
      "Iteration 515 - Grad. Norm.: 0.0037813468204061125 Norm. Diff.: 0.0024827807407231304 tk: 5 x_norm: 10.0\n",
      "Iteration 516 - Grad. Norm.: 0.0037820820305710353 Norm. Diff.: 0.0024731842957245523 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 517 - Grad. Norm.: 0.003782813929878474 Norm. Diff.: 0.0024636313284261135 tk: 5 x_norm: 10.0\n",
      "Iteration 518 - Grad. Norm.: 0.003783542537933255 Norm. Diff.: 0.0024541215626132464 tk: 5 x_norm: 10.0\n",
      "Iteration 519 - Grad. Norm.: 0.0037842678742049394 Norm. Diff.: 0.0024446547250642865 tk: 5 x_norm: 10.0\n",
      "Iteration 520 - Grad. Norm.: 0.0037849899580281047 Norm. Diff.: 0.0024352305455034784 tk: 5 x_norm: 10.0\n",
      "Iteration 521 - Grad. Norm.: 0.00378570880860268 Norm. Diff.: 0.0024258487565572092 tk: 5 x_norm: 10.0\n",
      "Iteration 522 - Grad. Norm.: 0.003786424444994346 Norm. Diff.: 0.0024165090937080433 tk: 5 x_norm: 10.0\n",
      "Iteration 523 - Grad. Norm.: 0.003787136886134954 Norm. Diff.: 0.0024072112952526923 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 524 - Grad. Norm.: 0.003787846150822919 Norm. Diff.: 0.0023979551022582594 tk: 5 x_norm: 10.0\n",
      "Iteration 525 - Grad. Norm.: 0.003788552257723676 Norm. Diff.: 0.002388740258520316 tk: 5 x_norm: 10.0\n",
      "Iteration 526 - Grad. Norm.: 0.0037892552253701623 Norm. Diff.: 0.002379566510521748 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 527 - Grad. Norm.: 0.0037899550721632726 Norm. Diff.: 0.002370433607391541 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 528 - Grad. Norm.: 0.003790651816372397 Norm. Diff.: 0.002361341300865052 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 529 - Grad. Norm.: 0.0037913454761359035 Norm. Diff.: 0.0023522893452441046 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 530 - Grad. Norm.: 0.003792036069461744 Norm. Diff.: 0.0023432774973587743 tk: 5 x_norm: 10.0\n",
      "Iteration 531 - Grad. Norm.: 0.0037927236142279307 Norm. Diff.: 0.0023343055165286355 tk: 5 x_norm: 10.0\n",
      "Iteration 532 - Grad. Norm.: 0.0037934081281831857 Norm. Diff.: 0.00232537316452596 tk: 5 x_norm: 10.0\n",
      "Iteration 533 - Grad. Norm.: 0.003794089628947459 Norm. Diff.: 0.0023164802055385854 tk: 5 x_norm: 10.0\n",
      "Iteration 534 - Grad. Norm.: 0.0037947681340126235 Norm. Diff.: 0.0023076264061330167 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 535 - Grad. Norm.: 0.0037954436607429572 Norm. Diff.: 0.0022988115352198103 tk: 5 x_norm: 10.0\n",
      "Iteration 536 - Grad. Norm.: 0.0037961162263759115 Norm. Diff.: 0.002290035364017561 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 537 - Grad. Norm.: 0.0037967858480227054 Norm. Diff.: 0.002281297666019245 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 538 - Grad. Norm.: 0.0037974525426688935 Norm. Diff.: 0.0022725982169574605 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 539 - Grad. Norm.: 0.003798116327175194 Norm. Diff.: 0.0022639367947719785 tk: 5 x_norm: 10.0\n",
      "Iteration 540 - Grad. Norm.: 0.0037987772182780178 Norm. Diff.: 0.0022553131795759835 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 541 - Grad. Norm.: 0.0037994352325902317 Norm. Diff.: 0.002246727153624943 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 542 - Grad. Norm.: 0.0038000903866018594 Norm. Diff.: 0.002238178501283421 tk: 5 x_norm: 10.0\n",
      "Iteration 543 - Grad. Norm.: 0.0038007426966807393 Norm. Diff.: 0.0022296670089955676 tk: 5 x_norm: 10.0\n",
      "Iteration 544 - Grad. Norm.: 0.003801392179073262 Norm. Diff.: 0.002221192465252888 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 545 - Grad. Norm.: 0.003802038849905116 Norm. Diff.: 0.002212754660565384 tk: 5 x_norm: 10.0\n",
      "Iteration 546 - Grad. Norm.: 0.0038026827251819713 Norm. Diff.: 0.002204353387431348 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 547 - Grad. Norm.: 0.0038033238207902578 Norm. Diff.: 0.002195988440307643 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 548 - Grad. Norm.: 0.0038039621524978587 Norm. Diff.: 0.0021876596155824104 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 549 - Grad. Norm.: 0.0038045977359549227 Norm. Diff.: 0.0021793667115452553 tk: 5 x_norm: 10.0\n",
      "Iteration 550 - Grad. Norm.: 0.0038052305866945533 Norm. Diff.: 0.0021711095283611166 tk: 5 x_norm: 10.0\n",
      "Iteration 551 - Grad. Norm.: 0.0038058607201336275 Norm. Diff.: 0.0021628878680417132 tk: 5 x_norm: 10.0\n",
      "Iteration 552 - Grad. Norm.: 0.003806488151573504 Norm. Diff.: 0.002154701534419594 tk: 5 x_norm: 10.0\n",
      "Iteration 553 - Grad. Norm.: 0.0038071128962008267 Norm. Diff.: 0.0021465503331212572 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 554 - Grad. Norm.: 0.003807734969088306 Norm. Diff.: 0.0021384340715416977 tk: 5 x_norm: 10.0\n",
      "Iteration 555 - Grad. Norm.: 0.0038083543851954527 Norm. Diff.: 0.0021303525588184024 tk: 5 x_norm: 10.0\n",
      "Iteration 556 - Grad. Norm.: 0.0038089711593694013 Norm. Diff.: 0.002122305605806717 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 557 - Grad. Norm.: 0.003809585306345659 Norm. Diff.: 0.002114293025055684 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 558 - Grad. Norm.: 0.0038101968407489354 Norm. Diff.: 0.0021063146307820297 tk: 5 x_norm: 10.0\n",
      "Iteration 559 - Grad. Norm.: 0.003810805777093865 Norm. Diff.: 0.002098370238848671 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 560 - Grad. Norm.: 0.00381141212978587 Norm. Diff.: 0.0020904596667398103 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 561 - Grad. Norm.: 0.0038120159131218635 Norm. Diff.: 0.0020825827335379187 tk: 5 x_norm: 10.0\n",
      "Iteration 562 - Grad. Norm.: 0.0038126171412911558 Norm. Diff.: 0.002074739259902281 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 563 - Grad. Norm.: 0.0038132158283761303 Norm. Diff.: 0.0020669290680452755 tk: 5 x_norm: 10.0\n",
      "Iteration 564 - Grad. Norm.: 0.003813811988353129 Norm. Diff.: 0.0020591519817116095 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 565 - Grad. Norm.: 0.003814405635093198 Norm. Diff.: 0.002051407826155968 tk: 5 x_norm: 10.0\n",
      "Iteration 566 - Grad. Norm.: 0.0038149967823628912 Norm. Diff.: 0.0020436964281225407 tk: 5 x_norm: 10.0\n",
      "Iteration 567 - Grad. Norm.: 0.003815585443825086 Norm. Diff.: 0.0020360176158239784 tk: 5 x_norm: 10.0\n",
      "Iteration 568 - Grad. Norm.: 0.003816171633039741 Norm. Diff.: 0.0020283712189203876 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 569 - Grad. Norm.: 0.0038167553634647295 Norm. Diff.: 0.0020207570685005245 tk: 5 x_norm: 10.0\n",
      "Iteration 570 - Grad. Norm.: 0.003817336648456605 Norm. Diff.: 0.0020131749970605127 tk: 5 x_norm: 10.0\n",
      "Iteration 571 - Grad. Norm.: 0.003817915501271442 Norm. Diff.: 0.0020056248384855524 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 572 - Grad. Norm.: 0.003818491935065531 Norm. Diff.: 0.0019981064280301494 tk: 5 x_norm: 10.0\n",
      "Iteration 573 - Grad. Norm.: 0.0038190659628962874 Norm. Diff.: 0.0019906196022994155 tk: 5 x_norm: 10.0\n",
      "Iteration 574 - Grad. Norm.: 0.0038196375977229617 Norm. Diff.: 0.0019831641992313796 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 575 - Grad. Norm.: 0.0038202068524074498 Norm. Diff.: 0.001975740058077455 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 576 - Grad. Norm.: 0.0038207737397150765 Norm. Diff.: 0.00196834701938522 tk: 5 x_norm: 10.0\n",
      "Iteration 577 - Grad. Norm.: 0.003821338272315403 Norm. Diff.: 0.00196098492498161 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 578 - Grad. Norm.: 0.00382190046278297 Norm. Diff.: 0.0019536536179541163 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 579 - Grad. Norm.: 0.0038224603235980942 Norm. Diff.: 0.0019463529426351292 tk: 5 x_norm: 10.0\n",
      "Iteration 580 - Grad. Norm.: 0.0038230178671476624 Norm. Diff.: 0.0019390827445847384 tk: 5 x_norm: 10.0\n",
      "Iteration 581 - Grad. Norm.: 0.003823573105725883 Norm. Diff.: 0.0019318428705736256 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 582 - Grad. Norm.: 0.0038241260515350663 Norm. Diff.: 0.0019246331685686948 tk: 5 x_norm: 10.0\n",
      "Iteration 583 - Grad. Norm.: 0.0038246767166864076 Norm. Diff.: 0.001917453487715154 tk: 5 x_norm: 10.0\n",
      "Iteration 584 - Grad. Norm.: 0.0038252251132007073 Norm. Diff.: 0.0019103036783222882 tk: 5 x_norm: 10.0\n",
      "Iteration 585 - Grad. Norm.: 0.0038257712530092047 Norm. Diff.: 0.001903183591847355 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 586 - Grad. Norm.: 0.0038263151479542836 Norm. Diff.: 0.001896093080881202 tk: 5 x_norm: 10.0\n",
      "Iteration 587 - Grad. Norm.: 0.0038268568097902128 Norm. Diff.: 0.0018890319991328541 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 588 - Grad. Norm.: 0.00382739625018396 Norm. Diff.: 0.001882000201414841 tk: 5 x_norm: 10.0\n",
      "Iteration 589 - Grad. Norm.: 0.0038279334807159314 Norm. Diff.: 0.0018749975436294737 tk: 5 x_norm: 10.0\n",
      "Iteration 590 - Grad. Norm.: 0.0038284685128806687 Norm. Diff.: 0.0018680238827538584 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 591 - Grad. Norm.: 0.0038290013580876228 Norm. Diff.: 0.001861079076827289 tk: 5 x_norm: 10.0\n",
      "Iteration 592 - Grad. Norm.: 0.0038295320276618785 Norm. Diff.: 0.0018541629849358641 tk: 5 x_norm: 10.0\n",
      "Iteration 593 - Grad. Norm.: 0.0038300605328449163 Norm. Diff.: 0.0018472754672009357 tk: 5 x_norm: 10.0\n",
      "Iteration 594 - Grad. Norm.: 0.003830586884795314 Norm. Diff.: 0.0018404163847641369 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 595 - Grad. Norm.: 0.0038311110945895093 Norm. Diff.: 0.0018335855997760428 tk: 5 x_norm: 10.0\n",
      "Iteration 596 - Grad. Norm.: 0.0038316331732224677 Norm. Diff.: 0.0018267829753822093 tk: 5 x_norm: 10.0\n",
      "Iteration 597 - Grad. Norm.: 0.003832153131608418 Norm. Diff.: 0.001820008375711453 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 598 - Grad. Norm.: 0.003832670980581623 Norm. Diff.: 0.0018132616658623184 tk: 5 x_norm: 10.0\n",
      "Iteration 599 - Grad. Norm.: 0.0038331867308969993 Norm. Diff.: 0.0018065427118927847 tk: 5 x_norm: 10.0\n",
      "Iteration 600 - Grad. Norm.: 0.0038337003932308747 Norm. Diff.: 0.0017998513808062686 tk: 5 x_norm: 10.0\n",
      "Iteration 601 - Grad. Norm.: 0.003834211978181695 Norm. Diff.: 0.0017931875405418493 tk: 5 x_norm: 10.0\n",
      "Iteration 602 - Grad. Norm.: 0.003834721496270696 Norm. Diff.: 0.0017865510599612885 tk: 5 x_norm: 10.0\n",
      "Iteration 603 - Grad. Norm.: 0.003835228957942592 Norm. Diff.: 0.001779941808838191 tk: 5 x_norm: 10.0\n",
      "Iteration 604 - Grad. Norm.: 0.0038357343735662777 Norm. Diff.: 0.0017733596578470344 tk: 5 x_norm: 10.0\n",
      "Iteration 605 - Grad. Norm.: 0.003836237753435507 Norm. Diff.: 0.0017668044785516036 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 606 - Grad. Norm.: 0.0038367391077695745 Norm. Diff.: 0.0017602761433952935 tk: 5 x_norm: 10.0\n",
      "Iteration 607 - Grad. Norm.: 0.003837238446713971 Norm. Diff.: 0.0017537745256890904 tk: 5 x_norm: 10.0\n",
      "Iteration 608 - Grad. Norm.: 0.003837735780341071 Norm. Diff.: 0.0017472994996020662 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 609 - Grad. Norm.: 0.003838231118650779 Norm. Diff.: 0.0017408509401509523 tk: 5 x_norm: 10.0\n",
      "Iteration 610 - Grad. Norm.: 0.0038387244715711965 Norm. Diff.: 0.0017344287231897946 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 611 - Grad. Norm.: 0.0038392158489593013 Norm. Diff.: 0.0017280327254005628 tk: 5 x_norm: 10.0\n",
      "Iteration 612 - Grad. Norm.: 0.003839705260601528 Norm. Diff.: 0.0017216628242825605 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 613 - Grad. Norm.: 0.0038401927162144698 Norm. Diff.: 0.0017153188981433316 tk: 5 x_norm: 10.0\n",
      "Iteration 614 - Grad. Norm.: 0.003840678225445517 Norm. Diff.: 0.0017090008260890626 tk: 5 x_norm: 10.0\n",
      "Iteration 615 - Grad. Norm.: 0.00384116179787346 Norm. Diff.: 0.0017027084880157385 tk: 5 x_norm: 10.0\n",
      "Iteration 616 - Grad. Norm.: 0.0038416434430091325 Norm. Diff.: 0.0016964417645991229 tk: 5 x_norm: 10.0\n",
      "Iteration 617 - Grad. Norm.: 0.0038421231702960453 Norm. Diff.: 0.0016902005372866494 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 618 - Grad. Norm.: 0.0038426009891109956 Norm. Diff.: 0.0016839846882876197 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 619 - Grad. Norm.: 0.003843076908764655 Norm. Diff.: 0.001677794100565619 tk: 5 x_norm: 10.0\n",
      "Iteration 620 - Grad. Norm.: 0.0038435509385022558 Norm. Diff.: 0.0016716286578290684 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 621 - Grad. Norm.: 0.0038440230875041196 Norm. Diff.: 0.0016654882445230493 tk: 5 x_norm: 10.0\n",
      "Iteration 622 - Grad. Norm.: 0.003844493364886268 Norm. Diff.: 0.0016593727458215357 tk: 5 x_norm: 10.0\n",
      "Iteration 623 - Grad. Norm.: 0.003844961779701075 Norm. Diff.: 0.00165328204761815 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 624 - Grad. Norm.: 0.0038454283409377465 Norm. Diff.: 0.0016472160365189365 tk: 5 x_norm: 10.0\n",
      "Iteration 625 - Grad. Norm.: 0.0038458930575230297 Norm. Diff.: 0.0016411745998346598 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 626 - Grad. Norm.: 0.003846355938321723 Norm. Diff.: 0.0016351576255723864 tk: 5 x_norm: 10.0\n",
      "Iteration 627 - Grad. Norm.: 0.00384681699213726 Norm. Diff.: 0.001629165002428355 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 628 - Grad. Norm.: 0.0038472762277122494 Norm. Diff.: 0.001623196619779839 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 629 - Grad. Norm.: 0.003847733653729122 Norm. Diff.: 0.001617252367678736 tk: 5 x_norm: 10.0\n",
      "Iteration 630 - Grad. Norm.: 0.0038481892788105906 Norm. Diff.: 0.0016113321368433223 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 631 - Grad. Norm.: 0.0038486431115203074 Norm. Diff.: 0.001605435818651891 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 632 - Grad. Norm.: 0.00384909516036332 Norm. Diff.: 0.0015995633051348458 tk: 5 x_norm: 10.0\n",
      "Iteration 633 - Grad. Norm.: 0.0038495454337866657 Norm. Diff.: 0.001593714488968601 tk: 5 x_norm: 10.0\n",
      "Iteration 634 - Grad. Norm.: 0.0038499939401799087 Norm. Diff.: 0.0015878892634684138 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 635 - Grad. Norm.: 0.0038504406878756645 Norm. Diff.: 0.0015820875225815683 tk: 5 x_norm: 10.0\n",
      "Iteration 636 - Grad. Norm.: 0.0038508856851501502 Norm. Diff.: 0.0015763091608809292 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 637 - Grad. Norm.: 0.003851328940223672 Norm. Diff.: 0.001570554073558537 tk: 5 x_norm: 10.0\n",
      "Iteration 638 - Grad. Norm.: 0.003851770461261181 Norm. Diff.: 0.0015648221564186892 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 639 - Grad. Norm.: 0.0038522102563727776 Norm. Diff.: 0.0015591133058725384 tk: 5 x_norm: 10.0\n",
      "Iteration 640 - Grad. Norm.: 0.003852648333614216 Norm. Diff.: 0.001553427418931094 tk: 5 x_norm: 10.0\n",
      "Iteration 641 - Grad. Norm.: 0.003853084700987443 Norm. Diff.: 0.0015477643931996509 tk: 5 x_norm: 10.0\n",
      "Iteration 642 - Grad. Norm.: 0.0038535193664410296 Norm. Diff.: 0.001542124126871222 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 643 - Grad. Norm.: 0.003853952337870745 Norm. Diff.: 0.0015365065187214898 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 644 - Grad. Norm.: 0.0038543836231200103 Norm. Diff.: 0.0015309114681022742 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 645 - Grad. Norm.: 0.003854813229980366 Norm. Diff.: 0.0015253388749359622 tk: 5 x_norm: 10.0\n",
      "Iteration 646 - Grad. Norm.: 0.0038552411661920147 Norm. Diff.: 0.0015197886397099196 tk: 5 x_norm: 10.0\n",
      "Iteration 647 - Grad. Norm.: 0.0038556674394442554 Norm. Diff.: 0.0015142606634714706 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 648 - Grad. Norm.: 0.0038560920573759592 Norm. Diff.: 0.0015087548478213118 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 649 - Grad. Norm.: 0.003856515027576028 Norm. Diff.: 0.0015032710949090943 tk: 5 x_norm: 10.0\n",
      "Iteration 650 - Grad. Norm.: 0.003856936357583931 Norm. Diff.: 0.0014978093074279617 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 651 - Grad. Norm.: 0.003857356054890043 Norm. Diff.: 0.0014923693886093803 tk: 5 x_norm: 10.0\n",
      "Iteration 652 - Grad. Norm.: 0.003857774126936223 Norm. Diff.: 0.0014869512422171433 tk: 5 x_norm: 10.0\n",
      "Iteration 653 - Grad. Norm.: 0.0038581905811161635 Norm. Diff.: 0.0014815547725438452 tk: 5 x_norm: 10.0\n",
      "Iteration 654 - Grad. Norm.: 0.0038586054247759234 Norm. Diff.: 0.0014761798844046597 tk: 5 x_norm: 10.0\n",
      "Iteration 655 - Grad. Norm.: 0.003859018665214299 Norm. Diff.: 0.0014708264831331873 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 656 - Grad. Norm.: 0.0038594303096833146 Norm. Diff.: 0.0014654944745760364 tk: 5 x_norm: 10.0\n",
      "Iteration 657 - Grad. Norm.: 0.0038598403653886094 Norm. Diff.: 0.001460183765088913 tk: 5 x_norm: 10.0\n",
      "Iteration 658 - Grad. Norm.: 0.003860248839489907 Norm. Diff.: 0.001454894261530908 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 659 - Grad. Norm.: 0.0038606557391014183 Norm. Diff.: 0.0014496258712607519 tk: 5 x_norm: 10.0\n",
      "Iteration 660 - Grad. Norm.: 0.003861061071292268 Norm. Diff.: 0.0014443785021320248 tk: 5 x_norm: 10.0\n",
      "Iteration 661 - Grad. Norm.: 0.0038614648430869126 Norm. Diff.: 0.0014391520624885904 tk: 5 x_norm: 10.0\n",
      "Iteration 662 - Grad. Norm.: 0.0038618670614655316 Norm. Diff.: 0.001433946461160094 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 663 - Grad. Norm.: 0.0038622677333644837 Norm. Diff.: 0.001428761607458161 tk: 5 x_norm: 10.0\n",
      "Iteration 664 - Grad. Norm.: 0.00386266686567664 Norm. Diff.: 0.0014235974111716137 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 665 - Grad. Norm.: 0.003863064465251877 Norm. Diff.: 0.0014184537825623468 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 666 - Grad. Norm.: 0.0038634605388973564 Norm. Diff.: 0.0014133306323615208 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 667 - Grad. Norm.: 0.003863855093378016 Norm. Diff.: 0.0014082278717651529 tk: 5 x_norm: 10.0\n",
      "Iteration 668 - Grad. Norm.: 0.003864248135416909 Norm. Diff.: 0.0014031454124303288 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 669 - Grad. Norm.: 0.0038646396716956045 Norm. Diff.: 0.0013980831664708812 tk: 5 x_norm: 10.0\n",
      "Iteration 670 - Grad. Norm.: 0.003865029708854539 Norm. Diff.: 0.0013930410464543209 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 671 - Grad. Norm.: 0.0038654182534934257 Norm. Diff.: 0.0013880189653970043 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 672 - Grad. Norm.: 0.00386580531217162 Norm. Diff.: 0.0013830168367610016 tk: 5 x_norm: 10.0\n",
      "Iteration 673 - Grad. Norm.: 0.003866190891408463 Norm. Diff.: 0.0013780345744499756 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 674 - Grad. Norm.: 0.0038665749976836655 Norm. Diff.: 0.0013730720928057008 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 675 - Grad. Norm.: 0.003866957637437659 Norm. Diff.: 0.001368129306604948 tk: 5 x_norm: 10.0\n",
      "Iteration 676 - Grad. Norm.: 0.0038673388170719788 Norm. Diff.: 0.0013632061310552284 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 677 - Grad. Norm.: 0.0038677185429495486 Norm. Diff.: 0.0013583024817910815 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 678 - Grad. Norm.: 0.0038680968213951296 Norm. Diff.: 0.0013534182748715837 tk: 5 x_norm: 10.0\n",
      "Iteration 679 - Grad. Norm.: 0.0038684736586955654 Norm. Diff.: 0.0013485534267758995 tk: 5 x_norm: 10.0\n",
      "Iteration 680 - Grad. Norm.: 0.00386884906110018 Norm. Diff.: 0.0013437078544011947 tk: 5 x_norm: 10.0\n",
      "Iteration 681 - Grad. Norm.: 0.0038692230348211223 Norm. Diff.: 0.0013388814750574668 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 682 - Grad. Norm.: 0.003869595586033653 Norm. Diff.: 0.0013340742064661887 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 683 - Grad. Norm.: 0.0038699667208765155 Norm. Diff.: 0.0013292859667560066 tk: 5 x_norm: 10.0\n",
      "Iteration 684 - Grad. Norm.: 0.0038703364454522487 Norm. Diff.: 0.0013245166744595648 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 685 - Grad. Norm.: 0.003870704765827517 Norm. Diff.: 0.0013197662485111207 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 686 - Grad. Norm.: 0.0038710716880334052 Norm. Diff.: 0.0013150346082426232 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 687 - Grad. Norm.: 0.003871437218065781 Norm. Diff.: 0.001310321673381039 tk: 5 x_norm: 10.0\n",
      "Iteration 688 - Grad. Norm.: 0.003871801361885579 Norm. Diff.: 0.001305627364045665 tk: 5 x_norm: 10.0\n",
      "Iteration 689 - Grad. Norm.: 0.003872164125419089 Norm. Diff.: 0.001300951600744179 tk: 5 x_norm: 10.0\n",
      "Iteration 690 - Grad. Norm.: 0.003872525514558308 Norm. Diff.: 0.0012962943043712447 tk: 5 x_norm: 10.0\n",
      "Iteration 691 - Grad. Norm.: 0.00387288553516122 Norm. Diff.: 0.0012916553962037693 tk: 5 x_norm: 10.0\n",
      "Iteration 692 - Grad. Norm.: 0.0038732441930520996 Norm. Diff.: 0.0012870347979001234 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 693 - Grad. Norm.: 0.0038736014940217818 Norm. Diff.: 0.0012824324314957719 tk: 5 x_norm: 10.0\n",
      "Iteration 694 - Grad. Norm.: 0.003873957443828014 Norm. Diff.: 0.0012778482194005859 tk: 5 x_norm: 10.0\n",
      "Iteration 695 - Grad. Norm.: 0.0038743120481956944 Norm. Diff.: 0.001273282084397668 tk: 5 x_norm: 10.0\n",
      "Iteration 696 - Grad. Norm.: 0.003874665312817189 Norm. Diff.: 0.0012687339496386101 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 697 - Grad. Norm.: 0.003875017243352595 Norm. Diff.: 0.0012642037386421797 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 698 - Grad. Norm.: 0.003875367845430025 Norm. Diff.: 0.001259691375291289 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 699 - Grad. Norm.: 0.0038757171246458968 Norm. Diff.: 0.0012551967838307606 tk: 5 x_norm: 10.0\n",
      "Iteration 700 - Grad. Norm.: 0.0038760650865652087 Norm. Diff.: 0.001250719888863966 tk: 5 x_norm: 10.0\n",
      "Iteration 701 - Grad. Norm.: 0.003876411736721788 Norm. Diff.: 0.0012462606153511029 tk: 5 x_norm: 10.0\n",
      "Iteration 702 - Grad. Norm.: 0.0038767570806185958 Norm. Diff.: 0.0012418188886065583 tk: 5 x_norm: 10.0\n",
      "Iteration 703 - Grad. Norm.: 0.003877101123727949 Norm. Diff.: 0.0012373946342963491 tk: 5 x_norm: 10.0\n",
      "Iteration 704 - Grad. Norm.: 0.0038774438714918235 Norm. Diff.: 0.0012329877784351274 tk: 5 x_norm: 10.0\n",
      "Iteration 705 - Grad. Norm.: 0.003877785329322102 Norm. Diff.: 0.0012285982473852653 tk: 5 x_norm: 10.0\n",
      "Iteration 706 - Grad. Norm.: 0.0038781255026008018 Norm. Diff.: 0.0012242259678531527 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 707 - Grad. Norm.: 0.0038784643966803846 Norm. Diff.: 0.0012198708668875 tk: 5 x_norm: 10.0\n",
      "Iteration 708 - Grad. Norm.: 0.0038788020168839636 Norm. Diff.: 0.0012155328718765415 tk: 5 x_norm: 10.0\n",
      "Iteration 709 - Grad. Norm.: 0.003879138368505554 Norm. Diff.: 0.001211211910546584 tk: 5 x_norm: 10.0\n",
      "Iteration 710 - Grad. Norm.: 0.0038794734568103548 Norm. Diff.: 0.0012069079109591871 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 711 - Grad. Norm.: 0.0038798072870349573 Norm. Diff.: 0.001202620801508987 tk: 5 x_norm: 10.0\n",
      "Iteration 712 - Grad. Norm.: 0.0038801398643875935 Norm. Diff.: 0.0011983505109215706 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 713 - Grad. Norm.: 0.003880471194048394 Norm. Diff.: 0.001194096968251736 tk: 5 x_norm: 10.0\n",
      "Iteration 714 - Grad. Norm.: 0.0038808012811695717 Norm. Diff.: 0.0011898601028806684 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 715 - Grad. Norm.: 0.0038811301308757356 Norm. Diff.: 0.0011856398445142262 tk: 5 x_norm: 10.0\n",
      "Iteration 716 - Grad. Norm.: 0.0038814577482640528 Norm. Diff.: 0.001181436123180979 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 717 - Grad. Norm.: 0.003881784138404494 Norm. Diff.: 0.0011772488692299264 tk: 5 x_norm: 10.0\n",
      "Iteration 718 - Grad. Norm.: 0.003882109306340067 Norm. Diff.: 0.0011730780133285808 tk: 5 x_norm: 10.0\n",
      "Iteration 719 - Grad. Norm.: 0.003882433257087051 Norm. Diff.: 0.0011689234864608654 tk: 5 x_norm: 10.0\n",
      "Iteration 720 - Grad. Norm.: 0.0038827559956351896 Norm. Diff.: 0.0011647852199252022 tk: 5 x_norm: 10.0\n",
      "Iteration 721 - Grad. Norm.: 0.0038830775269479124 Norm. Diff.: 0.001160663145333232 tk: 5 x_norm: 10.0\n",
      "Iteration 722 - Grad. Norm.: 0.0038833978559625858 Norm. Diff.: 0.0011565571946059055 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 723 - Grad. Norm.: 0.0038837169875906687 Norm. Diff.: 0.0011524672999743966 tk: 5 x_norm: 10.0\n",
      "Iteration 724 - Grad. Norm.: 0.0038840349267179734 Norm. Diff.: 0.0011483933939757677 tk: 5 x_norm: 10.0\n",
      "Iteration 725 - Grad. Norm.: 0.0038843516782048598 Norm. Diff.: 0.0011443354094525567 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 726 - Grad. Norm.: 0.003884667246886401 Norm. Diff.: 0.0011402932795501947 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 727 - Grad. Norm.: 0.003884981637572663 Norm. Diff.: 0.0011362669377158107 tk: 5 x_norm: 10.0\n",
      "Iteration 728 - Grad. Norm.: 0.003885294855048858 Norm. Diff.: 0.0011322563176955536 tk: 5 x_norm: 10.0\n",
      "Iteration 729 - Grad. Norm.: 0.003885606904075533 Norm. Diff.: 0.0011282613535341897 tk: 5 x_norm: 10.0\n",
      "Iteration 730 - Grad. Norm.: 0.003885917789388801 Norm. Diff.: 0.0011242819795717022 tk: 5 x_norm: 10.0\n",
      "Iteration 731 - Grad. Norm.: 0.003886227515700497 Norm. Diff.: 0.001120318130442743 tk: 5 x_norm: 10.0\n",
      "Iteration 732 - Grad. Norm.: 0.0038865360876984357 Norm. Diff.: 0.0011163697410746334 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 733 - Grad. Norm.: 0.0038868435100465272 Norm. Diff.: 0.0011124367466854752 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 734 - Grad. Norm.: 0.003887149787385007 Norm. Diff.: 0.0011085190827825923 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 735 - Grad. Norm.: 0.003887454924330613 Norm. Diff.: 0.0011046166851607735 tk: 5 x_norm: 10.0\n",
      "Iteration 736 - Grad. Norm.: 0.0038877589254767797 Norm. Diff.: 0.0011007294899008735 tk: 5 x_norm: 10.0\n",
      "Iteration 737 - Grad. Norm.: 0.0038880617953938024 Norm. Diff.: 0.0010968574333677945 tk: 5 x_norm: 10.0\n",
      "Iteration 738 - Grad. Norm.: 0.00388836353862903 Norm. Diff.: 0.0010930004522096257 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 739 - Grad. Norm.: 0.0038886641597070458 Norm. Diff.: 0.0010891584833550567 tk: 5 x_norm: 10.0\n",
      "Iteration 740 - Grad. Norm.: 0.003888963663129826 Norm. Diff.: 0.0010853314640125382 tk: 5 x_norm: 10.0\n",
      "Iteration 741 - Grad. Norm.: 0.0038892620533769484 Norm. Diff.: 0.001081519331668544 tk: 5 x_norm: 10.0\n",
      "Iteration 742 - Grad. Norm.: 0.0038895593349057285 Norm. Diff.: 0.0010777220240857069 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 743 - Grad. Norm.: 0.0038898555121514014 Norm. Diff.: 0.001073939479302069 tk: 5 x_norm: 10.0\n",
      "Iteration 744 - Grad. Norm.: 0.003890150589527319 Norm. Diff.: 0.0010701716356284398 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 745 - Grad. Norm.: 0.0038904445714250517 Norm. Diff.: 0.0010664184316480062 tk: 5 x_norm: 10.0\n",
      "Iteration 746 - Grad. Norm.: 0.003890737462214633 Norm. Diff.: 0.0010626798062144652 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 747 - Grad. Norm.: 0.003891029266244668 Norm. Diff.: 0.0010589556984501773 tk: 5 x_norm: 10.0\n",
      "Iteration 748 - Grad. Norm.: 0.0038913199878425155 Norm. Diff.: 0.0010552460477450715 tk: 5 x_norm: 10.0\n",
      "Iteration 749 - Grad. Norm.: 0.003891609631314434 Norm. Diff.: 0.0010515507937555945 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 750 - Grad. Norm.: 0.003891898200945775 Norm. Diff.: 0.0010478698764024561 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 751 - Grad. Norm.: 0.0038921857010010674 Norm. Diff.: 0.0010442032358696362 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 752 - Grad. Norm.: 0.0038924721357242635 Norm. Diff.: 0.0010405508126032261 tk: 5 x_norm: 10.0\n",
      "Iteration 753 - Grad. Norm.: 0.0038927575093388265 Norm. Diff.: 0.0010369125473099722 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 754 - Grad. Norm.: 0.0038930418260479066 Norm. Diff.: 0.0010332883809556466 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 755 - Grad. Norm.: 0.003893325090034474 Norm. Diff.: 0.0010296782547634911 tk: 5 x_norm: 10.0\n",
      "Iteration 756 - Grad. Norm.: 0.0038936073054615123 Norm. Diff.: 0.0010260821102137006 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 757 - Grad. Norm.: 0.0038938884764720817 Norm. Diff.: 0.0010224998890415466 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 758 - Grad. Norm.: 0.003894168607189549 Norm. Diff.: 0.0010189315332358754 tk: 5 x_norm: 10.0\n",
      "Iteration 759 - Grad. Norm.: 0.0038944477017176953 Norm. Diff.: 0.0010153769850382024 tk: 5 x_norm: 10.0\n",
      "Iteration 760 - Grad. Norm.: 0.0038947257641408456 Norm. Diff.: 0.0010118361869414095 tk: 5 x_norm: 10.0\n",
      "Iteration 761 - Grad. Norm.: 0.0038950027985240163 Norm. Diff.: 0.0010083090816885862 tk: 5 x_norm: 10.0\n",
      "Iteration 762 - Grad. Norm.: 0.003895278808913078 Norm. Diff.: 0.0010047956122709635 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 763 - Grad. Norm.: 0.0038955537993348548 Norm. Diff.: 0.0010012957219275532 tk: 5 x_norm: 10.0\n",
      "Iteration 764 - Grad. Norm.: 0.0038958277737972838 Norm. Diff.: 0.000997809354143688 tk: 5 x_norm: 10.0\n",
      "Iteration 765 - Grad. Norm.: 0.0038961007362895523 Norm. Diff.: 0.0009943364526495187 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 766 - Grad. Norm.: 0.0038963726907822187 Norm. Diff.: 0.0009908769614189647 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 767 - Grad. Norm.: 0.003896643641227336 Norm. Diff.: 0.000987430824668986 tk: 5 x_norm: 10.0\n",
      "Iteration 768 - Grad. Norm.: 0.0038969135915586004 Norm. Diff.: 0.0009839979868569888 tk: 5 x_norm: 10.0\n",
      "Iteration 769 - Grad. Norm.: 0.00389718254569148 Norm. Diff.: 0.0009805783926813626 tk: 5 x_norm: 10.0\n",
      "Iteration 770 - Grad. Norm.: 0.0038974505075233226 Norm. Diff.: 0.0009771719870793648 tk: 5 x_norm: 10.0\n",
      "Iteration 771 - Grad. Norm.: 0.0038977174809334903 Norm. Diff.: 0.0009737787152258562 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 772 - Grad. Norm.: 0.003897983469783515 Norm. Diff.: 0.0009703985225324012 tk: 5 x_norm: 10.0\n",
      "Iteration 773 - Grad. Norm.: 0.0038982484779171585 Norm. Diff.: 0.0009670313546464251 tk: 5 x_norm: 10.0\n",
      "Iteration 774 - Grad. Norm.: 0.0038985125091605777 Norm. Diff.: 0.0009636771574496433 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 775 - Grad. Norm.: 0.0038987755673224574 Norm. Diff.: 0.0009603358770566481 tk: 5 x_norm: 10.0\n",
      "Iteration 776 - Grad. Norm.: 0.003899037656194099 Norm. Diff.: 0.0009570074598145158 tk: 5 x_norm: 10.0\n",
      "Iteration 777 - Grad. Norm.: 0.0038992987795495207 Norm. Diff.: 0.0009536918523015838 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 778 - Grad. Norm.: 0.0038995589411456423 Norm. Diff.: 0.0009503890013255552 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 779 - Grad. Norm.: 0.0038998181447223355 Norm. Diff.: 0.0009470988539234985 tk: 5 x_norm: 10.0\n",
      "Iteration 780 - Grad. Norm.: 0.0039000763940025703 Norm. Diff.: 0.0009438213573600635 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 781 - Grad. Norm.: 0.003900333692692529 Norm. Diff.: 0.0009405564591264799 tk: 5 x_norm: 10.0\n",
      "Iteration 782 - Grad. Norm.: 0.0039005900444816955 Norm. Diff.: 0.0009373041069398475 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 783 - Grad. Norm.: 0.0039008454530429872 Norm. Diff.: 0.0009340642487415995 tk: 5 x_norm: 10.0\n",
      "Iteration 784 - Grad. Norm.: 0.003901099922032853 Norm. Diff.: 0.000930836832696842 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 785 - Grad. Norm.: 0.0039013534550914066 Norm. Diff.: 0.0009276218071930328 tk: 5 x_norm: 10.0\n",
      "Iteration 786 - Grad. Norm.: 0.003901606055842484 Norm. Diff.: 0.000924419120839088 tk: 5 x_norm: 10.0\n",
      "Iteration 787 - Grad. Norm.: 0.0039018577278937976 Norm. Diff.: 0.0009212287224644855 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 788 - Grad. Norm.: 0.003902108474837015 Norm. Diff.: 0.000918050561117855 tk: 5 x_norm: 10.0\n",
      "Iteration 789 - Grad. Norm.: 0.00390235830024789 Norm. Diff.: 0.0009148845860664815 tk: 5 x_norm: 10.0\n",
      "Iteration 790 - Grad. Norm.: 0.0039026072076863293 Norm. Diff.: 0.0009117307467949254 tk: 5 x_norm: 10.0\n",
      "Iteration 791 - Grad. Norm.: 0.0039028552006965126 Norm. Diff.: 0.0009085889930038363 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 792 - Grad. Norm.: 0.003903102282806984 Norm. Diff.: 0.0009054592746097435 tk: 5 x_norm: 10.0\n",
      "Iteration 793 - Grad. Norm.: 0.003903348457530797 Norm. Diff.: 0.0009023415417431308 tk: 5 x_norm: 10.0\n",
      "Iteration 794 - Grad. Norm.: 0.0039035937283655384 Norm. Diff.: 0.0008992357447482585 tk: 5 x_norm: 10.0\n",
      "Iteration 795 - Grad. Norm.: 0.00390383809879348 Norm. Diff.: 0.000896141834181982 tk: 5 x_norm: 10.0\n",
      "Iteration 796 - Grad. Norm.: 0.0039040815722816606 Norm. Diff.: 0.0008930597608119725 tk: 5 x_norm: 10.0\n",
      "Iteration 797 - Grad. Norm.: 0.003904324152281987 Norm. Diff.: 0.0008899894756171871 tk: 5 x_norm: 10.0\n",
      "Iteration 798 - Grad. Norm.: 0.003904565842231309 Norm. Diff.: 0.0008869309297856074 tk: 5 x_norm: 10.0\n",
      "Iteration 799 - Grad. Norm.: 0.003904806645551532 Norm. Diff.: 0.0008838840747143465 tk: 5 x_norm: 10.0\n",
      "Iteration 800 - Grad. Norm.: 0.0039050465656497004 Norm. Diff.: 0.0008808488620079645 tk: 5 x_norm: 10.0\n",
      "Iteration 801 - Grad. Norm.: 0.0039052856059181283 Norm. Diff.: 0.0008778252434779232 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 802 - Grad. Norm.: 0.003905523769734406 Norm. Diff.: 0.0008748131711414305 tk: 5 x_norm: 10.0\n",
      "Iteration 803 - Grad. Norm.: 0.0039057610604615687 Norm. Diff.: 0.0008718125972207798 tk: 5 x_norm: 10.0\n",
      "Iteration 804 - Grad. Norm.: 0.00390599748144816 Norm. Diff.: 0.0008688234741420564 tk: 5 x_norm: 10.0\n",
      "Iteration 805 - Grad. Norm.: 0.0039062330360283186 Norm. Diff.: 0.0008658457545349073 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 806 - Grad. Norm.: 0.003906467727521838 Norm. Diff.: 0.0008628793912308648 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 807 - Grad. Norm.: 0.003906701559234333 Norm. Diff.: 0.0008599243372630243 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 808 - Grad. Norm.: 0.0039069345344572275 Norm. Diff.: 0.0008569805458646727 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 809 - Grad. Norm.: 0.00390716665646788 Norm. Diff.: 0.0008540479704687728 tk: 5 x_norm: 10.0\n",
      "Iteration 810 - Grad. Norm.: 0.003907397928529721 Norm. Diff.: 0.0008511265647072291 tk: 5 x_norm: 10.0\n",
      "Iteration 811 - Grad. Norm.: 0.003907628353892246 Norm. Diff.: 0.0008482162824093187 tk: 5 x_norm: 10.0\n",
      "Iteration 812 - Grad. Norm.: 0.003907857935791162 Norm. Diff.: 0.000845317077601694 tk: 5 x_norm: 10.0\n",
      "Iteration 813 - Grad. Norm.: 0.003908086677448429 Norm. Diff.: 0.0008424289045068656 tk: 5 x_norm: 10.0\n",
      "Iteration 814 - Grad. Norm.: 0.003908314582072382 Norm. Diff.: 0.0008395517175425283 tk: 5 x_norm: 10.0\n",
      "Iteration 815 - Grad. Norm.: 0.003908541652857752 Norm. Diff.: 0.0008366854713211013 tk: 5 x_norm: 10.0\n",
      "Iteration 816 - Grad. Norm.: 0.003908767892985831 Norm. Diff.: 0.0008338301206481618 tk: 5 x_norm: 10.0\n",
      "Iteration 817 - Grad. Norm.: 0.003908993305624441 Norm. Diff.: 0.0008309856205220385 tk: 5 x_norm: 10.0\n",
      "Iteration 818 - Grad. Norm.: 0.003909217893928133 Norm. Diff.: 0.0008281519261332392 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 819 - Grad. Norm.: 0.003909441661038157 Norm. Diff.: 0.0008253289928630125 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 820 - Grad. Norm.: 0.00390966461008261 Norm. Diff.: 0.0008225167762829566 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 821 - Grad. Norm.: 0.003909886744176471 Norm. Diff.: 0.0008197152321539023 tk: 5 x_norm: 10.0\n",
      "Iteration 822 - Grad. Norm.: 0.003910108066421704 Norm. Diff.: 0.0008169243164252116 tk: 5 x_norm: 10.0\n",
      "Iteration 823 - Grad. Norm.: 0.003910328579907295 Norm. Diff.: 0.000814143985234662 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 824 - Grad. Norm.: 0.0039105482877093735 Norm. Diff.: 0.0008113741949060686 tk: 5 x_norm: 10.0\n",
      "Iteration 825 - Grad. Norm.: 0.0039107671928912515 Norm. Diff.: 0.000808614901949905 tk: 5 x_norm: 10.0\n",
      "Iteration 826 - Grad. Norm.: 0.003910985298503492 Norm. Diff.: 0.0008058660630618191 tk: 5 x_norm: 10.0\n",
      "Iteration 827 - Grad. Norm.: 0.003911202607584023 Norm. Diff.: 0.000803127635122349 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 828 - Grad. Norm.: 0.003911419123158149 Norm. Diff.: 0.0008003995751953328 tk: 5 x_norm: 10.0\n",
      "Iteration 829 - Grad. Norm.: 0.0039116348482386735 Norm. Diff.: 0.0007976818405279172 tk: 5 x_norm: 10.0\n",
      "Iteration 830 - Grad. Norm.: 0.003911849785825926 Norm. Diff.: 0.0007949743885491214 tk: 5 x_norm: 10.0\n",
      "Iteration 831 - Grad. Norm.: 0.003912063938907859 Norm. Diff.: 0.0007922771768701819 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 832 - Grad. Norm.: 0.003912277310460116 Norm. Diff.: 0.0007895901632819337 tk: 5 x_norm: 10.0\n",
      "Iteration 833 - Grad. Norm.: 0.00391248990344606 Norm. Diff.: 0.0007869133057556134 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 834 - Grad. Norm.: 0.003912701720816915 Norm. Diff.: 0.0007842465624417213 tk: 5 x_norm: 10.0\n",
      "Iteration 835 - Grad. Norm.: 0.003912912765511758 Norm. Diff.: 0.0007815898916690185 tk: 5 x_norm: 10.0\n",
      "Iteration 836 - Grad. Norm.: 0.003913123040457616 Norm. Diff.: 0.000778943251943658 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 837 - Grad. Norm.: 0.003913332548569531 Norm. Diff.: 0.0007763066019488808 tk: 5 x_norm: 10.0\n",
      "Iteration 838 - Grad. Norm.: 0.003913541292750652 Norm. Diff.: 0.0007736799005439119 tk: 5 x_norm: 10.0\n",
      "Iteration 839 - Grad. Norm.: 0.003913749275892221 Norm. Diff.: 0.000771063106763532 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 840 - Grad. Norm.: 0.003913956500873727 Norm. Diff.: 0.0007684561798167812 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 841 - Grad. Norm.: 0.003914162970562908 Norm. Diff.: 0.000765859079086923 tk: 5 x_norm: 10.0\n",
      "Iteration 842 - Grad. Norm.: 0.003914368687815831 Norm. Diff.: 0.0007632717641301187 tk: 5 x_norm: 10.0\n",
      "Iteration 843 - Grad. Norm.: 0.003914573655476965 Norm. Diff.: 0.0007606941946754004 tk: 5 x_norm: 10.0\n",
      "Iteration 844 - Grad. Norm.: 0.003914777876379238 Norm. Diff.: 0.0007581263306227025 tk: 5 x_norm: 10.0\n",
      "Iteration 845 - Grad. Norm.: 0.003914981353344072 Norm. Diff.: 0.0007555681320434753 tk: 5 x_norm: 10.0\n",
      "Iteration 846 - Grad. Norm.: 0.003915184089181481 Norm. Diff.: 0.000753019559179439 tk: 5 x_norm: 10.0\n",
      "Iteration 847 - Grad. Norm.: 0.003915386086690108 Norm. Diff.: 0.0007504805724419134 tk: 5 x_norm: 10.0\n",
      "Iteration 848 - Grad. Norm.: 0.00391558734865728 Norm. Diff.: 0.0007479511324106139 tk: 5 x_norm: 10.0\n",
      "Iteration 849 - Grad. Norm.: 0.003915787877859085 Norm. Diff.: 0.0007454311998334816 tk: 5 x_norm: 10.0\n",
      "Iteration 850 - Grad. Norm.: 0.003915987677060427 Norm. Diff.: 0.0007429207356264785 tk: 5 x_norm: 10.0\n",
      "Iteration 851 - Grad. Norm.: 0.003916186749015055 Norm. Diff.: 0.0007404197008712637 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 852 - Grad. Norm.: 0.0039163850964656745 Norm. Diff.: 0.0007379280568163952 tk: 5 x_norm: 10.0\n",
      "Iteration 853 - Grad. Norm.: 0.003916582722143935 Norm. Diff.: 0.0007354457648749451 tk: 5 x_norm: 10.0\n",
      "Iteration 854 - Grad. Norm.: 0.003916779628770553 Norm. Diff.: 0.0007329727866254875 tk: 5 x_norm: 10.0\n",
      "Iteration 855 - Grad. Norm.: 0.0039169758190553145 Norm. Diff.: 0.0007305090838099482 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 856 - Grad. Norm.: 0.0039171712956971826 Norm. Diff.: 0.0007280546183333057 tk: 5 x_norm: 10.0\n",
      "Iteration 857 - Grad. Norm.: 0.003917366061384287 Norm. Diff.: 0.0007256093522638927 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 858 - Grad. Norm.: 0.003917560118794038 Norm. Diff.: 0.0007231732478309854 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 859 - Grad. Norm.: 0.003917753470593148 Norm. Diff.: 0.0007207462674257778 tk: 5 x_norm: 10.0\n",
      "Iteration 860 - Grad. Norm.: 0.003917946119437694 Norm. Diff.: 0.0007183283735995083 tk: 5 x_norm: 10.0\n",
      "Iteration 861 - Grad. Norm.: 0.003918138067973173 Norm. Diff.: 0.0007159195290634877 tk: 5 x_norm: 10.0\n",
      "Iteration 862 - Grad. Norm.: 0.003918329318834537 Norm. Diff.: 0.0007135196966886062 tk: 5 x_norm: 10.0\n",
      "Iteration 863 - Grad. Norm.: 0.00391851987464628 Norm. Diff.: 0.0007111288395035563 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 864 - Grad. Norm.: 0.003918709738022444 Norm. Diff.: 0.0007087469206952539 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 865 - Grad. Norm.: 0.003918898911566721 Norm. Diff.: 0.0007063739036079272 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 866 - Grad. Norm.: 0.003919087397872469 Norm. Diff.: 0.0007040097517421034 tk: 5 x_norm: 10.0\n",
      "Iteration 867 - Grad. Norm.: 0.003919275199522743 Norm. Diff.: 0.0007016544287544113 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 868 - Grad. Norm.: 0.003919462319090425 Norm. Diff.: 0.0006993078984564283 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 869 - Grad. Norm.: 0.003919648759138175 Norm. Diff.: 0.0006969701248148096 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 870 - Grad. Norm.: 0.003919834522218543 Norm. Diff.: 0.0006946410719499382 tk: 5 x_norm: 10.0\n",
      "Iteration 871 - Grad. Norm.: 0.0039200196108740175 Norm. Diff.: 0.0006923207041351393 tk: 5 x_norm: 10.0\n",
      "Iteration 872 - Grad. Norm.: 0.003920204027637023 Norm. Diff.: 0.0006900089857971103 tk: 5 x_norm: 10.0\n",
      "Iteration 873 - Grad. Norm.: 0.003920387775030032 Norm. Diff.: 0.0006877058815140508 tk: 5 x_norm: 10.0\n",
      "Iteration 874 - Grad. Norm.: 0.003920570855565572 Norm. Diff.: 0.0006854113560157263 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 875 - Grad. Norm.: 0.003920753271746278 Norm. Diff.: 0.0006831253741827166 tk: 5 x_norm: 10.0\n",
      "Iteration 876 - Grad. Norm.: 0.003920935026064961 Norm. Diff.: 0.000680847901045792 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 877 - Grad. Norm.: 0.003921116121004637 Norm. Diff.: 0.0006785789017850145 tk: 5 x_norm: 10.0\n",
      "Iteration 878 - Grad. Norm.: 0.003921296559038552 Norm. Diff.: 0.0006763183417297055 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 879 - Grad. Norm.: 0.00392147634263028 Norm. Diff.: 0.0006740661863571119 tk: 5 x_norm: 10.0\n",
      "Iteration 880 - Grad. Norm.: 0.003921655474233726 Norm. Diff.: 0.0006718224012923847 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 881 - Grad. Norm.: 0.00392183395629317 Norm. Diff.: 0.0006695869523075395 tk: 5 x_norm: 10.0\n",
      "Iteration 882 - Grad. Norm.: 0.003922011791243356 Norm. Diff.: 0.0006673598053210962 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 883 - Grad. Norm.: 0.003922188981509512 Norm. Diff.: 0.0006651409263976851 tk: 5 x_norm: 10.0\n",
      "Iteration 884 - Grad. Norm.: 0.003922365529507333 Norm. Diff.: 0.0006629302817464484 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 885 - Grad. Norm.: 0.003922541437643135 Norm. Diff.: 0.0006607278377215963 tk: 5 x_norm: 10.0\n",
      "Iteration 886 - Grad. Norm.: 0.003922716708313828 Norm. Diff.: 0.0006585335608216131 tk: 5 x_norm: 10.0\n",
      "Iteration 887 - Grad. Norm.: 0.003922891343906964 Norm. Diff.: 0.0006563474176881046 tk: 5 x_norm: 10.0\n",
      "Iteration 888 - Grad. Norm.: 0.003923065346800823 Norm. Diff.: 0.0006541693751052457 tk: 5 x_norm: 10.0\n",
      "Iteration 889 - Grad. Norm.: 0.003923238719364382 Norm. Diff.: 0.0006519993999994434 tk: 5 x_norm: 10.0\n",
      "Iteration 890 - Grad. Norm.: 0.0039234114639574245 Norm. Diff.: 0.0006498374594395066 tk: 5 x_norm: 10.0\n",
      "Iteration 891 - Grad. Norm.: 0.003923583582930558 Norm. Diff.: 0.000647683520634272 tk: 5 x_norm: 10.0\n",
      "Iteration 892 - Grad. Norm.: 0.003923755078625232 Norm. Diff.: 0.0006455375509335623 tk: 5 x_norm: 10.0\n",
      "Iteration 893 - Grad. Norm.: 0.003923925953373851 Norm. Diff.: 0.0006433995178269541 tk: 5 x_norm: 10.0\n",
      "Iteration 894 - Grad. Norm.: 0.003924096209499698 Norm. Diff.: 0.0006412693889432554 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 895 - Grad. Norm.: 0.003924265849317105 Norm. Diff.: 0.0006391471320499191 tk: 5 x_norm: 10.0\n",
      "Iteration 896 - Grad. Norm.: 0.003924434875131393 Norm. Diff.: 0.0006370327150529136 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 897 - Grad. Norm.: 0.00392460328923896 Norm. Diff.: 0.0006349261059948422 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 898 - Grad. Norm.: 0.003924771093927332 Norm. Diff.: 0.0006328272730563748 tk: 5 x_norm: 10.0\n",
      "Iteration 899 - Grad. Norm.: 0.003924938291475152 Norm. Diff.: 0.0006307361845538187 tk: 5 x_norm: 10.0\n",
      "Iteration 900 - Grad. Norm.: 0.00392510488415227 Norm. Diff.: 0.0006286528089393597 tk: 5 x_norm: 10.0\n",
      "Iteration 901 - Grad. Norm.: 0.003925270874219771 Norm. Diff.: 0.0006265771148009065 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 902 - Grad. Norm.: 0.003925436263929972 Norm. Diff.: 0.0006245090708605986 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 903 - Grad. Norm.: 0.003925601055526514 Norm. Diff.: 0.0006224486459746857 tk: 5 x_norm: 10.0\n",
      "Iteration 904 - Grad. Norm.: 0.0039257652512444015 Norm. Diff.: 0.0006203958091335635 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 905 - Grad. Norm.: 0.003925928853309968 Norm. Diff.: 0.00061835052945974 tk: 5 x_norm: 10.0\n",
      "Iteration 906 - Grad. Norm.: 0.003926091863941008 Norm. Diff.: 0.0006163127762088931 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 907 - Grad. Norm.: 0.003926254285346741 Norm. Diff.: 0.000614282518768245 tk: 5 x_norm: 10.0\n",
      "Iteration 908 - Grad. Norm.: 0.003926416119727906 Norm. Diff.: 0.0006122597266566768 tk: 5 x_norm: 10.0\n",
      "Iteration 909 - Grad. Norm.: 0.003926577369276743 Norm. Diff.: 0.0006102443695233021 tk: 5 x_norm: 10.0\n",
      "Iteration 910 - Grad. Norm.: 0.003926738036177078 Norm. Diff.: 0.0006082364171481688 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 911 - Grad. Norm.: 0.003926898122604355 Norm. Diff.: 0.0006062358394404793 tk: 5 x_norm: 10.0\n",
      "Iteration 912 - Grad. Norm.: 0.003927057630725601 Norm. Diff.: 0.0006042426064388324 tk: 5 x_norm: 10.0\n",
      "Iteration 913 - Grad. Norm.: 0.003927216562699563 Norm. Diff.: 0.0006022566883103874 tk: 5 x_norm: 10.0\n",
      "Iteration 914 - Grad. Norm.: 0.0039273749206766954 Norm. Diff.: 0.0006002780553504101 tk: 5 x_norm: 10.0\n",
      "Iteration 915 - Grad. Norm.: 0.003927532706799182 Norm. Diff.: 0.0005983066779816176 tk: 5 x_norm: 10.0\n",
      "Iteration 916 - Grad. Norm.: 0.0039276899232010085 Norm. Diff.: 0.0005963425267539728 tk: 5 x_norm: 10.0\n",
      "Iteration 917 - Grad. Norm.: 0.003927846572007956 Norm. Diff.: 0.0005943855723435932 tk: 5 x_norm: 10.0\n",
      "Iteration 918 - Grad. Norm.: 0.00392800265533766 Norm. Diff.: 0.000592435785552843 tk: 5 x_norm: 10.0\n",
      "Iteration 919 - Grad. Norm.: 0.003928158175299658 Norm. Diff.: 0.0005904931373088822 tk: 5 x_norm: 10.0\n",
      "Iteration 920 - Grad. Norm.: 0.003928313133995388 Norm. Diff.: 0.0005885575986645051 tk: 5 x_norm: 10.0\n",
      "Iteration 921 - Grad. Norm.: 0.003928467533518261 Norm. Diff.: 0.0005866291407964799 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 922 - Grad. Norm.: 0.003928621375953673 Norm. Diff.: 0.0005847077350056687 tk: 5 x_norm: 10.0\n",
      "Iteration 923 - Grad. Norm.: 0.003928774663379027 Norm. Diff.: 0.0005827933527158169 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 924 - Grad. Norm.: 0.0039289273978638 Norm. Diff.: 0.000580885965473689 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 925 - Grad. Norm.: 0.003929079581469549 Norm. Diff.: 0.0005789855449486233 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 926 - Grad. Norm.: 0.003929231216249948 Norm. Diff.: 0.0005770920629310024 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 927 - Grad. Norm.: 0.0039293823042508445 Norm. Diff.: 0.0005752054913330462 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 928 - Grad. Norm.: 0.003929532847510267 Norm. Diff.: 0.0005733258021877801 tk: 5 x_norm: 10.0\n",
      "Iteration 929 - Grad. Norm.: 0.0039296828480584614 Norm. Diff.: 0.0005714529676477617 tk: 5 x_norm: 10.0\n",
      "Iteration 930 - Grad. Norm.: 0.003929832307917929 Norm. Diff.: 0.0005695869599862656 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 931 - Grad. Norm.: 0.003929981229103436 Norm. Diff.: 0.0005677277515949397 tk: 5 x_norm: 10.0\n",
      "Iteration 932 - Grad. Norm.: 0.003930129613622128 Norm. Diff.: 0.0005658753149843331 tk: 5 x_norm: 10.0\n",
      "Iteration 933 - Grad. Norm.: 0.003930277463473426 Norm. Diff.: 0.0005640296227835369 tk: 5 x_norm: 10.0\n",
      "Iteration 934 - Grad. Norm.: 0.0039304247806491764 Norm. Diff.: 0.000562190647738932 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 935 - Grad. Norm.: 0.003930571567133623 Norm. Diff.: 0.0005603583627145807 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 936 - Grad. Norm.: 0.00393071782490345 Norm. Diff.: 0.0005585327406906917 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 937 - Grad. Norm.: 0.00393086355592785 Norm. Diff.: 0.000556713754764195 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 938 - Grad. Norm.: 0.00393100876216846 Norm. Diff.: 0.0005549013781473779 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 939 - Grad. Norm.: 0.0039311534455795235 Norm. Diff.: 0.0005530955841678814 tk: 5 x_norm: 10.0\n",
      "Iteration 940 - Grad. Norm.: 0.0039312976081077875 Norm. Diff.: 0.0005512963462685243 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 941 - Grad. Norm.: 0.003931441251692636 Norm. Diff.: 0.0005495036380055815 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 942 - Grad. Norm.: 0.00393158437826607 Norm. Diff.: 0.0005477174330497013 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 943 - Grad. Norm.: 0.003931726989752746 Norm. Diff.: 0.0005459377051848418 tk: 5 x_norm: 10.0\n",
      "Iteration 944 - Grad. Norm.: 0.003931869088070023 Norm. Diff.: 0.0005441644283070915 tk: 5 x_norm: 10.0\n",
      "Iteration 945 - Grad. Norm.: 0.003932010675127938 Norm. Diff.: 0.0005423975764259873 tk: 5 x_norm: 10.0\n",
      "Iteration 946 - Grad. Norm.: 0.003932151752829309 Norm. Diff.: 0.0005406371236619936 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 947 - Grad. Norm.: 0.003932292323069742 Norm. Diff.: 0.0005388830442472111 tk: 5 x_norm: 10.0\n",
      "Iteration 948 - Grad. Norm.: 0.003932432387737606 Norm. Diff.: 0.0005371353125248386 tk: 5 x_norm: 10.0\n",
      "Iteration 949 - Grad. Norm.: 0.0039325719487141395 Norm. Diff.: 0.0005353939029482517 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 950 - Grad. Norm.: 0.003932711007873412 Norm. Diff.: 0.0005336587900812106 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 951 - Grad. Norm.: 0.003932849567082419 Norm. Diff.: 0.0005319299485963626 tk: 5 x_norm: 10.0\n",
      "Iteration 952 - Grad. Norm.: 0.003932987628201057 Norm. Diff.: 0.000530207353275985 tk: 5 x_norm: 10.0\n",
      "Iteration 953 - Grad. Norm.: 0.003933125193082185 Norm. Diff.: 0.0005284909790104303 tk: 5 x_norm: 10.0\n",
      "Iteration 954 - Grad. Norm.: 0.003933262263571626 Norm. Diff.: 0.0005267808007983924 tk: 5 x_norm: 10.0\n",
      "Iteration 955 - Grad. Norm.: 0.003933398841508197 Norm. Diff.: 0.0005250767937464137 tk: 5 x_norm: 10.0\n",
      "Iteration 956 - Grad. Norm.: 0.003933534928723772 Norm. Diff.: 0.0005233789330674974 tk: 5 x_norm: 9.999999999999996\n",
      "Iteration 957 - Grad. Norm.: 0.0039336705270432756 Norm. Diff.: 0.0005216871940825576 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 958 - Grad. Norm.: 0.003933805638284729 Norm. Diff.: 0.0005200015522175269 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 959 - Grad. Norm.: 0.003933940264259257 Norm. Diff.: 0.0005183219830048819 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 960 - Grad. Norm.: 0.003934074406771139 Norm. Diff.: 0.0005166484620822431 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 961 - Grad. Norm.: 0.003934208067617825 Norm. Diff.: 0.0005149809651921463 tk: 5 x_norm: 10.0\n",
      "Iteration 962 - Grad. Norm.: 0.003934341248589935 Norm. Diff.: 0.0005133194681818323 tk: 5 x_norm: 10.0\n",
      "Iteration 963 - Grad. Norm.: 0.003934473951471356 Norm. Diff.: 0.000511663947002056 tk: 5 x_norm: 10.0\n",
      "Iteration 964 - Grad. Norm.: 0.003934606178039205 Norm. Diff.: 0.0005100143777076678 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 965 - Grad. Norm.: 0.003934737930063867 Norm. Diff.: 0.0005083707364561337 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 966 - Grad. Norm.: 0.003934869209309073 Norm. Diff.: 0.0005067329995081553 tk: 5 x_norm: 10.0\n",
      "Iteration 967 - Grad. Norm.: 0.003935000017531815 Norm. Diff.: 0.0005051011432261172 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 968 - Grad. Norm.: 0.003935130356482519 Norm. Diff.: 0.0005034751440745628 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 969 - Grad. Norm.: 0.003935260227904945 Norm. Diff.: 0.0005018549786194415 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 970 - Grad. Norm.: 0.003935389633536273 Norm. Diff.: 0.0005002406235272533 tk: 5 x_norm: 10.0\n",
      "Iteration 971 - Grad. Norm.: 0.003935518575107138 Norm. Diff.: 0.0004986320555652523 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 972 - Grad. Norm.: 0.003935647054341601 Norm. Diff.: 0.0004970292516010637 tk: 5 x_norm: 10.0\n",
      "Iteration 973 - Grad. Norm.: 0.003935775072957237 Norm. Diff.: 0.0004954321886011592 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 974 - Grad. Norm.: 0.00393590263266513 Norm. Diff.: 0.000493840843631752 tk: 5 x_norm: 10.0\n",
      "Iteration 975 - Grad. Norm.: 0.003936029735169895 Norm. Diff.: 0.0004922551938578673 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 976 - Grad. Norm.: 0.003936156382169707 Norm. Diff.: 0.0004906752165426884 tk: 5 x_norm: 10.0\n",
      "Iteration 977 - Grad. Norm.: 0.003936282575356339 Norm. Diff.: 0.0004891008890473912 tk: 5 x_norm: 10.0\n",
      "Iteration 978 - Grad. Norm.: 0.003936408316415166 Norm. Diff.: 0.0004875321888306599 tk: 5 x_norm: 10.0\n",
      "Iteration 979 - Grad. Norm.: 0.003936533607025211 Norm. Diff.: 0.00048596909344824263 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 980 - Grad. Norm.: 0.003936658448859163 Norm. Diff.: 0.0004844115805523763 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 981 - Grad. Norm.: 0.003936782843583375 Norm. Diff.: 0.0004828596278918004 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 982 - Grad. Norm.: 0.003936906792857927 Norm. Diff.: 0.00048131321331115084 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 983 - Grad. Norm.: 0.0039370302983366525 Norm. Diff.: 0.0004797723147499685 tk: 5 x_norm: 10.0\n",
      "Iteration 984 - Grad. Norm.: 0.003937153361667121 Norm. Diff.: 0.0004782369102433756 tk: 5 x_norm: 10.0\n",
      "Iteration 985 - Grad. Norm.: 0.0039372759844906804 Norm. Diff.: 0.00047670697792089636 tk: 5 x_norm: 10.000000000000004\n",
      "Iteration 986 - Grad. Norm.: 0.003937398168442514 Norm. Diff.: 0.0004751824960059138 tk: 5 x_norm: 10.0\n",
      "Iteration 987 - Grad. Norm.: 0.00393751991515162 Norm. Diff.: 0.0004736634428159444 tk: 5 x_norm: 10.0\n",
      "Iteration 988 - Grad. Norm.: 0.003937641226240855 Norm. Diff.: 0.0004721497967617428 tk: 5 x_norm: 10.0\n",
      "Iteration 989 - Grad. Norm.: 0.003937762103326961 Norm. Diff.: 0.00047064153634733557 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 990 - Grad. Norm.: 0.003937882548020574 Norm. Diff.: 0.0004691386401686499 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 991 - Grad. Norm.: 0.00393800256192627 Norm. Diff.: 0.000467641086914253 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 992 - Grad. Norm.: 0.003938122146642567 Norm. Diff.: 0.00046614885536430407 tk: 5 x_norm: 10.0\n",
      "Iteration 993 - Grad. Norm.: 0.003938241303761949 Norm. Diff.: 0.0004646619243903941 tk: 5 x_norm: 10.0\n",
      "Iteration 994 - Grad. Norm.: 0.003938360034870919 Norm. Diff.: 0.0004631802729550627 tk: 5 x_norm: 10.0\n",
      "Iteration 995 - Grad. Norm.: 0.003938478341549992 Norm. Diff.: 0.00046170388011126885 tk: 5 x_norm: 9.999999999999998\n",
      "Iteration 996 - Grad. Norm.: 0.003938596225373692 Norm. Diff.: 0.00046023272500239724 tk: 5 x_norm: 10.0\n",
      "Iteration 997 - Grad. Norm.: 0.003938713687910673 Norm. Diff.: 0.0004587667868614312 tk: 5 x_norm: 10.0\n",
      "Iteration 998 - Grad. Norm.: 0.003938830730723631 Norm. Diff.: 0.00045730604501039666 tk: 5 x_norm: 10.000000000000002\n",
      "Iteration 999 - Grad. Norm.: 0.00393894735536937 Norm. Diff.: 0.0004558504788613431 tk: 5 x_norm: 10.0\n",
      "最小值: 0.093422\t耗时: 100.567786s\n"
     ]
    }
   ],
   "source": [
    "#* Armijo rule \n",
    "def armijo_search(f, f_grad, xk, t_hat, alpha, beta, D, isNewton=False, dk=None):\n",
    "    if isNewton:\n",
    "        assert dk is not None\n",
    "    tk = t_hat*1\n",
    "    grad = f_grad(xk)\n",
    "    while True:\n",
    "        if isNewton:\n",
    "            if np.linalg.norm(xk+tk*dk,ord=2)<=D/2 and f(xk+tk*dk) <= f(xk) + alpha*tk*grad.T@dk:\n",
    "                break\n",
    "        else:\n",
    "            if f(xk-tk*grad) <= f(xk)-alpha*tk*grad.T@grad:\n",
    "                break\n",
    "        tk *= beta\n",
    "    return tk\n",
    "\n",
    "def project(x,D):\n",
    "    x_norm = np.linalg.norm(x)\n",
    "    if x_norm <= D/2:\n",
    "        return x\n",
    "    coef = D/2/x_norm\n",
    "    return coef*x\n",
    "\n",
    "#* 投影梯度法\n",
    "def projected_gradient_descent(f, f_grad, x0, D, t_hat=1, epsilon=1e-6, max_iters=10000):\n",
    "    # func_val_record = []\n",
    "    func_val_record = []\n",
    "    grad_norm_record = []\n",
    "    xk = x0\n",
    "    xk_norm = np.linalg.norm(xk)\n",
    "    t_s = time()\n",
    "    # for idx in trange(max_iters):\n",
    "    for idx in range(max_iters):\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=t_hat, alpha=0.1, beta=0.5, D=D)\n",
    "        xk_next = project(xk-tk*f_grad(xk), D)\n",
    "        fval_xk_next = f(xk_next)\n",
    "        grad_xk_next = f_grad(xk_next)\n",
    "        func_val_record.append(fval_xk_next)\n",
    "        grad_norm_next = np.linalg.norm(grad_xk_next,ord=2)\n",
    "        grad_norm_record.append(grad_norm_next)\n",
    "        # termination criteria\n",
    "        # if grad_norm_next<=epsilon:\n",
    "        #     break\n",
    "        # if grad_norm_next<=epsilon or np.linalg.norm(xk_next)>=D/2-1e-3:\n",
    "        #     break\n",
    "        norm_diff = np.linalg.norm(xk_next-xk)\n",
    "        if norm_diff<=epsilon:\n",
    "            break\n",
    "        # if (func_val_record[-1]-func_val_record[-2])/func_val_record[-2]<=epsilon:\n",
    "        #     break\n",
    "        else:\n",
    "            print(f'Iteration {idx} - Grad. Norm.:',grad_norm_next, 'Norm. Diff.:',norm_diff,'tk:',tk, 'x_norm:',np.linalg.norm(xk_next))\n",
    "        xk = xk_next\n",
    "    t_e = time()\n",
    "    return xk_next, np.asarray(func_val_record), np.asarray(grad_norm_record), t_e-t_s\n",
    "\n",
    "np.random.seed(1000)\n",
    "init_x = np.zeros(n)+0.005\n",
    "x_opt_pgd, _, _, t_pgd = projected_gradient_descent(f=f, f_grad=f_grad, x0=init_x, D=20, t_hat=5, epsilon=1e-4, max_iters=1000)\n",
    "print(f'最小值: {f(x_opt_pgd):>2f}\\t耗时: {t_pgd:>2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.736836381041056e-06"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_opt_pgd)-f(x_opt_ipm_damped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVXPY-计算 optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优值: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzq22/anaconda3/envs/optim/lib/python3.8/site-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 8 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# D=500\n",
    "# x = cp.Variable(n)\n",
    "# # objective = cp.Minimize(sum(cp.log1p(cp.exp(-b*(A@x))))/m + 1/(100*m)* x.T@x)\n",
    "# # objective = cp.Minimize(sum(cp.log1p(cp.exp(-b*(A@x))))/m)\n",
    "# objective = cp.Minimize(cp.norm(x)**2 + sum(cp.log1p(cp.exp(-b*(A@x))))/m)\n",
    "# constraints = [cp.norm(x)<=D/2]\n",
    "# problem = cp.Problem(objective,constraints)\n",
    "# result = problem.solve()\n",
    "# print(f'最优值: {f(x.value)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyhton 3 [optim]",
   "language": "python",
   "name": "optim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
