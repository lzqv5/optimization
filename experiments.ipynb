{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from time import time\n",
    "from scipy.interpolate import interp1d\n",
    "from libsvm.svmutil import svm_read_problem # https://blog.csdn.net/u013630349/article/details/47323883\n",
    "\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single(seq, xlabel='Iteration', ylabel='Gradient Norm', title=''):\n",
    "    plt.figure()\n",
    "    iterations = np.arange(len(seq))+1\n",
    "    plt.semilogy(iterations,seq)\n",
    "    plt.xticks(iterations)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "def plot_multi_seqs(seqs, xlabel='Iteration', ylabel='Gradient Norm', title='', xtick_step = 50):\n",
    "    plt.figure(figsize=(16,8), dpi=150)\n",
    "    maxLen = 0\n",
    "    for seq in seqs:\n",
    "        iterations = seq.index\n",
    "        if iterations.size > maxLen:\n",
    "            maxLen = iterations.size\n",
    "        plt.semilogy(iterations, seq, label=seq.name)\n",
    "    plt.xticks(np.arange(stop=maxLen,step=xtick_step)+1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45546, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(path):\n",
    "    b, A = svm_read_problem(path)\n",
    "    rows = len(b)   # 矩阵行数, i.e. sample 数\n",
    "    cols = max([max(row.keys()) if len(row)>0 else 0 for row in A])  # 矩阵列数, i.e. feature 数\n",
    "    b = np.array(b)\n",
    "    A_np = np.zeros((rows,cols))\n",
    "    for r in range(rows):\n",
    "        for c in A[r].keys():\n",
    "            # MatLab 是 1-index, python 则是 0-index\n",
    "            A_np[r,c-1] = A[r][c]\n",
    "    # 清楚全 0 features\n",
    "    effective_row_ids = []\n",
    "    for idx, row in enumerate(A_np):\n",
    "        if np.sum(row) > 1e-3:\n",
    "            effective_row_ids.append(idx)\n",
    "    return b[effective_row_ids], A_np[effective_row_ids]\n",
    "\n",
    "b, A = read_data('w8a')\n",
    "m,n = A.shape\n",
    "m,n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令 $g_i(x)=\\exp(b_i x^\\top a_i), i=1,2,...,m$\n",
    "\n",
    "所以可以得到目标函数为:\n",
    "$$\n",
    "f(x)=\\frac{1}{m}\\sum_{i=1}^{m}\\log\\left( 1+ \\frac{1}{g_i(x)}\\right) + \\frac{1}{100m}\\left\\|x\\right\\|^2\n",
    "$$\n",
    "进而可以得到梯度 $\\nabla f(x)$ 和 Hessian矩阵 $\\nabla^2 f(x)$ 的形式:\n",
    "$$\n",
    "\\nabla f(x) = \\frac{1}{m}\\sum_{i=1}^{m}\\left[ -b_i a_i (1+g_i(x))^{-1} \\right] + \\frac{1}{50m}x\n",
    "$$\n",
    "$$\n",
    "\\nabla^2 f(x) =  \\frac{1}{m}\\sum_{i=1}^{m}\\left( b_i^2 \\frac{g_i(x)}{(1+g_i(x))^2}a_i a_i^\\top \\right) + \\frac{1}{50m}I\n",
    "$$\n",
    "\n",
    "此处函数内的 $a_i,b_i$ 和数据本身相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    bAx = b*(A@x)\n",
    "    exp_mbAx = np.exp(-bAx)\n",
    "    log1p_exp = np.log(1+exp_mbAx)\n",
    "    overflow_idxs = np.where(exp_mbAx==float('inf'))\n",
    "    log1p_exp[overflow_idxs] = -bAx[overflow_idxs]\n",
    "    # return np.log(1+np.exp(-bAx)).mean() + 1/(100*m)* x.T@x\n",
    "    return log1p_exp.mean() + 1/(100*m)* x.T@x\n",
    "\n",
    "def f_grad(x):\n",
    "    # return np.ones(m)@(np.expand_dims((-b)/(1+np.exp(b*(A@x))), axis=1)*A)/m\n",
    "    return np.ones(m)@(np.expand_dims((-b)/(1+np.exp(b*(A@x))), axis=1)*A)/m + 1/(50*m)*x\n",
    "\n",
    "def f_hessian(x):\n",
    "    Ax = A@x\n",
    "    exp_bAx = np.exp(b*Ax)\n",
    "    # return (A.T @ (np.expand_dims(b*b*exp_bAx/(1+exp_bAx)**2, axis=1)*A) )/m\n",
    "    return (A.T @ (np.expand_dims(b*b*exp_bAx/(1+exp_bAx)**2, axis=1)*A) )/m + 1/(50*m)*np.diag([1.0]*x.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Armijo rule \n",
    "def armijo_search(f, f_grad, xk, t_hat, alpha, beta, D, isNewton=False, dk=None):\n",
    "    if isNewton:\n",
    "        assert dk is not None\n",
    "    tk = t_hat*1\n",
    "    grad = f_grad(xk)\n",
    "    while True:\n",
    "        if isNewton:\n",
    "            if np.linalg.norm(xk+tk*dk,ord=2)<=D/2 and f(xk+tk*dk) <= f(xk) + alpha*tk*grad.T@dk:\n",
    "                break\n",
    "        else:\n",
    "            if np.linalg.norm(xk-tk*grad,ord=2)<=D/2 and f(xk-tk*grad) <= f(xk)-alpha*tk*grad.T@grad:\n",
    "                break\n",
    "        tk *= beta\n",
    "    return tk\n",
    "\n",
    "#* 梯度下降法\n",
    "def gradient_descent(f, f_grad, x0, D, t_hat=1, epsilon=1e-6, max_iters=10000):\n",
    "    # func_val_record = []\n",
    "    func_val_record = [f(x0)]\n",
    "    grad_norm_record = []\n",
    "    xk = x0\n",
    "    t_s = time()\n",
    "    # for idx in trange(max_iters):\n",
    "    for idx in range(max_iters):\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=t_hat, alpha=0.1, beta=0.5, D=D)\n",
    "        xk_next = xk-tk*f_grad(xk)\n",
    "        fval_xk_next = f(xk_next)\n",
    "        grad_xk_next = f_grad(xk_next)\n",
    "        func_val_record.append(fval_xk_next)\n",
    "        grad_norm_next = np.linalg.norm(grad_xk_next,ord=2)\n",
    "        grad_norm_record.append(grad_norm_next)\n",
    "        # termination criteria\n",
    "        # if grad_norm_next<=epsilon:\n",
    "        #     break\n",
    "        if grad_norm_next<=epsilon or np.linalg.norm(xk_next)>=D/2-1e-3:\n",
    "            break\n",
    "        # if (func_val_record[-1]-func_val_record[-2])/func_val_record[-2]<=epsilon:\n",
    "        #     break\n",
    "        else:\n",
    "            print(f'Iteration {idx} - Grad. Norm.:',grad_norm_next, 'tk:',tk, 'x_norm:',np.linalg.norm(xk_next))\n",
    "        xk = xk_next\n",
    "    t_e = time()\n",
    "    return xk_next, np.asarray(func_val_record), np.asarray(grad_norm_record), t_e-t_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Grad. Norm.: 0.052109680543728935 tk: 5 x_norm: 3.1948158750857765\n",
      "Iteration 1 - Grad. Norm.: 0.03874538795695223 tk: 5 x_norm: 3.3007664609434073\n",
      "Iteration 2 - Grad. Norm.: 0.03247236111795165 tk: 5 x_norm: 3.39832364755234\n",
      "Iteration 3 - Grad. Norm.: 0.028707614758793267 tk: 5 x_norm: 3.488927444461727\n",
      "Iteration 4 - Grad. Norm.: 0.026137955548665787 tk: 5 x_norm: 3.573880284211031\n",
      "Iteration 5 - Grad. Norm.: 0.024238255924168794 tk: 5 x_norm: 3.654164258131663\n",
      "Iteration 6 - Grad. Norm.: 0.02275515238979999 tk: 5 x_norm: 3.730525846831219\n",
      "Iteration 7 - Grad. Norm.: 0.021550498078459098 tk: 5 x_norm: 3.803544343824972\n",
      "Iteration 8 - Grad. Norm.: 0.02054217078675201 tk: 5 x_norm: 3.873677891009221\n",
      "Iteration 9 - Grad. Norm.: 0.01967811317099874 tk: 5 x_norm: 3.9412945794501484\n",
      "Iteration 10 - Grad. Norm.: 0.018923649545407387 tk: 5 x_norm: 4.006694049320368\n",
      "Iteration 11 - Grad. Norm.: 0.018254755248512777 tk: 5 x_norm: 4.070122926376995\n",
      "Iteration 12 - Grad. Norm.: 0.017654243223426912 tk: 5 x_norm: 4.131786136275055\n",
      "Iteration 13 - Grad. Norm.: 0.017109485628512194 tk: 5 x_norm: 4.1918553780763546\n",
      "Iteration 14 - Grad. Norm.: 0.01661099162910515 tk: 5 x_norm: 4.250475586797769\n",
      "Iteration 15 - Grad. Norm.: 0.016151486504397905 tk: 5 x_norm: 4.307769938849529\n",
      "Iteration 16 - Grad. Norm.: 0.015725296588775545 tk: 5 x_norm: 4.3638437802112815\n",
      "Iteration 17 - Grad. Norm.: 0.015327927455063974 tk: 5 x_norm: 4.41878774419224\n",
      "Iteration 18 - Grad. Norm.: 0.01495576795345957 tk: 5 x_norm: 4.472680250221823\n",
      "Iteration 19 - Grad. Norm.: 0.014605878410723272 tk: 5 x_norm: 4.525589523576644\n",
      "Iteration 20 - Grad. Norm.: 0.014275836426987468 tk: 5 x_norm: 4.577575239960449\n",
      "Iteration 21 - Grad. Norm.: 0.013963622905490478 tk: 5 x_norm: 4.628689873249136\n",
      "Iteration 22 - Grad. Norm.: 0.013667536698368668 tk: 5 x_norm: 4.6789798061940004\n",
      "Iteration 23 - Grad. Norm.: 0.013386129933440693 tk: 5 x_norm: 4.728486250282938\n",
      "Iteration 24 - Grad. Norm.: 0.013118158498739279 tk: 5 x_norm: 4.777246010846559\n",
      "Iteration 25 - Grad. Norm.: 0.012862543774248811 tk: 5 x_norm: 4.825292125878381\n",
      "Iteration 26 - Grad. Norm.: 0.012618342799414947 tk: 5 x_norm: 4.872654401231959\n",
      "Iteration 27 - Grad. Norm.: 0.012384724827479992 tk: 5 x_norm: 4.919359860382872\n",
      "Iteration 28 - Grad. Norm.: 0.01216095275544152 tk: 5 x_norm: 4.9654331234576885\n",
      "Iteration 29 - Grad. Norm.: 0.01194636830344556 tk: 5 x_norm: 5.010896727490012\n",
      "Iteration 30 - Grad. Norm.: 0.011740380096867469 tk: 5 x_norm: 5.055771397686673\n",
      "Iteration 31 - Grad. Norm.: 0.011542454009604618 tk: 5 x_norm: 5.100076277744397\n",
      "Iteration 32 - Grad. Norm.: 0.011352105279463776 tk: 5 x_norm: 5.1438291258517665\n",
      "Iteration 33 - Grad. Norm.: 0.011168892020605283 tk: 5 x_norm: 5.187046481870443\n",
      "Iteration 34 - Grad. Norm.: 0.010992409844044392 tk: 5 x_norm: 5.229743810258475\n",
      "Iteration 35 - Grad. Norm.: 0.01082228736248468 tk: 5 x_norm: 5.271935622535017\n",
      "Iteration 36 - Grad. Norm.: 0.010658182405517774 tk: 5 x_norm: 5.313635582457262\n",
      "Iteration 37 - Grad. Norm.: 0.010499778809307676 tk: 5 x_norm: 5.3548565965613575\n",
      "Iteration 38 - Grad. Norm.: 0.010346783674123578 tk: 5 x_norm: 5.395610892289318\n",
      "Iteration 39 - Grad. Norm.: 0.010198925005613414 tk: 5 x_norm: 5.435910085567235\n",
      "Iteration 40 - Grad. Norm.: 0.010055949673115894 tk: 5 x_norm: 5.475765239403506\n",
      "Iteration 41 - Grad. Norm.: 0.00991762163179917 tk: 5 x_norm: 5.515186914828691\n",
      "Iteration 42 - Grad. Norm.: 0.009783720365906778 tk: 5 x_norm: 5.554185215292414\n",
      "Iteration 43 - Grad. Norm.: 0.009654039518584677 tk: 5 x_norm: 5.592769825460382\n",
      "Iteration 44 - Grad. Norm.: 0.009528385680190871 tk: 5 x_norm: 5.630950045210301\n",
      "Iteration 45 - Grad. Norm.: 0.00940657731205772 tk: 5 x_norm: 5.668734819504399\n",
      "Iteration 46 - Grad. Norm.: 0.009288443786697239 tk: 5 x_norm: 5.706132764714663\n",
      "Iteration 47 - Grad. Norm.: 0.009173824528649512 tk: 5 x_norm: 5.743152191891231\n",
      "Iteration 48 - Grad. Norm.: 0.009062568242754069 tk: 5 x_norm: 5.779801127392229\n",
      "Iteration 49 - Grad. Norm.: 0.008954532218712436 tk: 5 x_norm: 5.816087331232378\n",
      "Iteration 50 - Grad. Norm.: 0.008849581702513225 tk: 5 x_norm: 5.852018313456026\n",
      "Iteration 51 - Grad. Norm.: 0.008747589326689507 tk: 5 x_norm: 5.887601348796511\n",
      "Iteration 52 - Grad. Norm.: 0.008648434592534953 tk: 5 x_norm: 5.922843489846482\n",
      "Iteration 53 - Grad. Norm.: 0.008552003398367881 tk: 5 x_norm: 5.9577515789322195\n",
      "Iteration 54 - Grad. Norm.: 0.00845818760873844 tk: 5 x_norm: 5.992332258857884\n",
      "Iteration 55 - Grad. Norm.: 0.008366884660153454 tk: 5 x_norm: 6.026591982662588\n",
      "Iteration 56 - Grad. Norm.: 0.00827799719946767 tk: 5 x_norm: 6.060537022513511\n",
      "Iteration 57 - Grad. Norm.: 0.00819143275157915 tk: 5 x_norm: 6.094173477841201\n",
      "Iteration 58 - Grad. Norm.: 0.008107103413483375 tk: 5 x_norm: 6.127507282808913\n",
      "Iteration 59 - Grad. Norm.: 0.008024925572098265 tk: 5 x_norm: 6.160544213195214\n",
      "Iteration 60 - Grad. Norm.: 0.007944819643579409 tk: 5 x_norm: 6.193289892758523\n",
      "Iteration 61 - Grad. Norm.: 0.007866709832110139 tk: 5 x_norm: 6.2257497991429025\n",
      "Iteration 62 - Grad. Norm.: 0.007790523906379961 tk: 5 x_norm: 6.257929269376674\n",
      "Iteration 63 - Grad. Norm.: 0.007716192992164045 tk: 5 x_norm: 6.289833505008441\n",
      "Iteration 64 - Grad. Norm.: 0.007643651379589124 tk: 5 x_norm: 6.32146757691937\n",
      "Iteration 65 - Grad. Norm.: 0.007572836343822244 tk: 5 x_norm: 6.352836429845363\n",
      "Iteration 66 - Grad. Norm.: 0.007503687978050367 tk: 5 x_norm: 6.383944886638537\n",
      "Iteration 67 - Grad. Norm.: 0.007436149037734645 tk: 5 x_norm: 6.414797652293544\n",
      "Iteration 68 - Grad. Norm.: 0.007370164795224553 tk: 5 x_norm: 6.445399317761025\n",
      "Iteration 69 - Grad. Norm.: 0.007305682903906841 tk: 5 x_norm: 6.4757543635677415\n",
      "Iteration 70 - Grad. Norm.: 0.007242653271142864 tk: 5 x_norm: 6.505867163260412\n",
      "Iteration 71 - Grad. Norm.: 0.007181027939318518 tk: 5 x_norm: 6.535741986688262\n",
      "Iteration 72 - Grad. Norm.: 0.007120760974392532 tk: 5 x_norm: 6.565383003137404\n",
      "Iteration 73 - Grad. Norm.: 0.00706180836138476 tk: 5 x_norm: 6.594794284328683\n",
      "Iteration 74 - Grad. Norm.: 0.0070041279062951845 tk: 5 x_norm: 6.623979807289179\n",
      "Iteration 75 - Grad. Norm.: 0.006947679143988687 tk: 5 x_norm: 6.652943457106436\n",
      "Iteration 76 - Grad. Norm.: 0.006892423251620309 tk: 5 x_norm: 6.681689029573478\n",
      "Iteration 77 - Grad. Norm.: 0.006838322967211188 tk: 5 x_norm: 6.710220233731739\n",
      "Iteration 78 - Grad. Norm.: 0.006785342513017727 tk: 5 x_norm: 6.738540694318314\n",
      "Iteration 79 - Grad. Norm.: 0.006733447523365182 tk: 5 x_norm: 6.766653954123283\n",
      "Iteration 80 - Grad. Norm.: 0.006682604976643397 tk: 5 x_norm: 6.79456347626223\n",
      "Iteration 81 - Grad. Norm.: 0.006632783131185928 tk: 5 x_norm: 6.8222726463686225\n",
      "Iteration 82 - Grad. Norm.: 0.006583951464775524 tk: 5 x_norm: 6.849784774710303\n",
      "Iteration 83 - Grad. Norm.: 0.006536080617538423 tk: 5 x_norm: 6.877103098233852\n",
      "Iteration 84 - Grad. Norm.: 0.006489142338007995 tk: 5 x_norm: 6.904230782540435\n",
      "Iteration 85 - Grad. Norm.: 0.006443109432154281 tk: 5 x_norm: 6.931170923796241\n",
      "Iteration 86 - Grad. Norm.: 0.006397955715191257 tk: 5 x_norm: 6.957926550580559\n",
      "Iteration 87 - Grad. Norm.: 0.006353655965987035 tk: 5 x_norm: 6.984500625674142\n",
      "Iteration 88 - Grad. Norm.: 0.006310185883914801 tk: 5 x_norm: 7.010896047790501\n",
      "Iteration 89 - Grad. Norm.: 0.006267522047993853 tk: 5 x_norm: 7.037115653252374\n",
      "Iteration 90 - Grad. Norm.: 0.0062256418781806155 tk: 5 x_norm: 7.063162217615676\n",
      "Iteration 91 - Grad. Norm.: 0.006184523598679103 tk: 5 x_norm: 7.0890384572429275\n",
      "Iteration 92 - Grad. Norm.: 0.006144146203149559 tk: 5 x_norm: 7.114747030828146\n",
      "Iteration 93 - Grad. Norm.: 0.006104489421701856 tk: 5 x_norm: 7.140290540875032\n",
      "Iteration 94 - Grad. Norm.: 0.006065533689568208 tk: 5 x_norm: 7.1656715351301425\n",
      "Iteration 95 - Grad. Norm.: 0.0060272601173566075 tk: 5 x_norm: 7.190892507972765\n",
      "Iteration 96 - Grad. Norm.: 0.0059896504627929064 tk: 5 x_norm: 7.215955901762982\n",
      "Iteration 97 - Grad. Norm.: 0.005952687103865626 tk: 5 x_norm: 7.2408641081494585\n",
      "Iteration 98 - Grad. Norm.: 0.005916353013292988 tk: 5 x_norm: 7.2656194693383656\n",
      "Iteration 99 - Grad. Norm.: 0.005880631734237009 tk: 5 x_norm: 7.290224279324753\n",
      "Iteration 100 - Grad. Norm.: 0.005845507357194208 tk: 5 x_norm: 7.3146807850877344\n",
      "Iteration 101 - Grad. Norm.: 0.005810964497996975 tk: 5 x_norm: 7.338991187750657\n",
      "Iteration 102 - Grad. Norm.: 0.005776988276863809 tk: 5 x_norm: 7.363157643707488\n",
      "Iteration 103 - Grad. Norm.: 0.005743564298440498 tk: 5 x_norm: 7.38718226571656\n",
      "Iteration 104 - Grad. Norm.: 0.005710678632777918 tk: 5 x_norm: 7.4110671239627255\n",
      "Iteration 105 - Grad. Norm.: 0.005678317797195463 tk: 5 x_norm: 7.434814247089042\n",
      "Iteration 106 - Grad. Norm.: 0.00564646873898221 tk: 5 x_norm: 7.458425623198962\n",
      "Iteration 107 - Grad. Norm.: 0.005615118818890957 tk: 5 x_norm: 7.481903200829988\n",
      "Iteration 108 - Grad. Norm.: 0.005584255795382843 tk: 5 x_norm: 7.505248889899782\n",
      "Iteration 109 - Grad. Norm.: 0.005553867809582866 tk: 5 x_norm: 7.528464562625599\n",
      "Iteration 110 - Grad. Norm.: 0.005523943370908989 tk: 5 x_norm: 7.551552054417913\n",
      "Iteration 111 - Grad. Norm.: 0.0054944713433397645 tk: 5 x_norm: 7.574513164749113\n",
      "Iteration 112 - Grad. Norm.: 0.005465440932287348 tk: 5 x_norm: 7.597349657998026\n",
      "Iteration 113 - Grad. Norm.: 0.005436841672044922 tk: 5 x_norm: 7.620063264271106\n",
      "Iteration 114 - Grad. Norm.: 0.0054086634137791205 tk: 5 x_norm: 7.64265568020099\n",
      "Iteration 115 - Grad. Norm.: 0.005380896314039965 tk: 5 x_norm: 7.6651285697231835\n",
      "Iteration 116 - Grad. Norm.: 0.005353530823762216 tk: 5 x_norm: 7.687483564831559\n",
      "Iteration 117 - Grad. Norm.: 0.005326557677733729 tk: 5 x_norm: 7.709722266313349\n",
      "Iteration 118 - Grad. Norm.: 0.00529996788450755 tk: 5 x_norm: 7.7318462444642835\n",
      "Iteration 119 - Grad. Norm.: 0.005273752716736079 tk: 5 x_norm: 7.753857039784492\n",
      "Iteration 120 - Grad. Norm.: 0.005247903701906571 tk: 5 x_norm: 7.775756163655808\n",
      "Iteration 121 - Grad. Norm.: 0.005222412613458654 tk: 5 x_norm: 7.797545099001002\n",
      "Iteration 122 - Grad. Norm.: 0.0051972714622654 tk: 5 x_norm: 7.81922530092558\n",
      "Iteration 123 - Grad. Norm.: 0.005172472488460624 tk: 5 x_norm: 7.840798197342625\n",
      "Iteration 124 - Grad. Norm.: 0.005148008153596033 tk: 5 x_norm: 7.8622651895812625\n",
      "Iteration 125 - Grad. Norm.: 0.005123871133112667 tk: 5 x_norm: 7.883627652979195\n",
      "Iteration 126 - Grad. Norm.: 0.005100054309111971 tk: 5 x_norm: 7.904886937459868\n",
      "Iteration 127 - Grad. Norm.: 0.005076550763412619 tk: 5 x_norm: 7.926044368094676\n",
      "Iteration 128 - Grad. Norm.: 0.005053353770879928 tk: 5 x_norm: 7.947101245650691\n",
      "Iteration 129 - Grad. Norm.: 0.005030456793015412 tk: 5 x_norm: 7.968058847124352\n",
      "Iteration 130 - Grad. Norm.: 0.005007853471794668 tk: 5 x_norm: 7.988918426261543\n",
      "Iteration 131 - Grad. Norm.: 0.004985537623742439 tk: 5 x_norm: 8.009681214064443\n",
      "Iteration 132 - Grad. Norm.: 0.00496350323423421 tk: 5 x_norm: 8.030348419285577\n",
      "Iteration 133 - Grad. Norm.: 0.00494174445201434 tk: 5 x_norm: 8.050921228909427\n",
      "Iteration 134 - Grad. Norm.: 0.004920255583921091 tk: 5 x_norm: 8.071400808621982\n",
      "Iteration 135 - Grad. Norm.: 0.004899031089809628 tk: 5 x_norm: 8.091788303268565\n",
      "Iteration 136 - Grad. Norm.: 0.004878065577664277 tk: 5 x_norm: 8.112084837300293\n",
      "Iteration 137 - Grad. Norm.: 0.0048573537988919 tk: 5 x_norm: 8.132291515209515\n",
      "Iteration 138 - Grad. Norm.: 0.0048368906437886855 tk: 5 x_norm: 8.152409421954514\n",
      "Iteration 139 - Grad. Norm.: 0.004816671137172886 tk: 5 x_norm: 8.1724396233738\n",
      "Iteration 140 - Grad. Norm.: 0.004796690434176531 tk: 5 x_norm: 8.192383166590304\n",
      "Iteration 141 - Grad. Norm.: 0.004776943816189436 tk: 5 x_norm: 8.212241080405732\n",
      "Iteration 142 - Grad. Norm.: 0.004757426686949174 tk: 5 x_norm: 8.232014375685388\n",
      "Iteration 143 - Grad. Norm.: 0.004738134568770871 tk: 5 x_norm: 8.251704045733728\n",
      "Iteration 144 - Grad. Norm.: 0.004719063098911212 tk: 5 x_norm: 8.271311066660848\n",
      "Iteration 145 - Grad. Norm.: 0.004700208026061033 tk: 5 x_norm: 8.290836397740293\n",
      "Iteration 146 - Grad. Norm.: 0.004681565206961339 tk: 5 x_norm: 8.310280981758263\n",
      "Iteration 147 - Grad. Norm.: 0.0046631306031377695 tk: 5 x_norm: 8.329645745354577\n",
      "Iteration 148 - Grad. Norm.: 0.0046449002777486995 tk: 5 x_norm: 8.348931599355577\n",
      "Iteration 149 - Grad. Norm.: 0.004626870392542518 tk: 5 x_norm: 8.368139439099176\n",
      "Iteration 150 - Grad. Norm.: 0.004609037204919693 tk: 5 x_norm: 8.387270144752287\n",
      "Iteration 151 - Grad. Norm.: 0.004591397065095509 tk: 5 x_norm: 8.406324581620847\n",
      "Iteration 152 - Grad. Norm.: 0.004573946413359559 tk: 5 x_norm: 8.425303600452606\n",
      "Iteration 153 - Grad. Norm.: 0.004556681777428187 tk: 5 x_norm: 8.444208037732885\n",
      "Iteration 154 - Grad. Norm.: 0.004539599769886302 tk: 5 x_norm: 8.463038715973518\n",
      "Iteration 155 - Grad. Norm.: 0.00452269708571512 tk: 5 x_norm: 8.481796443995103\n",
      "Iteration 156 - Grad. Norm.: 0.004505970499902511 tk: 5 x_norm: 8.500482017202804\n",
      "Iteration 157 - Grad. Norm.: 0.004489416865132891 tk: 5 x_norm: 8.519096217855822\n",
      "Iteration 158 - Grad. Norm.: 0.004473033109553527 tk: 5 x_norm: 8.537639815330706\n",
      "Iteration 159 - Grad. Norm.: 0.0044568162346144986 tk: 5 x_norm: 8.556113566378716\n",
      "Iteration 160 - Grad. Norm.: 0.004440763312979452 tk: 5 x_norm: 8.574518215377303\n",
      "Iteration 161 - Grad. Norm.: 0.004424871486504592 tk: 5 x_norm: 8.592854494575944\n",
      "Iteration 162 - Grad. Norm.: 0.0044091379642833385 tk: 5 x_norm: 8.611123124336434\n",
      "Iteration 163 - Grad. Norm.: 0.004393560020754221 tk: 5 x_norm: 8.62932481336775\n",
      "Iteration 164 - Grad. Norm.: 0.004378134993869741 tk: 5 x_norm: 8.647460258955713\n",
      "Iteration 165 - Grad. Norm.: 0.0043628602833239135 tk: 5 x_norm: 8.665530147187466\n",
      "Iteration 166 - Grad. Norm.: 0.004347733348836403 tk: 5 x_norm: 8.683535153170999\n",
      "Iteration 167 - Grad. Norm.: 0.00433275170849119 tk: 5 x_norm: 8.70147594124977\n",
      "Iteration 168 - Grad. Norm.: 0.004317912937127823 tk: 5 x_norm: 8.719353165212606\n",
      "Iteration 169 - Grad. Norm.: 0.004303214664783347 tk: 5 x_norm: 8.737167468498956\n",
      "Iteration 170 - Grad. Norm.: 0.004288654575183136 tk: 5 x_norm: 8.754919484399625\n",
      "Iteration 171 - Grad. Norm.: 0.004274230404278877 tk: 5 x_norm: 8.772609836253098\n",
      "Iteration 172 - Grad. Norm.: 0.004259939938832053 tk: 5 x_norm: 8.79023913763761\n",
      "Iteration 173 - Grad. Norm.: 0.004245781015041309 tk: 5 x_norm: 8.807807992558946\n",
      "Iteration 174 - Grad. Norm.: 0.004231751517212209 tk: 5 x_norm: 8.82531699563423\n",
      "Iteration 175 - Grad. Norm.: 0.004217849376467856 tk: 5 x_norm: 8.842766732271683\n",
      "Iteration 176 - Grad. Norm.: 0.004204072569498997 tk: 5 x_norm: 8.860157778846485\n",
      "Iteration 177 - Grad. Norm.: 0.004190419117352251 tk: 5 x_norm: 8.877490702872857\n",
      "Iteration 178 - Grad. Norm.: 0.0041768870842551304 tk: 5 x_norm: 8.894766063172419\n",
      "Iteration 179 - Grad. Norm.: 0.004163474576476612 tk: 5 x_norm: 8.911984410038947\n",
      "Iteration 180 - Grad. Norm.: 0.00415017974122207 tk: 5 x_norm: 8.929146285399577\n",
      "Iteration 181 - Grad. Norm.: 0.004137000765561341 tk: 5 x_norm: 8.946252222972568\n",
      "Iteration 182 - Grad. Norm.: 0.0041239358753888955 tk: 5 x_norm: 8.963302748421711\n",
      "Iteration 183 - Grad. Norm.: 0.004110983334414956 tk: 5 x_norm: 8.98029837950742\n",
      "Iteration 184 - Grad. Norm.: 0.0040981414431865764 tk: 5 x_norm: 8.997239626234652\n",
      "Iteration 185 - Grad. Norm.: 0.004085408538137675 tk: 5 x_norm: 9.014126990997669\n",
      "Iteration 186 - Grad. Norm.: 0.004072782990667028 tk: 5 x_norm: 9.030960968721734\n",
      "Iteration 187 - Grad. Norm.: 0.004060263206243351 tk: 5 x_norm: 9.047742047001842\n",
      "Iteration 188 - Grad. Norm.: 0.004047847623536561 tk: 5 x_norm: 9.064470706238508\n",
      "Iteration 189 - Grad. Norm.: 0.004035534713574314 tk: 5 x_norm: 9.081147419770717\n",
      "Iteration 190 - Grad. Norm.: 0.0040233229789230765 tk: 5 x_norm: 9.097772654006082\n",
      "Iteration 191 - Grad. Norm.: 0.0040112109528928725 tk: 5 x_norm: 9.114346868548282\n",
      "Iteration 192 - Grad. Norm.: 0.0039991971987649345 tk: 5 x_norm: 9.130870516321844\n",
      "Iteration 193 - Grad. Norm.: 0.003987280309041588 tk: 5 x_norm: 9.147344043694327\n",
      "Iteration 194 - Grad. Norm.: 0.003975458904717568 tk: 5 x_norm: 9.163767890595956\n",
      "Iteration 195 - Grad. Norm.: 0.003963731634572135 tk: 5 x_norm: 9.180142490636797\n",
      "Iteration 196 - Grad. Norm.: 0.003952097174481336 tk: 5 x_norm: 9.196468271221502\n",
      "Iteration 197 - Grad. Norm.: 0.003940554226749706 tk: 5 x_norm: 9.212745653661653\n",
      "Iteration 198 - Grad. Norm.: 0.003929101519460888 tk: 5 x_norm: 9.22897505328586\n",
      "Iteration 199 - Grad. Norm.: 0.003917737805846478 tk: 5 x_norm: 9.245156879547515\n",
      "Iteration 200 - Grad. Norm.: 0.003906461863672607 tk: 5 x_norm: 9.261291536130411\n",
      "Iteration 201 - Grad. Norm.: 0.003895272494643651 tk: 5 x_norm: 9.277379421052132\n",
      "Iteration 202 - Grad. Norm.: 0.003884168523822536 tk: 5 x_norm: 9.293420926765386\n",
      "Iteration 203 - Grad. Norm.: 0.003873148799067174 tk: 5 x_norm: 9.309416440257232\n",
      "Iteration 204 - Grad. Norm.: 0.003862212190482467 tk: 5 x_norm: 9.325366343146303\n",
      "Iteration 205 - Grad. Norm.: 0.003851357589887427 tk: 5 x_norm: 9.341271011778074\n",
      "Iteration 206 - Grad. Norm.: 0.003840583910296955 tk: 5 x_norm: 9.357130817318152\n",
      "Iteration 207 - Grad. Norm.: 0.0038298900854178133 tk: 5 x_norm: 9.372946125843725\n",
      "Iteration 208 - Grad. Norm.: 0.0038192750691583787 tk: 5 x_norm: 9.388717298433141\n",
      "Iteration 209 - Grad. Norm.: 0.003808737835151722 tk: 5 x_norm: 9.404444691253678\n",
      "Iteration 210 - Grad. Norm.: 0.003798277376291648 tk: 5 x_norm: 9.420128655647579\n",
      "Iteration 211 - Grad. Norm.: 0.0037878927042812854 tk: 5 x_norm: 9.43576953821631\n",
      "Iteration 212 - Grad. Norm.: 0.003777582849193835 tk: 5 x_norm: 9.451367680903184\n",
      "Iteration 213 - Grad. Norm.: 0.003767346859045158 tk: 5 x_norm: 9.466923421074299\n",
      "Iteration 214 - Grad. Norm.: 0.003757183799377783 tk: 5 x_norm: 9.482437091597857\n",
      "Iteration 215 - Grad. Norm.: 0.0037470927528560558 tk: 5 x_norm: 9.497909020921941\n",
      "Iteration 216 - Grad. Norm.: 0.0037370728188720584 tk: 5 x_norm: 9.513339533150699\n",
      "Iteration 217 - Grad. Norm.: 0.0037271231131619877 tk: 5 x_norm: 9.528728948119044\n",
      "Iteration 218 - Grad. Norm.: 0.0037172427674327076 tk: 5 x_norm: 9.544077581465874\n",
      "Iteration 219 - Grad. Norm.: 0.003707430928998128 tk: 5 x_norm: 9.559385744705834\n",
      "Iteration 220 - Grad. Norm.: 0.00369768676042518 tk: 5 x_norm: 9.574653745299665\n",
      "Iteration 221 - Grad. Norm.: 0.003688009439189058 tk: 5 x_norm: 9.589881886723186\n",
      "Iteration 222 - Grad. Norm.: 0.0036783981573374775 tk: 5 x_norm: 9.605070468534876\n",
      "Iteration 223 - Grad. Norm.: 0.00366885212116371 tk: 5 x_norm: 9.620219786442187\n",
      "Iteration 224 - Grad. Norm.: 0.003659370550888077 tk: 5 x_norm: 9.635330132366512\n",
      "Iteration 225 - Grad. Norm.: 0.0036499526803477284 tk: 5 x_norm: 9.650401794506912\n",
      "Iteration 226 - Grad. Norm.: 0.0036405977566944386 tk: 5 x_norm: 9.66543505740259\n",
      "Iteration 227 - Grad. Norm.: 0.0036313050401001637 tk: 5 x_norm: 9.680430201994136\n",
      "Iteration 228 - Grad. Norm.: 0.0036220738034701853 tk: 5 x_norm: 9.695387505683604\n",
      "Iteration 229 - Grad. Norm.: 0.00361290333216359 tk: 5 x_norm: 9.71030724239342\n",
      "Iteration 230 - Grad. Norm.: 0.003603792923720884 tk: 5 x_norm: 9.725189682624134\n",
      "Iteration 231 - Grad. Norm.: 0.003594741887598559 tk: 5 x_norm: 9.740035093511064\n",
      "Iteration 232 - Grad. Norm.: 0.003585749544910379 tk: 5 x_norm: 9.75484373887986\n",
      "Iteration 233 - Grad. Norm.: 0.003576815228175211 tk: 5 x_norm: 9.769615879300982\n",
      "Iteration 234 - Grad. Norm.: 0.0035679382810712527 tk: 5 x_norm: 9.78435177214314\n",
      "Iteration 235 - Grad. Norm.: 0.0035591180581964043 tk: 5 x_norm: 9.7990516716257\n",
      "Iteration 236 - Grad. Norm.: 0.0035503539248346832 tk: 5 x_norm: 9.8137158288701\n",
      "Iteration 237 - Grad. Norm.: 0.0035416452567284683 tk: 5 x_norm: 9.828344491950283\n",
      "Iteration 238 - Grad. Norm.: 0.003532991439856431 tk: 5 x_norm: 9.842937905942149\n",
      "Iteration 239 - Grad. Norm.: 0.0035243918702169935 tk: 5 x_norm: 9.857496312972092\n",
      "Iteration 240 - Grad. Norm.: 0.0035158459536171406 tk: 5 x_norm: 9.8720199522646\n",
      "Iteration 241 - Grad. Norm.: 0.0035073531054664897 tk: 5 x_norm: 9.886509060188947\n",
      "Iteration 242 - Grad. Norm.: 0.0034989127505763986 tk: 5 x_norm: 9.900963870305036\n",
      "Iteration 243 - Grad. Norm.: 0.0034905243229640453 tk: 5 x_norm: 9.915384613408317\n",
      "Iteration 244 - Grad. Norm.: 0.003482187265661298 tk: 5 x_norm: 9.929771517573922\n",
      "Iteration 245 - Grad. Norm.: 0.003473901030528257 tk: 5 x_norm: 9.944124808199922\n",
      "Iteration 246 - Grad. Norm.: 0.0034656650780713503 tk: 5 x_norm: 9.958444708049793\n",
      "Iteration 247 - Grad. Norm.: 0.003457478877265842 tk: 5 x_norm: 9.972731437294097\n",
      "Iteration 248 - Grad. Norm.: 0.003449341905382658 tk: 5 x_norm: 9.986985213551353\n",
      "Iteration 249 - Grad. Norm.: 0.003445294909853667 tk: 2.5 x_norm: 9.994094541898372\n",
      "Iteration 250 - Grad. Norm.: 0.0034432767417426333 tk: 1.25 x_norm: 9.997644843128413\n",
      "最小值: 0.095011\t耗时: 26.128632s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "init_x = np.zeros(n)+0.005\n",
    "x_opt_gd, _, _, t_gd = gradient_descent(f=f, f_grad=f_grad, x0=init_x, D=20, t_hat=5, epsilon=1e-6, max_iters=300)\n",
    "optim_val = f(x_opt_gd)\n",
    "print(f'最小值: {optim_val:>2f}\\t耗时: {t_gd:>2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09501128827728388"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_opt_gd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton (Interior Point Methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logarithmic barrier:\n",
    "$$\n",
    "\\phi(x)=-\\log(-g(x)),\\quad g(x)=\\left\\|x\\right\\|_2 - D/2\n",
    "$$\n",
    "$$\n",
    "\\nabla g(x) = x/\\left\\|x\\right\\|_2, \\quad \\nabla^2 g(x)= \\left\\|x\\right\\|_2^{-1}I-\\left\\|x\\right\\|_2^{-3}xx^\\top\n",
    "$$\n",
    "gradient and hessian:\n",
    "$$\n",
    "\\nabla \\phi(x) = \\frac{1}{-g(x)}\\nabla g(x)=\\frac{x/\\left\\|x\\right\\|_2}{D/2-\\left\\|x\\right\\|_2}=\\frac{x}{\\left\\|x\\right\\|_2(D/2-\\left\\|x\\right\\|_2)}\n",
    "$$\n",
    "$$\n",
    "\\nabla^2\\phi(x)=\\frac{1}{g(x)^2}\\nabla g(x)\\nabla g(x)^\\top + \\frac{1}{-g(x)}\\nabla^2 g(x) = \\frac{1}{\\left\\|x\\right\\|_2(D/2-\\left\\|x\\right\\|_2)}I+\\frac{2\\left\\|x\\right\\|_2-D/2}{\\left\\|x\\right\\|_2^3(D/2-\\left\\|x\\right\\|_2)^2}xx^\\top\n",
    "$$\n",
    "central path - for $t>0$:\n",
    "$$\n",
    "\\min_x tf(x)+\\phi(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x,D):\n",
    "    return -np.log(D/2-np.linalg.norm(x,ord=2))\n",
    "\n",
    "def phi_grad(x,D):\n",
    "    x_norm = np.linalg.norm(x,ord=2)\n",
    "    return x/(x_norm*(D/2-x_norm))\n",
    "\n",
    "def phi_hessian(x,D):\n",
    "    x_norm = np.linalg.norm(x,ord=2)\n",
    "    xxT = np.matmul(x[:,None],x[None,:])    # x * xT\n",
    "    return np.eye(x.size)/(x_norm*(D/2-x_norm)) + (2*x_norm-D/2)/(x_norm**3 * (D/2-x_norm)**2)*xxT\n",
    "\n",
    "\n",
    "#* 外部迭代\n",
    "def barrier_method(t_init, f, f_grad, f_hessian, phi, phi_grad, phi_hessian, A, b, x0, D, num_constraints, mu,\n",
    "                        method='newton', epsilon=1e-6, maxIter=20):\n",
    "    xt = x0\n",
    "    t = t_init\n",
    "    duality_gaps = []\n",
    "    t_s = time()\n",
    "    for i in range(maxIter):\n",
    "        xt,num_newton_step = solve_central(f=lambda x:t*f(x)+phi(x,D), \n",
    "                                f_grad=lambda x:t*f_grad(x)+phi_grad(x,D), \n",
    "                                f_hessian=lambda x:t*f_hessian(x)+phi_hessian(x,D),\n",
    "                                x0=xt, D=D, method=method, epsilon=epsilon)\n",
    "        duality_gaps.extend([num_constraints/t]*num_newton_step)\n",
    "        if num_constraints/t < epsilon:\n",
    "            break\n",
    "        t *= mu\n",
    "    t_e = time()\n",
    "    return xt, t_e-t_s, duality_gaps\n",
    "\n",
    "def solve_central(f, f_grad, f_hessian, x0, D, method='newton', epsilon=1e-6, max_iter=50):\n",
    "    if method == 'newton':\n",
    "        return damped_newton(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=x0, D=D, epsilon=epsilon, max_iter=max_iter)\n",
    "    if method == 'bfgs':\n",
    "        return bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=x0, D=D, mat_type='H', epsilon=epsilon, max_iter=max_iter)\n",
    "\n",
    "\n",
    "#* 阻尼牛顿\n",
    "def damped_newton(f, f_grad, f_hessian, x0, D, epsilon=1e-6, max_iter=50):\n",
    "    xk = x0\n",
    "    iter_cnt = 0\n",
    "    for idx in range(max_iter):\n",
    "        iter_cnt += 1\n",
    "        grad = f_grad(xk)\n",
    "        hessian = f_hessian(xk)\n",
    "        dk = -np.linalg.inv(hessian)@grad\n",
    "        decrement = (-grad@dk)**0.5\n",
    "        if decrement**2/2 <= epsilon:\n",
    "            print('** End The Loop - Iter Cnt.:',iter_cnt, 'Decrement:',decrement, 'fval:',f(xk))\n",
    "            return xk, iter_cnt\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=1, alpha=0.1, beta=0.5, D=D, isNewton=True, dk=dk)\n",
    "        print('Iter Cnt.:',iter_cnt, 'Decrement:',decrement, 'fval:',f(xk), 'tk:',tk)\n",
    "        xk += tk*dk\n",
    "    return xk, iter_cnt\n",
    "\n",
    "#* 拟牛顿\n",
    "def bfgs(f, f_grad, f_hessian, x0, D, mat_type='H', alpha=0.1, beta=0.5, epsilon=1e-6, max_iter=500):\n",
    "    assert mat_type in ['H','B']\n",
    "    xk = x0\n",
    "    hessian = f_hessian(x0)\n",
    "    mat_k = np.linalg.inv(hessian) if mat_type=='H' else hessian\n",
    "    iter_cnt = 0\n",
    "    for idx in range(max_iter):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        dk = -mat_k@grad_k if mat_type=='H' else -np.linalg.inv(mat_k)@grad_k\n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, D, c1=1e-4, c2=0.9)\n",
    "        if tk<0:\n",
    "            return xk, iter_cnt-1\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "            return xk_next, iter_cnt\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        mat_k = update_approximation(mat=mat_k, sk=sk, yk=grad_next-grad_k, mat_type=mat_type)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt\n",
    "        \n",
    "def update_approximation(mat, sk, yk, mat_type='H'):\n",
    "    rhok = 1/(yk@sk)\n",
    "    if mat_type == 'H':\n",
    "        Hkyk = mat@yk\n",
    "        ykTHkyk = yk@Hkyk\n",
    "        HkykskT = Hkyk[:,None]@sk[None,:]\n",
    "        skskT = sk[:,None]@sk[None,:]\n",
    "        mat_new = mat + rhok*((rhok*ykTHkyk+1)*skskT - HkykskT - HkykskT.T)\n",
    "    else:\n",
    "        Bksk = mat@sk\n",
    "        skTBksk = sk@Bksk\n",
    "        mat_new = mat - Bksk[:,None]@Bksk[None,:]/skTBksk + yk[:,None]@yk[None,:]*rhok\n",
    "    return mat_new\n",
    "\n",
    "#* 拟牛顿方法 - 选择步长\n",
    "def wolfe_condition(f, f_grad, xk, pk, D, c1=1e-4, c2=0.9, multiplier=1.25, t0=0, tmax=2):\n",
    "    ### \n",
    "    while (np.linalg.norm(xk+tmax*pk)>=D/2):\n",
    "        tmax /= 2\n",
    "        if tmax<1e-6:\n",
    "            print('too small stepsize')\n",
    "            return -1\n",
    "    ###\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "def zoom(f, f_grad, xk, pk, fval, grad, c1, c2, t_lo, t_hi):\n",
    "    while True:\n",
    "        # print(f\"t_lo: {t_lo}\\tt_hi: {t_hi}\")\n",
    "        t = (t_lo+t_hi)/2\n",
    "        xk_next = xk + t*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if fval_next > fval + c1*t*grad@pk or fval_next >= f(xk+t_lo*pk):\n",
    "            t_hi = t\n",
    "        else:\n",
    "            grad_next = f_grad(xk_next)\n",
    "            grad_next_T_pk = grad_next@pk\n",
    "            if np.abs(grad_next_T_pk) <= -c2*grad@pk:\n",
    "                return t\n",
    "            if grad_next_T_pk*(t_hi-t_lo)>=0:\n",
    "                t_hi = t_lo\n",
    "            t_lo = t\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter Cnt.: 1 Decrement: 0.9525578038914555 fval: -1.569533749629282 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6084895374423559 fval: -1.7498984994316462 tk: 0.5\n",
      "Iter Cnt.: 3 Decrement: 0.12992597126245592 fval: -1.8633527068729119 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.02321306437254386 fval: -1.8726656070316618 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.0008165055995858134 fval: -1.8729411315741025\n",
      "Iter Cnt.: 1 Decrement: 1.0505730577510222 fval: 0.5845139209528405 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.585223074171573 fval: -0.10818477669154491 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.21697609488014757 fval: -0.3119420644621729 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.031463882526236386 fval: -0.33751015706476695 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.0006658402149019884 fval: -0.3380119579267411\n",
      "Iter Cnt.: 1 Decrement: 2.157234704733954 fval: 12.255104830179413 tk: 1\n",
      "Iter Cnt.: 2 Decrement: 0.6585288604887605 fval: 9.84285369927622 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.10642726247470574 fval: 9.607231458766698 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.004245314434446256 fval: 9.601433403592393\n",
      "Iter Cnt.: 1 Decrement: 4.746415588292139 fval: 101.2434144945373 tk: 0.25\n",
      "Iter Cnt.: 2 Decrement: 1.5043772013562888 fval: 96.55780558795904 tk: 0.5\n",
      "Iter Cnt.: 3 Decrement: 0.25120190928268443 fval: 95.84903366797712 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.019139934940813196 fval: 95.81665440760761 tk: 1\n",
      "** End The Loop - Iter Cnt.: 5 Decrement: 0.00032806147860080324 fval: 95.81646920622667\n",
      "Iter Cnt.: 1 Decrement: 7.887038200022307 fval: 945.3674432347402 tk: 0.125\n",
      "Iter Cnt.: 2 Decrement: 0.40502549032038165 fval: 938.9136693855903 tk: 1\n",
      "Iter Cnt.: 3 Decrement: 0.09743907788643821 fval: 938.8460042605169 tk: 1\n",
      "** End The Loop - Iter Cnt.: 4 Decrement: 0.008793514293852382 fval: 938.840991585126\n",
      "Iter Cnt.: 1 Decrement: 8.787064779051512 fval: 9355.157745611248 tk: 0.0625\n",
      "Iter Cnt.: 2 Decrement: 3.523316666472217 fval: 9350.574731024411 tk: 0.25\n",
      "Iter Cnt.: 3 Decrement: 0.44818003731580836 fval: 9348.697231745165 tk: 1\n",
      "Iter Cnt.: 4 Decrement: 0.19998956585532118 fval: 9348.5743278675 tk: 1\n",
      "Iter Cnt.: 5 Decrement: 0.03989031548455568 fval: 9348.552014773097 tk: 1\n",
      "** End The Loop - Iter Cnt.: 6 Decrement: 0.0015886699793915353 fval: 9348.551198644815\n",
      "最小值: 0.093426\t耗时: 133.234586s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "# D=20\n",
    "t_init = 1\n",
    "x0 = np.zeros(n)+0.005\n",
    "x_opt_ipm, t_ipm, duality_gaps = barrier_method(t_init=t_init, f=f, f_grad=f_grad, f_hessian=f_hessian, phi=phi, phi_grad=phi_grad, phi_hessian=phi_hessian, \n",
    "                A=A, b=b, x0=x0, D=20, num_constraints=1, method='newton', mu=10, epsilon=1e-4, maxIter=20)\n",
    "optim_val = f(x_opt_ipm)\n",
    "print(f'最小值: {optim_val:>2f}\\t耗时: {t_ipm:>2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.16436546152681786 tk: 1.0 x_norm: 1.895870645724538\n",
      "Iteration 2 - grad_norm: 0.14819799632485806 tk: 1.25 x_norm: 1.60105491595516\n",
      "Iteration 3 - grad_norm: 0.07501909486740485 tk: 0.5 x_norm: 0.9970702106665017\n",
      "Iteration 4 - grad_norm: 0.0560938807030226 tk: 1.0 x_norm: 1.0942080489373525\n",
      "Iteration 5 - grad_norm: 0.014818380495669102 tk: 1.0 x_norm: 1.3775874029982025\n",
      "Iteration 6 - grad_norm: 0.010122705458675912 tk: 1.0 x_norm: 1.4372392645287562\n",
      "Iteration 7 - grad_norm: 0.008782553751764855 tk: 1.0 x_norm: 1.455905728391707\n",
      "Iteration 8 - grad_norm: 0.006140141945200271 tk: 1.0 x_norm: 1.471384474129371\n",
      "Iteration 9 - grad_norm: 0.003157080754592828 tk: 1.0 x_norm: 1.4687294680438514\n",
      "Iteration 10 - grad_norm: 0.0015370680724593243 tk: 1.0 x_norm: 1.4569282261587784\n",
      "Iteration 11 - grad_norm: 0.0012081718529017302 tk: 1.0 x_norm: 1.4511887349361337\n",
      "Iteration 12 - grad_norm: 0.0010278736554497983 tk: 1.0 x_norm: 1.449116146443951\n",
      "Iteration 1 - grad_norm: 0.3789712531607941 tk: 1.0 x_norm: 2.671629785607948\n",
      "Iteration 2 - grad_norm: 0.1910296333454964 tk: 1.0 x_norm: 3.378126289778345\n",
      "Iteration 3 - grad_norm: 0.06582490334449599 tk: 1.0 x_norm: 4.022968944272671\n",
      "Iteration 4 - grad_norm: 0.020006191948624442 tk: 1.0 x_norm: 4.279909150181426\n",
      "Iteration 5 - grad_norm: 0.007213548022334256 tk: 1.0 x_norm: 4.340148505324591\n",
      "Iteration 6 - grad_norm: 0.004324547177849425 tk: 1.0 x_norm: 4.339109434915201\n",
      "Iteration 7 - grad_norm: 0.0024511105707731295 tk: 1.0 x_norm: 4.324917858964552\n",
      "Iteration 8 - grad_norm: 0.0012403045220642137 tk: 1.0 x_norm: 4.3179899442284\n",
      "Iteration 1 - grad_norm: 0.9369478390445507 tk: 0.5 x_norm: 6.152674095876703\n",
      "Iteration 2 - grad_norm: 0.5634704624365673 tk: 0.5 x_norm: 7.329215465136132\n",
      "Iteration 3 - grad_norm: 0.10944488055706747 tk: 1.0 x_norm: 8.37172825384597\n",
      "Iteration 4 - grad_norm: 0.052809445830019675 tk: 1.0 x_norm: 8.11705853256108\n",
      "Iteration 5 - grad_norm: 0.03041518301210653 tk: 1.0 x_norm: 8.186857629757148\n",
      "Iteration 6 - grad_norm: 0.017508757420024026 tk: 1.0 x_norm: 8.219009049387795\n",
      "Iteration 7 - grad_norm: 0.007513357293586675 tk: 1.0 x_norm: 8.216061839260798\n",
      "Iteration 8 - grad_norm: 0.0013403782001252004 tk: 1.0 x_norm: 8.20884547858042\n",
      "Iteration 1 - grad_norm: 4.3221074350275295 tk: 0.125 x_norm: 8.796834487884682\n",
      "Iteration 2 - grad_norm: 3.5118232375507463 tk: 0.125 x_norm: 9.2506568511739\n",
      "Iteration 3 - grad_norm: 2.7713303553120685 tk: 0.125 x_norm: 9.470911012017307\n",
      "Iteration 4 - grad_norm: 1.5470289854456971 tk: 0.25 x_norm: 9.654831243740679\n",
      "Iteration 5 - grad_norm: 0.16764841204437184 tk: 0.5 x_norm: 9.756881970609463\n",
      "Iteration 6 - grad_norm: 0.07257469680676289 tk: 1.0 x_norm: 9.754279962434921\n",
      "Iteration 7 - grad_norm: 0.03621970301673808 tk: 0.5 x_norm: 9.758335600576526\n",
      "Iteration 8 - grad_norm: 0.033408437008760165 tk: 0.5 x_norm: 9.760403428498151\n",
      "Iteration 9 - grad_norm: 0.008991311845756408 tk: 0.5 x_norm: 9.758971567712223\n",
      "Iteration 10 - grad_norm: 0.010167210292720621 tk: 0.5 x_norm: 9.75827203465799\n",
      "Iteration 11 - grad_norm: 0.004981563173978088 tk: 0.5 x_norm: 9.758448716928477\n",
      "Iteration 12 - grad_norm: 0.0013525759981819986 tk: 0.5 x_norm: 9.758683469552953\n",
      "Iteration 13 - grad_norm: 0.0013849749474987353 tk: 0.5 x_norm: 9.758723594700779\n",
      "Iteration 1 - grad_norm: 33.565102325862995 tk: 0.0625 x_norm: 9.863697887592851\n",
      "Iteration 2 - grad_norm: 27.673734162217258 tk: 0.0625 x_norm: 9.92253004298255\n",
      "Iteration 3 - grad_norm: 17.33905462373147 tk: 0.125 x_norm: 9.956491767035807\n",
      "Iteration 4 - grad_norm: 6.245752487875069 tk: 0.25 x_norm: 9.970477307596575\n",
      "Iteration 5 - grad_norm: 1.0520143018606287 tk: 0.5 x_norm: 9.9743021960429\n",
      "Iteration 6 - grad_norm: 0.15121164706245935 tk: 0.25 x_norm: 9.974880861373792\n",
      "Iteration 7 - grad_norm: 0.20836123720504687 tk: 1.0 x_norm: 9.97505837917602\n",
      "Iteration 8 - grad_norm: 0.1917952197578579 tk: 0.25 x_norm: 9.975046582116326\n",
      "Iteration 9 - grad_norm: 0.047080567578473335 tk: 0.5 x_norm: 9.974957892286787\n",
      "Iteration 10 - grad_norm: 0.08991912185598196 tk: 0.5 x_norm: 9.97488582039267\n",
      "Iteration 11 - grad_norm: 0.08057421155219611 tk: 0.25 x_norm: 9.974892136524838\n",
      "Iteration 12 - grad_norm: 0.010067699478859089 tk: 0.5 x_norm: 9.974945316102481\n",
      "Iteration 13 - grad_norm: 0.026629493294079674 tk: 0.5 x_norm: 9.974956612450667\n",
      "Iteration 14 - grad_norm: 0.011437620005062437 tk: 0.5 x_norm: 9.974945218269946\n",
      "Iteration 15 - grad_norm: 0.007220000357875016 tk: 0.5 x_norm: 9.974935427407608\n",
      "Iteration 16 - grad_norm: 0.006460160579096762 tk: 0.5 x_norm: 9.97493640098992\n",
      "Iteration 17 - grad_norm: 0.0027677591015709572 tk: 0.5 x_norm: 9.97494061107029\n",
      "Iteration 18 - grad_norm: 0.0021989518409778704 tk: 0.5 x_norm: 9.974941357685092\n",
      "Iteration 19 - grad_norm: 0.0017196036481905688 tk: 0.5 x_norm: 9.974940538911618\n",
      "Iteration 20 - grad_norm: 0.0011119312641360867 tk: 0.5 x_norm: 9.974939804871173\n",
      "Iteration 21 - grad_norm: 0.0010099412789054021 tk: 0.5 x_norm: 9.974939651169077\n",
      "最小值: 0.093515\t耗时: 69.243815s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "# D=20\n",
    "t_init = 1\n",
    "x0 = np.zeros(n)+0.005\n",
    "x_opt_ipm, t_ipm, duality_gaps = barrier_method(t_init=t_init, f=f, f_grad=f_grad, f_hessian=f_hessian, phi=phi, phi_grad=phi_grad, phi_hessian=phi_hessian, \n",
    "                A=A, b=b, x0=x0, D=20, num_constraints=1, method='bfgs', mu=10, epsilon=1e-3, maxIter=20)\n",
    "optim_val = f(x_opt_ipm)\n",
    "print(f'最小值: {optim_val:>2f}\\t耗时: {t_ipm:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure damped newton w/o constraints (D=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_norm: 0.18017720987362976 tk: 1 x_norm: 11.165278301280532\n",
      "grad_norm: 0.07430235763542076 tk: 1 x_norm: 18.78206938371404\n",
      "grad_norm: 0.03121190141848213 tk: 1 x_norm: 27.44040477856484\n",
      "grad_norm: 0.012796497780903252 tk: 1 x_norm: 36.77727415441325\n",
      "grad_norm: 0.004894660242605477 tk: 1 x_norm: 45.9257331177887\n",
      "grad_norm: 0.0016353055256952348 tk: 1 x_norm: 54.13674948486485\n",
      "grad_norm: 0.0004390302509728913 tk: 1 x_norm: 60.10520617618396\n",
      "grad_norm: 9.125726807373675e-05 tk: 1 x_norm: 63.53445177996058\n",
      "grad_norm: 2.4768185298650292e-05 tk: 1 x_norm: 65.32760767837152\n",
      "grad_norm: 6.8735057881137935e-06 tk: 1 x_norm: 66.52694639503159\n",
      "grad_norm: 1.2104066887873718e-06 tk: 1 x_norm: 67.1480388626449\n",
      "grad_norm: 9.161945407374053e-08 tk: 1 x_norm: 67.27924417413054\n",
      "最小值: 0.058278\t耗时: 2.153006s\n"
     ]
    }
   ],
   "source": [
    "#* pure阻尼牛顿\n",
    "def pure_damped_newton(f, f_grad, f_hessian, x0, D, epsilon=1e-6, max_iters=100):\n",
    "    func_val_record = []\n",
    "    grad_norm_record = []\n",
    "    xk = x0\n",
    "    t_s = time()\n",
    "    # for idx in trange(max_iters):\n",
    "    for idx in range(max_iters):\n",
    "        grad = f_grad(xk)\n",
    "        # print(np.linalg.norm(grad))\n",
    "        hessian = f_hessian(xk)\n",
    "        # dk = -np.linalg.pinv(hessian)@grad\n",
    "        dk = -np.linalg.inv(hessian)@grad\n",
    "        tk = armijo_search(f, f_grad, xk, t_hat=1, alpha=0.1, beta=0.5, D=D, isNewton=True, dk=dk)\n",
    "        xk_next = xk + tk*dk\n",
    "        func_val_record.append(f(xk_next))\n",
    "        # grad_norm_record.append(np.linalg.norm(f_grad(xk_next),ord=2))\n",
    "        grad_next = np.linalg.norm(f_grad(xk_next),ord=2)\n",
    "        grad_norm_record.append(grad_next)\n",
    "        # termination criteria\n",
    "        if grad_next<=epsilon:\n",
    "            break\n",
    "        else:\n",
    "            print('grad_norm:',grad_next,\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        xk = xk_next\n",
    "    t_e = time()\n",
    "    return xk_next, np.asarray(func_val_record), np.asarray(grad_norm_record), t_e-t_s\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "# init_x = np.zeros(n)+0.5\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# D=500 半径足够大，本质上是无约束\n",
    "x_opt_damped, _, _, t_damped = pure_damped_newton(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, D=500, epsilon=1e-8, max_iters=50)\n",
    "print(f'最小值: {f(x_opt_damped):>2f}\\t耗时: {t_damped:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure quasi newton w/o constraints (D=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - grad_norm: 0.18017720987362976 tk: 1.0 x_norm: 11.165278301280532\n",
      "Iteration 2 - grad_norm: 0.11332750219773532 tk: 1.0 x_norm: 15.135338733342227\n",
      "Iteration 3 - grad_norm: 0.0588887609960591 tk: 1.0 x_norm: 21.533528580786278\n",
      "Iteration 4 - grad_norm: 0.03370457453496356 tk: 1.0 x_norm: 28.007182606015498\n",
      "Iteration 5 - grad_norm: 0.018882899036194684 tk: 1.0 x_norm: 35.87806759864599\n",
      "Iteration 6 - grad_norm: 0.010866841039356227 tk: 1.0 x_norm: 44.44864398760293\n",
      "Iteration 7 - grad_norm: 0.006490045202947455 tk: 1.0 x_norm: 53.327850892309826\n",
      "Iteration 8 - grad_norm: 0.004300271670796263 tk: 1.0 x_norm: 61.34159830918189\n",
      "Iteration 9 - grad_norm: 0.0033286798522448226 tk: 1.0 x_norm: 67.46185331330766\n",
      "Iteration 10 - grad_norm: 0.0029130781154975767 tk: 1.0 x_norm: 71.52202526197196\n",
      "Iteration 11 - grad_norm: 0.0026709621915537753 tk: 1.0 x_norm: 74.55496886554727\n",
      "Iteration 12 - grad_norm: 0.0024711794717667194 tk: 1.0 x_norm: 77.01301640920776\n",
      "Iteration 13 - grad_norm: 0.0022580722379469745 tk: 1.0 x_norm: 77.61081896031442\n",
      "Iteration 14 - grad_norm: 0.0019611305006011176 tk: 1.0 x_norm: 74.87516342684319\n",
      "Iteration 15 - grad_norm: 0.001815015621707459 tk: 1.0 x_norm: 69.7557515224829\n",
      "Iteration 16 - grad_norm: 0.0019063176832953225 tk: 1.0 x_norm: 64.42711104360677\n",
      "Iteration 17 - grad_norm: 0.0018763345697549107 tk: 1.0 x_norm: 61.94319651661975\n",
      "Iteration 18 - grad_norm: 0.0017208657496739978 tk: 1.0 x_norm: 61.616147745504676\n",
      "Iteration 19 - grad_norm: 0.001386078159312382 tk: 1.0 x_norm: 62.89409076494536\n",
      "Iteration 20 - grad_norm: 0.0011635196375529271 tk: 1.0 x_norm: 65.10222137327875\n",
      "Iteration 21 - grad_norm: 0.0010389973610115406 tk: 1.0 x_norm: 67.10553071463985\n",
      "BFGS - 迭代次数: 22\t最小值: 0.061258\t耗时: 10.131429s\n"
     ]
    }
   ],
   "source": [
    "#* 拟牛顿\n",
    "def quasi_newton_bfgs(f, f_grad, f_hessian, x0, mat_type='H', alpha=0.1, beta=0.5, epsilon=1e-6, max_iters=500):\n",
    "    assert mat_type in ['H','B']\n",
    "    xk = x0\n",
    "    hessian = f_hessian(x0)\n",
    "    mat_k = np.linalg.inv(hessian) if mat_type=='H' else hessian\n",
    "    iter_cnt = 0\n",
    "    t_s = time()\n",
    "    for idx in range(max_iters):\n",
    "        iter_cnt += 1\n",
    "        grad_k = f_grad(xk)\n",
    "        dk = -mat_k@grad_k if mat_type=='H' else -np.linalg.inv(mat_k)@grad_k\n",
    "        tk = wolfe_condition(f, f_grad, xk, dk, c1=1e-4, c2=0.9)\n",
    "        sk = tk*dk\n",
    "        xk_next = xk + sk\n",
    "        grad_next = f_grad(xk_next)\n",
    "        if np.linalg.norm(grad_next, ord=2) <= epsilon:\n",
    "            return xk_next, iter_cnt, time()-t_s\n",
    "        else:\n",
    "            print(f'Iteration {iter_cnt} - grad_norm:',np.linalg.norm(grad_next),\"tk:\",tk, \"x_norm:\",np.linalg.norm(xk_next))\n",
    "        # mat_k = np.linalg.inv(f_hessian(xk_next))\n",
    "        mat_k = update_approximation(mat=mat_k, sk=sk, yk=grad_next-grad_k, mat_type=mat_type)\n",
    "        xk = xk_next\n",
    "    return xk_next, iter_cnt, time()-t_s\n",
    "\n",
    "def wolfe_condition(f, f_grad, xk, pk, c1=1e-4, c2=0.9, multiplier=1.25, t0=0, tmax=2):\n",
    "    ti = tmax/2\n",
    "    tprev = t0\n",
    "    i = 1\n",
    "    fval_cur = f(xk)\n",
    "    grad_cur = f_grad(xk)\n",
    "    while True:\n",
    "        xk_next = xk+ti*pk\n",
    "        fval_next = f(xk_next)\n",
    "        if (fval_next > fval_cur + c1*ti*grad_cur@pk) or (fval_next >= fval_cur and i>1):\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, tprev, ti)\n",
    "        grad_next = f_grad(xk_next)\n",
    "        grad_next_T_pk = grad_next@pk\n",
    "        if np.abs(grad_next_T_pk) <= -c2*grad_cur@pk:\n",
    "            return ti\n",
    "        if grad_next_T_pk >= 0:\n",
    "            return zoom(f, f_grad, xk, pk, fval_cur, grad_cur, c1, c2, ti, tprev)\n",
    "        tprev = ti\n",
    "        ti = tprev*multiplier\n",
    "        i += 1\n",
    "\n",
    "np.random.seed(1000)\n",
    "init_x = np.zeros(n)+0.005\n",
    "\n",
    "# 使用 quasi-newton 求解无约束问题\n",
    "# quasi_newton_bfgs(f, f_grad, f_hessian, x0, mat_type='H', alpha=0.1, beta=0.5, epsilon=1e-6, maxIter=100):\n",
    "x_opt_bfgs, iter_cnt_bfgs, t_bfgs = quasi_newton_bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, mat_type='H', epsilon=1e-3, max_iters=200)\n",
    "# x_opt_bfgs, iter_cnt_bfgs, t_bfgs = quasi_newton_bfgs(f=f, f_grad=f_grad, f_hessian=f_hessian, x0=init_x, mat_type='B', epsilon=1e-8, max_iters=200)\n",
    "print(f'BFGS - 迭代次数: {iter_cnt_bfgs}\\t最小值: {f(x_opt_bfgs):>2f}\\t耗时: {t_bfgs:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi Newton - BFGS (Interior Point Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "# D=20\n",
    "t_init = 1\n",
    "x0 = np.zeros(n)+0.005\n",
    "x_opt_ipm, t_ipm, duality_gaps = barrier_method(t_init=t_init, f=f, f_grad=f_grad, f_hessian=f_hessian, phi=phi, phi_grad=phi_grad, phi_hessian=phi_hessian, \n",
    "                A=A, b=b, x0=x0, D=20, num_constraints=1, mu=10, epsilon=1e-8, maxIter=20)\n",
    "optim_val = f(x_opt_ipm)\n",
    "print(f'最小值: {optim_val:>2f}\\t耗时: {t_ipm:>2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVXPY-计算 optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优值: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzq22/anaconda3/envs/optim/lib/python3.8/site-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 8 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# D=500\n",
    "# x = cp.Variable(n)\n",
    "# # objective = cp.Minimize(sum(cp.log1p(cp.exp(-b*(A@x))))/m + 1/(100*m)* x.T@x)\n",
    "# # objective = cp.Minimize(sum(cp.log1p(cp.exp(-b*(A@x))))/m)\n",
    "# objective = cp.Minimize(cp.norm(x)**2 + sum(cp.log1p(cp.exp(-b*(A@x))))/m)\n",
    "# constraints = [cp.norm(x)<=D/2]\n",
    "# problem = cp.Problem(objective,constraints)\n",
    "# result = problem.solve()\n",
    "# print(f'最优值: {f(x.value)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyhton 3 [optim]",
   "language": "python",
   "name": "optim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
